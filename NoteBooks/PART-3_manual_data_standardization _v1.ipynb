{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1z1YHvZqvJTMYgk42sOzKpl1kKOC9eIao","timestamp":1709145326525},{"file_id":"1U5EAk7nA75pZnCHAu5Yx3w7Jm9M9b-_4","timestamp":1708867975574},{"file_id":"1b4kco862abMBn8j5PEdAWKMgLBydWEGF","timestamp":1704562577409},{"file_id":"1vqp38x5aWySx5FrEC_JbzHbA3kUK6-ZK","timestamp":1703961883733},{"file_id":"1DtDZ-FBxcNSbZAxbiAE3A3gfTx9We9ki","timestamp":1703872868379}],"authorship_tag":"ABX9TyOsxgKogZwMIZojXOU1AVQ4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# MANUAL DATA STANDARDIZATION\n","\n","----------------------------\n","### GIST OF CHANGES DONE IN THIS NOTEBOOK\n","----------------------------\n","-   The primary appproach for standardizing code descriptions was to replace some commonly occuring verbs and terms across multiple rows with desired standardization\n","- <b>STANDARDIZATION APPROACH</b>  \n","  -   Standardize the code to contain standard column names like COL_A, COL B etc and standard literals like LIT_STR_1, LIT_INT_1, LIT_DEC_1 etc for various types of literals using regex\n","  -   Incorporate the standardized columns and literals above into the code descriptions to establish a direct relation between the code and descriptions via these standards\n","  -   Standardization are applied primarily to functions using up to three standardized arguments. Some complicated code and description were not standardized in the interest of time and due to the complicated nature of the code.\n","  -   A very small set of code descriptions which were found to be incoherent were adjusted to describe the code briefly\n","-   <b>NOTE</b>: Standardization done below will be appended to the original dataset hence retaining the original code and descriptions\n","----------------------------\n","<br>\n","<br>\n"],"metadata":{"id":"jdYDazNza3x0"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mTYySyCN_Z9H","executionInfo":{"status":"ok","timestamp":1710278041499,"user_tz":-330,"elapsed":3163,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"e72aa893-a32c-4cb5-8d43-b79ef1bc0ed7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["# IMPORT NECESSARY PACKAGES\n","import io\n","import re\n","import pandas as pd\n","from tqdm import tqdm\n","from google.colab import drive, files\n","\n","import warnings\n","warnings.simplefilter(action='ignore')\n"],"metadata":{"id":"OfPB61Gz8VfY","executionInfo":{"status":"ok","timestamp":1710278041502,"user_tz":-330,"elapsed":78,"user":{"displayName":"SK G","userId":"17874234191554613076"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# DISPLAY FUNCTIONS\n","def fn_display_header(msg):\n","  print('-' * 80)\n","  print(' ' * 10, msg)\n","  print('-' * 80)\n","\n","def fn_display_message(msg):\n","  print(msg)"],"metadata":{"id":"pEiTi6z_8rTF","executionInfo":{"status":"ok","timestamp":1710278041503,"user_tz":-330,"elapsed":74,"user":{"displayName":"SK G","userId":"17874234191554613076"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# READ WEB SCRAPED DATA INTO DATAFRAME AND DISPLAY DETAILS\n","\n","df_raw = pd.read_csv('ETL_P2_model_input_data_v1.csv').drop('Unnamed: 0', axis=1)\n","df_raw['code_description'] = df_raw['code_description'].str.replace(r'\\s+', ' ', regex=True)\n","df_raw['code_snippet'] = df_raw['code_snippet'].str.replace('save(my_dir)AnalysisException:', 'save(my_dir)')\n","df_raw['code_snippet'] = df_raw['code_snippet'].str.replace('my_dir', 'MY_DIR')\n","\n","fn_display_header('Display Dataframe Metadata')\n","df_raw.info()\n","\n","fn_display_header('Display initial Web Scraped Dataframe')\n","#df_raw.style.set_properties(**{'text-align': 'left'})\n","df_raw.head()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":538},"id":"AJTTKnDD3UA8","executionInfo":{"status":"ok","timestamp":1710278084920,"user_tz":-330,"elapsed":786,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"06502c39-796f-4b98-c514-0d952f1e00db"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           Display Dataframe Metadata\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 694 entries, 0 to 693\n","Data columns (total 5 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   code_description  694 non-null    object\n"," 1   code_snippet      694 non-null    object\n"," 2   import_line       62 non-null     object\n"," 3   Category          694 non-null    object\n"," 4   function          694 non-null    object\n","dtypes: object(5)\n","memory usage: 27.2+ KB\n","--------------------------------------------------------------------------------\n","           Display initial Web Scraped Dataframe\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["                                    code_description  \\\n","0  Pyspark code to Write a DataFrame into a CSV file   \n","1  Pyspark code which Loads a CSV file and return...   \n","2  Pyspark code to Read the CSV file as a DataFra...   \n","3  Pyspark code which Loads a CSV file and return...   \n","4  Pyspark code to Write a DataFrame into a JSON ...   \n","\n","                                        code_snippet import_line  \\\n","0  df.write.mode(\"overwrite\").format(\"csv\").save(...         NaN   \n","1  df.write.mode(\"overwrite\").format(\"csv\").save(...         NaN   \n","2  df = spark.read.csv(MY_DIR, schema=df.schema, ...         NaN   \n","3  df = spark.read.csv(MY_DIR, schema=df.schema, ...         NaN   \n","4  df.write.mode(\"overwrite\").format(\"json\").save...         NaN   \n","\n","       Category                            function  \n","0  Input/Output     pyspark.sql.DataFrameReader.csv  \n","1  Input/Output     pyspark.sql.DataFrameReader.csv  \n","2  Input/Output     pyspark.sql.DataFrameReader.csv  \n","3  Input/Output     pyspark.sql.DataFrameReader.csv  \n","4  Input/Output  pyspark.sql.DataFrameReader.format  "],"text/html":["\n","  <div id=\"df-b11554ba-7b44-4514-b1c8-1acf332437a6\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>code_description</th>\n","      <th>code_snippet</th>\n","      <th>import_line</th>\n","      <th>Category</th>\n","      <th>function</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Pyspark code to Write a DataFrame into a CSV file</td>\n","      <td>df.write.mode(\"overwrite\").format(\"csv\").save(...</td>\n","      <td>NaN</td>\n","      <td>Input/Output</td>\n","      <td>pyspark.sql.DataFrameReader.csv</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Pyspark code which Loads a CSV file and return...</td>\n","      <td>df.write.mode(\"overwrite\").format(\"csv\").save(...</td>\n","      <td>NaN</td>\n","      <td>Input/Output</td>\n","      <td>pyspark.sql.DataFrameReader.csv</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Pyspark code to Read the CSV file as a DataFra...</td>\n","      <td>df = spark.read.csv(MY_DIR, schema=df.schema, ...</td>\n","      <td>NaN</td>\n","      <td>Input/Output</td>\n","      <td>pyspark.sql.DataFrameReader.csv</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Pyspark code which Loads a CSV file and return...</td>\n","      <td>df = spark.read.csv(MY_DIR, schema=df.schema, ...</td>\n","      <td>NaN</td>\n","      <td>Input/Output</td>\n","      <td>pyspark.sql.DataFrameReader.csv</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Pyspark code to Write a DataFrame into a JSON ...</td>\n","      <td>df.write.mode(\"overwrite\").format(\"json\").save...</td>\n","      <td>NaN</td>\n","      <td>Input/Output</td>\n","      <td>pyspark.sql.DataFrameReader.format</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b11554ba-7b44-4514-b1c8-1acf332437a6')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-b11554ba-7b44-4514-b1c8-1acf332437a6 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-b11554ba-7b44-4514-b1c8-1acf332437a6');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-1e7c637e-12e5-430a-bcd4-fcb67b086c46\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1e7c637e-12e5-430a-bcd4-fcb67b086c46')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-1e7c637e-12e5-430a-bcd4-fcb67b086c46 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_raw","summary":"{\n  \"name\": \"df_raw\",\n  \"rows\": 694,\n  \"fields\": [\n    {\n      \"column\": \"code_description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 426,\n        \"samples\": [\n          \"Pyspark code which Computes the min value for each numeric column for each group.\",\n          \"Pyspark code which Returns the positive value of dividend mod divisor.\",\n          \"Pyspark code which Returns whether a predicate holds for one or more elements in the array.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"code_snippet\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 603,\n        \"samples\": [\n          \"df.select(add_months(df.dt, 1).alias('next_month')).collect()\",\n          \"df.select(aes_decrypt(aes_encrypt(df.input, df.key, df.mode),    df.key, df.mode).alias('r')).collect()\",\n          \"df.select(sf.ifnull(df.e, sf.lit(8)))\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"import_line\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"import math\",\n          \"from pyspark.sql.streaming.state import GroupStateTimeout\",\n          \"import pyspark.sql.functions as sf\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 14,\n        \"samples\": [\n          \"Bitwise Functions\",\n          \"Predicate Functions\",\n          \"Input/Output\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"function\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 417,\n        \"samples\": [\n          \"pyspark.sql.functions.array_append\",\n          \"pyspark.sql.functions.xpath\",\n          \"pyspark.sql.GroupedData.count\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["<br>\n","<br>\n","<br>\n","\n","----\n","# <b>CODE STANDARDIZATION: DATAFRAME NAMES</b>\n","----\n","<br>"],"metadata":{"id":"1MJDUVTzRfQB"}},{"cell_type":"code","source":["# STANDARDIZATION: DATAFRAME NAMES\n","\n","df_standardize_df_names = df_raw.copy()\n","\n","# Function to standardize dataframe names as MY_DF\n","def fn_standardize_df_names(code, function):\n","  standard_df_name = 'MY_DF'\n","  fn_main = function.split('.')[-1].strip()\n","\n","  code = code.replace('df \\ .', 'df.').replace('rodf.', f'{standard_df_name}.')\n","\n","  replace_str_list1 = re.findall(r'(df[a-zA-Z0-9]*)\\.', code)\n","  replace_str_list1 = [x + '.' for x in replace_str_list1]\n","  replace_str_list1 = list(set(replace_str_list1))\n","\n","  replace_str_list2 = re.findall(r'(df[a-zA-Z0-9]*)\\)\\)', code)\n","  replace_str_list2 = [x + '.' for x in replace_str_list2]\n","  replace_str_list2 = list(set(replace_str_list2))\n","\n","  replace_str_list_final = replace_str_list1 + replace_str_list2\n","  for rpl in replace_str_list_final:\n","    code = code.replace(rpl, f'{standard_df_name}.')\n","\n","  special_str_list = ['df = ', 'df[\"']\n","  for item in special_str_list:\n","    if item in code:\n","      code = code.replace('df', f'{standard_df_name}')\n","\n","  if '_union_' in fn_main and 'df2' in code:\n","    code = code.replace('df1 = ', '').replace('df2 = ', '').replace('df3 = ', '').replace('df4 = ', '').replace('df5 = ', '')\n","    code = code.replace(f'{standard_df_name}.union(df2)', f'{standard_df_name}1.union(MY_DF2)')\n","  if '_sketch_' in fn_main:\n","    code = code.replace('df2 = ', '').replace('df3 = ', '')\n","\n","  result = code\n","  return result\n","\n","\n","# Standardize Dataframe names to MY_DF\n","df_standardize_df_names['code_snippet_1'] = df_raw.apply(lambda x: fn_standardize_df_names(x['code_snippet'], x['function']), axis=1)\n","\n","# UNCOMMENT TO DEBUG\n","df_standardize_df_names[\n","    #(df_standardize_df_names['function'].str.contains('pyspark.sql.DataFrameReader.option'))\n","    (df_standardize_df_names['code_snippet_1'].str.contains('MY_DF.'))\n","    #& (~df_standardize_df_names['code_snippet_1'].str.startswith('MY_DF.'))\n","#].info()\n","#].count()\n","].style.set_properties(**{'text-align': 'left'})\n","\n","\n","#df_standardize_df_names.style.set_properties(**{'text-align': 'left'})\n","df_standardize_df_names.head().style.set_properties(**{'text-align': 'left'})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"PvGoDXBOFzEc","executionInfo":{"status":"ok","timestamp":1710278094142,"user_tz":-330,"elapsed":559,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"43154e16-316d-4451-b7c9-bcdc22c61966"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x791d9bd90370>"],"text/html":["<style type=\"text/css\">\n","#T_3e561_row0_col0, #T_3e561_row0_col1, #T_3e561_row0_col2, #T_3e561_row0_col3, #T_3e561_row0_col4, #T_3e561_row0_col5, #T_3e561_row1_col0, #T_3e561_row1_col1, #T_3e561_row1_col2, #T_3e561_row1_col3, #T_3e561_row1_col4, #T_3e561_row1_col5, #T_3e561_row2_col0, #T_3e561_row2_col1, #T_3e561_row2_col2, #T_3e561_row2_col3, #T_3e561_row2_col4, #T_3e561_row2_col5, #T_3e561_row3_col0, #T_3e561_row3_col1, #T_3e561_row3_col2, #T_3e561_row3_col3, #T_3e561_row3_col4, #T_3e561_row3_col5, #T_3e561_row4_col0, #T_3e561_row4_col1, #T_3e561_row4_col2, #T_3e561_row4_col3, #T_3e561_row4_col4, #T_3e561_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_3e561\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_3e561_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_3e561_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_3e561_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_3e561_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_3e561_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_3e561_level0_col5\" class=\"col_heading level0 col5\" >code_snippet_1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_3e561_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_3e561_row0_col0\" class=\"data row0 col0\" >Pyspark code to Write a DataFrame into a CSV file</td>\n","      <td id=\"T_3e561_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_3e561_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_3e561_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_3e561_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_3e561_row0_col5\" class=\"data row0 col5\" >MY_DF.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_3e561_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_3e561_row1_col0\" class=\"data row1 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_3e561_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_3e561_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_3e561_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_3e561_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_3e561_row1_col5\" class=\"data row1 col5\" >MY_DF.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_3e561_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_3e561_row2_col0\" class=\"data row2 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.</td>\n","      <td id=\"T_3e561_row2_col1\" class=\"data row2 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_3e561_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_3e561_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_3e561_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_3e561_row2_col5\" class=\"data row2 col5\" >MY_DF = spark.read.csv(MY_DIR, schema=MY_DF.schema, nullValue=\"Hyukjin Kwon\")</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_3e561_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_3e561_row3_col0\" class=\"data row3 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_3e561_row3_col1\" class=\"data row3 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_3e561_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_3e561_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_3e561_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_3e561_row3_col5\" class=\"data row3 col5\" >MY_DF = spark.read.csv(MY_DIR, schema=MY_DF.schema, nullValue=\"Hyukjin Kwon\")</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_3e561_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_3e561_row4_col0\" class=\"data row4 col0\" >Pyspark code to Write a DataFrame into a JSON file</td>\n","      <td id=\"T_3e561_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_3e561_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_3e561_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_3e561_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_3e561_row4_col5\" class=\"data row4 col5\" >MY_DF.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["<br>\n","<br>\n","<br>\n","\n","----\n","# <b>CODE STANDARDIZATION: LITERALS</b>\n","----\n","<br>"],"metadata":{"id":"ym2M4Kt9RpxR"}},{"cell_type":"code","source":["# STANDARDIZATION: LITERALS are replaced with a standard prefix LIT_INT, LIT_DEC, LIT_STR.\n","#                  ALIASES are replaced with MY_ALIAS_A, MY_ALIAS_B\n","\n","# Function to standardize literals\n","def fn_standardize_literals(code, function):\n","  literal_prefix = 'LIT'\n","  fn_main = function.split('.')[-1].strip()\n","\n","  lit_match = []\n","  int_common_idx = -1\n","  dec_common_idx = -1\n","  str_common_idx = -1\n","  # Match positive integers\n","  lit_match1 = re.findall(r'lit\\((\\d+?)\\)', code)\n","  for idx, item in enumerate(lit_match1):\n","    int_common_idx = int_common_idx + 1\n","    code = code.replace(f'lit({item})', f'lit({literal_prefix}_INT_{int_common_idx})')\n","\n","  # Match negative integers\n","  lit_match2 = re.findall(r'lit\\(-(\\d+?)\\)', code)\n","  for idx, item in enumerate(lit_match2):\n","    int_common_idx = int_common_idx + 1\n","    code = code.replace(f'lit(-{item})', f'lit({literal_prefix}_INT_{int_common_idx})')\n","\n","  # Match positive Decimals\n","  lit_match3 = re.findall(r'lit\\((\\d+\\.\\d+)\\)', code)\n","  for idx, item in enumerate(lit_match3):\n","    dec_common_idx = dec_common_idx + 1\n","    code = code.replace(f'lit({item})', f'lit({literal_prefix}_DEC_{dec_common_idx})')\n","\n","  # Match negative Decimals\n","  lit_match4 = re.findall(r'lit\\(-(\\d+\\.\\d+)\\)', code)\n","  for idx, item in enumerate(lit_match4):\n","    dec_common_idx = dec_common_idx + 1\n","    code = code.replace(f'lit(-{item})', f'lit({literal_prefix}_DEC_{dec_common_idx})')\n","\n","  # Match strings with patter lit(\"<str>\")\n","  lit_match5 = re.findall(r'lit\\(\"+(\\w+?)\"+\\)', code)\n","  for idx, item in enumerate(lit_match5):\n","    str_common_idx = str_common_idx + 1\n","    code = code.replace(f'lit(\"{item}\")', f'lit(\"{literal_prefix}_STR_{str_common_idx}\")')\n","\n","  # Match strings with patter lit('<str>')\n","  lit_match5 = re.findall(r\"lit\\('+(\\w+?)'+\\)\", code)\n","  for idx, item in enumerate(lit_match5):\n","    str_common_idx = str_common_idx + 1\n","    code = code.replace(f\"lit('{item}')\", f'lit(\"{literal_prefix}_STR_{str_common_idx}\")')\n","\n","  #Handle alias(<>) as alias(\"MY_ALIAS\")\n","  alias_match1 = re.findall(r\"alias\\('+(\\w+?)'+\\)\", code)\n","  if len(alias_match1) == 1:\n","    code = code.replace(f\"alias('{alias_match1[0]}')\", f'alias(MY_ALIAS_A)')\n","  alias_match2 = re.findall(r'alias\\(\"+(\\w+?)\"+\\)', code)\n","  if len(alias_match2) == 1:\n","    code = code.replace(f'alias(\"{alias_match2[0]}\")', f'alias(MY_ALIAS_A)')\n","  if 'alias(' in code:\n","    code = code.replace('alias(\"key\", \"value\"', 'alias(MY_ALIAS_A, MY_ALIAS_B')\n","\n","  if '\"dotNET\", \"Java\"' in code:\n","    code = code.replace('\"dotNET\", \"Java\"', f'\"{literal_prefix}_STR_1\", \"{literal_prefix}_STR_2\"')\n","\n","  num_lit_match = re.findall(r'\\(\\d+?\\)', code)\n","  if len(num_lit_match) == 1:\n","    code = code.replace(f'{num_lit_match[0]}', '(LIT_DEC_1)')\n","\n","  # Special cases\n","  code = code.replace(\"('a', 1)\", \"('a', LIT_INT_1)\")\n","  code = code.replace(\", 2, 16\", \", LIT_INT_1, LIT_INT_2\")\n","  code = code.replace('MY_DF.dt, 1', 'MY_DF.COL_A, LIT_INT_1')\n","  code = code.replace('\"dt\", -1', '\"dt\", LIT_INT_1').replace(\"'dt', -1\", \"'dt', LIT_INT_1\").replace(\"'dt', -2\", \"'dt', LIT_INT_1\").replace('\"dt\", 1', '\"dt\", LIT_INT_1')\n","  code = code.replace('5 seconds', 'LIT_INT_1 seconds')\n","  code = code.replace('MY_DF.data, 1', 'MY_DF.data, LIT_INT_1').replace('MY_DF.data, 3', 'MY_DF.data, LIT_INT_1').replace('MY_DF.data, 5', 'MY_DF.data, LIT_INT_1').replace('MY_DF.data, -1', 'MY_DF.data, LIT_INT_1')\n","  code = code.replace('\"data\", 1', '\"data\", LIT_INT_1').replace('\"data\", -1', '\"data\", LIT_INT_1')\n","  code = code.replace('\"c2\", 2, -1', '\"c2\", LIT_INT_1, LIT_INT_2').replace('\"c2\", 1, 0', '\"c2\", LIT_INT_1, LIT_INT_2').replace('\"c2\", 1', '\"c2\", LIT_INT_1').replace('\"c2\", 2', '\"c2\", LIT_INT_1')\n","  code = code.replace(\"'a', 4\", \"'a', LIT_INT_1\")\n","  code = code.replace(\"'l', 'r', 2\", \"'l', 'r', LIT_INT_1\")\n","  code = code.replace('MY_DF.s, 1, 2', 'MY_DF.s, LIT_INT_1, LIT_INT_2')\n","  code = code.replace('MY_DF.s, 1', 'MY_DF.s, LIT_INT_1').replace('MY_DF.s, 6', 'MY_DF.s, LIT_INT_1').replace(\"MY_DF.s, '.', 2\", \"MY_DF.s, '.', LIT_INT_1\").replace(\"MY_DF.s, '.', -3\", \"MY_DF.s, '.', LIT_INT_1\")\n","  code = code.replace('\"x\", \"y\", 7, 0', '\"x\", \"y\", LIT_INT_1, LIT_INT_2').replace('\"x\", \"y\", 7, 2', '\"x\", \"y\", LIT_INT_1, LIT_INT_2').replace('\"x\", \"y\", 7', '\"x\", \"y\", LIT_INT_1')\n","  code = code.replace(', \"rnlt\", \"123\"', ', \"rnlt\", \"LIT_STR_1\"')\n","  code = code.replace('MY_DF.x, 2, 2', 'MY_DF.x, LIT_INT_1, LIT_INT_2')\n","  code = code.replace(\"MY_DF.s, 'b'\", \"MY_DF.COL_A, LIT_STR_1\").replace('MY_DF.s, 3', 'MY_DF.s, LIT_INT_1')\n","  code = code.replace(\"'b', MY_DF.COL_A, 1\", \"'LIT_STR_1', MY_DF.COL_A, LIT_INT_1\")\n","  code = code.replace('raise_error(\"My error message\"', 'raise_error(\"MY_ERROR_MSG\"')\n","\n","\n","  lit_match = lit_match1 + lit_match2 + lit_match3 + lit_match4 + lit_match5 + alias_match1 + alias_match2\n","\n","  #if len(lit_match) > 0 and function == 'pyspark.sql.functions.sign':\n","  #  print(f'--DEBUG: lit_match1:{lit_match1} -- lit_match2:{lit_match2} -- lit_match3:{lit_match3} -- lit_match4:{lit_match4} -- ')\n","\n","  result = [code, lit_match]\n","  return result\n","\n","# Standardize Dataframe names to MY_DF\n","df_standardize_literals = df_standardize_df_names.copy()\n","#df_standardize_literals['literals'] = df_standardize_literals.apply(lambda x: fn_standardize_literals(x['code_snippet_1'], x['function'])[1], axis=1)  # ENABLE FOR DEBUGGING ONLY\n","df_standardize_literals['code_snippet_1'] = df_standardize_literals.apply(lambda x: fn_standardize_literals(x['code_snippet_1'], x['function'])[0], axis=1)\n","\n","# UNCOMMENT TO DEBUG\n","\n","df_standardize_literals[\n","    #(~df_standardize_literals['code_snippet_1'].str.contains('LIT_'))\n","    #(df_standardize_literals['code_snippet_1'].str.contains('alias\\('))\n","    #(df_standardize_literals['code_snippet_1'].str.contains(r'\"(\\w+?)\"'))\n","    #(df_standardize_literals['code_snippet'].str.contains(r'\\(\\d+?\\)'))\n","    (df_standardize_literals['code_snippet'].str.contains(r'\\d+?'))\n","    & (~df_standardize_literals['function'].str.contains(r'DataFrameReader'))\n","    & (~df_standardize_literals['function'].str.contains(r'DataFrameWriter'))\n","    #(df_standardize_literals['code_snippet'].str.contains(r'lit\\(\"(\\w+?)\"\\)'))\n","    #(df_standardize_literals['code_snippet'].str.contains(r\"lit\\('(\\w+?)'\\)\"))\n","    #(df_standardize_literals['code_snippet_1'].str.contains('lit\\('))\n","#].info()\n","#].count()\n","].style.set_properties(**{'text-align': 'left'})\n","\n","#df_standardize_literals.style.set_properties(**{'text-align': 'left'})\n","df_standardize_literals.head().style.set_properties(**{'text-align': 'left'})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"MIN1ZxGCS2iD","executionInfo":{"status":"ok","timestamp":1710278111276,"user_tz":-330,"elapsed":78,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"62591651-c427-47fb-b103-b4f7c789300b"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x791d94203460>"],"text/html":["<style type=\"text/css\">\n","#T_a5e3b_row0_col0, #T_a5e3b_row0_col1, #T_a5e3b_row0_col2, #T_a5e3b_row0_col3, #T_a5e3b_row0_col4, #T_a5e3b_row0_col5, #T_a5e3b_row1_col0, #T_a5e3b_row1_col1, #T_a5e3b_row1_col2, #T_a5e3b_row1_col3, #T_a5e3b_row1_col4, #T_a5e3b_row1_col5, #T_a5e3b_row2_col0, #T_a5e3b_row2_col1, #T_a5e3b_row2_col2, #T_a5e3b_row2_col3, #T_a5e3b_row2_col4, #T_a5e3b_row2_col5, #T_a5e3b_row3_col0, #T_a5e3b_row3_col1, #T_a5e3b_row3_col2, #T_a5e3b_row3_col3, #T_a5e3b_row3_col4, #T_a5e3b_row3_col5, #T_a5e3b_row4_col0, #T_a5e3b_row4_col1, #T_a5e3b_row4_col2, #T_a5e3b_row4_col3, #T_a5e3b_row4_col4, #T_a5e3b_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_a5e3b\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_a5e3b_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_a5e3b_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_a5e3b_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_a5e3b_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_a5e3b_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_a5e3b_level0_col5\" class=\"col_heading level0 col5\" >code_snippet_1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_a5e3b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_a5e3b_row0_col0\" class=\"data row0 col0\" >Pyspark code to Write a DataFrame into a CSV file</td>\n","      <td id=\"T_a5e3b_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_a5e3b_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_a5e3b_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_a5e3b_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_a5e3b_row0_col5\" class=\"data row0 col5\" >MY_DF.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_a5e3b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_a5e3b_row1_col0\" class=\"data row1 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_a5e3b_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_a5e3b_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_a5e3b_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_a5e3b_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_a5e3b_row1_col5\" class=\"data row1 col5\" >MY_DF.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_a5e3b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_a5e3b_row2_col0\" class=\"data row2 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.</td>\n","      <td id=\"T_a5e3b_row2_col1\" class=\"data row2 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_a5e3b_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_a5e3b_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_a5e3b_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_a5e3b_row2_col5\" class=\"data row2 col5\" >MY_DF = spark.read.csv(MY_DIR, schema=MY_DF.schema, nullValue=\"Hyukjin Kwon\")</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_a5e3b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_a5e3b_row3_col0\" class=\"data row3 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_a5e3b_row3_col1\" class=\"data row3 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_a5e3b_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_a5e3b_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_a5e3b_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_a5e3b_row3_col5\" class=\"data row3 col5\" >MY_DF = spark.read.csv(MY_DIR, schema=MY_DF.schema, nullValue=\"Hyukjin Kwon\")</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_a5e3b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_a5e3b_row4_col0\" class=\"data row4 col0\" >Pyspark code to Write a DataFrame into a JSON file</td>\n","      <td id=\"T_a5e3b_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_a5e3b_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_a5e3b_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_a5e3b_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_a5e3b_row4_col5\" class=\"data row4 col5\" >MY_DF.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["<br>\n","<br>\n","<br>\n","\n","----\n","# <b>CODE STANDARDIZATION: COLUMN NAMES</b>\n","----\n","<br>"],"metadata":{"id":"2u6uhciiSIYO"}},{"cell_type":"code","source":["# STANDARDIZATION: Standardize random column names to COL_A, COL_B, COL_C, COL_D, COL_E\n","\n","# Function to standardize column names\n","def fn_standardize_col_names(code, function, level):\n","  standard_df_name = 'MY_DF'\n","  ignore_prefix = code[:6]\n","  code = code[6:]\n","  fn_main = function.split('.')[-1].strip()\n","\n","  #print(f'-- DEBUG --: ignore_prefix: {ignore_prefix} -- code: {code}')\n","\n","  # Call function with level 3 argument to replace triple column list first\n","  if level == 3:\n","\n","    # Replace columns in special cases frst\n","    spl_fn = ['first', 'last', 'max_by', 'median', 'min_by', 'mode', 'hex', 'sum', 'agg', 'explode_outer', 'posexplode_outer', 'grouping', 'applyInPandas', 'pivot', 'avg', 'min', 'max', 'sum']\n","    spl_cols = ['name',    'age', 'course',  'year', 'earnings', 'height',    'id', 'a_map', 'an_array',   \"'b'\",   'val', 'year', 'course', 'earnings', 'sales.year', 'sales.course', 'sales.earnings']\n","    spl_rplc = ['COL_A', 'COL_B',  'COL_C', 'COL_D',    'COL_E',  'COL_C', 'COL_A', 'COL_B',    'COL_C', 'COL_B', 'COL_B', 'COL_A', 'COL_B',    'COL_C',      'COL_A',        'COL_B',          'COL_C']\n","    if fn_main in spl_fn \\\n","      and (\n","        'groupBy' in code or 'explode_outer' in code or 'grouping' in code or f\"{fn_main}(\" in code\n","      ):\n","      #if fn_main == 'first':\n","      #  print(f\" --- code: {code}\")\n","      for item, replacement in zip(spl_cols, spl_rplc):\n","        if 'MY_DF.' in code:\n","          code = code.replace(f'MY_DF.{item}', f'MY_DF.{replacement}')\n","        else:\n","          code = code.replace(f'\"{item}\"', f'\"{replacement}\"')\n","\n","    code = code.replace('\"COL_A\", lead(\"c2\"', '\"next_value\", lead(\"COL_B\"')\n","    code = code.replace('unbase64(MY_DF.input), MY_DF.key, MY_DF.mode', 'unbase64(MY_DF.COL_A), MY_DF.COL_B, MY_DF.COL_C')\n","    code = code.replace(\"MY_DF.data, MY_DF.pos.cast('integer'), MY_DF.val\", \"MY_DF.COL_A, MY_DF.COL_B.cast('integer'), MY_DF.COL_C\")\n","    code = code.replace('unbase64(MY_DF.input), MY_DF.key, MY_DF.mode, MY_DF.padding, MY_DF.aad', 'unbase64(MY_DF.COL_A), MY_DF.COL_B, MY_DF.COL_C, MY_DF.COL_D, MY_DF.COL_E')\n","    code = code.replace('unbase64(MY_DF.input), MY_DF.key, MY_DF.mode, MY_DF.padding', 'unbase64(MY_DF.COL_A), MY_DF.COL_B, MY_DF.COL_C, MY_DF.COL_D')\n","    code = code.replace('unhex(MY_DF.input), MY_DF.key', 'unhex(MY_DF.COL_A), MY_DF.COL_B')\n","    code = code.replace('MY_DF.id, ceil(MY_DF.v', 'MY_DF.COL_A, ceil(MY_DF.COL_B')\n","    code = code.replace('make_interval(MY_DF.a), MY_DF.b)', 'make_interval(MY_DF.COL_A), MY_DF.COL_B)')\n","    code = code.replace('MY_DF.d, make_interval(MY_DF.i)', 'MY_DF.COL_A, make_interval(MY_DF.COL_B)')\n","    code = code.replace(\"any_value('c1'), any_value('c2')\", \"any_value('COL_A'), any_value('COL_B')\").replace(\"any_value('c1', True), any_value('c2'\", \"any_value('COL_A', True), any_value('COL_B'\")\n","    code = code.replace('count_distinct(MY_DF.value, MY_DF.value)', 'count_distinct(MY_DF1.COL_A, MY_DF2.COL_A)')\n","    code = code.replace('\"previos_value\", lag(\"c2', '\"COL_A\", lag(\"COL_B')\n","    code = code.replace(\"unbase64(MY_DF.input), MY_DF.key, MY_DF.mode, MY_DF.padding, MY_DF.aad\", \"unbase64(MY_DF.COL_A), MY_DF.COL_B, MY_DF.COL_C, MY_DF.COL_D, MY_DF.COL_E\")\n","    code = code.replace(\"unbase64(MY_DF.input), MY_DF.key, MY_DF.mode, MY_DF.padding\", \"unbase64(MY_DF.COL_A), MY_DF.COL_B, MY_DF.COL_C, MY_DF.COL_D\")\n","    code = code.replace(\"hex('a'), hex('b')\", \"hex('COL_A'), hex('COL_B')\")\n","    code = code.replace(\"sf.last_value('a'), sf.last_value('b')\", \"sf.last_value('COL_A'), sf.last_value(COL_B')\").replace(\"sf.last_value('a', True), sf.last_value('b'\", \"sf.last_value('COL_A', True), sf.last_value('COL_B'\")\n","    code = code.replace(\"'id', inline_outer(MY_DF.structlist)\", \"'COL_A', inline_outer(MY_DF.COL_B)\")\n","    code = code.replace(\"(make_interval(MY_DF.i), make_interval(MY_DF.i)\", \"(make_interval(MY_DF.COL_A), make_interval(MY_DF.COL_B)\")\n","    code = code.replace(\"MY_DF.dt, MY_DF.add.cast('integer')\", \"MY_DF.COL_A, MY_DF.COL_B.cast('integer')\")\n","\n","\n","    date_columns = ['MY_DF.year', 'MY_DF.month', 'MY_DF.week', 'MY_DF.day', 'MY_DF.hour', 'MY_DF.min', 'MY_DF.sec']\n","    for item in date_columns:\n","      if item in code:\n","        code = code.replace(item, item.upper())\n","    code = code.replace('df.Y, df.M, df.D', 'df.YEAR, df.MONTH, df.DAY')\n","\n","    triple_col_list = [x.replace('df.', f'{standard_df_name}.') for x in [\n","        'df.a, df.b, df.c', \"df.vals1, df.vals2, df.vals3\", \"df.COL_A, df.COL_B, df.COL_C\", \"'C1', 'C2', 'C3'\"\n","        ]]\n","    #print(f'-- DEBUG --:triple_col_list: {triple_col_list}')\n","    for item in triple_col_list:\n","      if 'df.' in item:\n","        code = code.replace(item, f'{standard_df_name}.COL_A, {standard_df_name}.COL_B, {standard_df_name}.COL_C')\n","      else:\n","        code = code.replace(item, f'\"COL_A\", \"COL_B\", \"COL_C\"')\n","\n","  # Call function with level 2 argument to replace double column list after level 3 processing\n","  if level == 2:\n","    double_col_list = [x.replace('df.', f'{standard_df_name}.') for x in [\n","        'df.age, df.age', 'df.name, df.age', 'df.c1, df.c2', 'df.a, df.b, df.c', 'df.age, df.name', '\"a\", \"b\"', 'df.a, df.b', 'df.x, df.y',\n","        'df.k, df.v', 'unhex(df.input), df.key', 'df.s, df.d', 'df.data, \"a\"', 'df.ts, df.tz', 'df.d2, df.d1', 'df.birth, df.age', \"df.c1, 'x'\",\n","        \"'age', 'age'\", \"'name', 'age'\", \"'age', 'name'\", '\"age\", \"height\"', \"'age', 'height'\", '\"x\", \"y\"', '\"y\", \"x\"', '\"map1\", \"map2\"', 'df.date1, df.date2',\n","      'col(\"pattern\"), col(\"replacement\")', 'df.birth, df.age', 'df.d, df.i', \"'c1', 'c2'\", \"'l', 'r'\", '\"c2\", \"c3\"', \"'C1', 'C2'\", '\"xs\", \"ys\"', 'df.dt, df.sub'\n","        ]]\n","    #print(f'-- DEBUG --:double_col_list: {double_col_list}')\n","    for item in double_col_list:\n","      if 'df.' in item:\n","        code = code.replace(item, f'{standard_df_name}.COL_A, {standard_df_name}.COL_B')\n","      elif 'col(' in item:\n","          code = code.replace(item, 'col(\"COL_A\"), col(\"COL_B\")')\n","      else:\n","        code = code.replace(item, '\"COL_A\", \"COL_B\"')\n","\n","  # Call function with level 2 argument to replace double column list after level 2 processing\n","  if level == 1:\n","    single_col_list1 = [x.replace('df', f'{standard_df_name}') for x in [\n","        'df[\"numbers\"]', 'df.value', 'df.alphabets', 'df.input', 'df.string', 'df.name', 'df.data', 'df.id', #'df.schema',\n","        'df.a', 'df.b', 'df.c', 'df.c1', 'df.d', 'df.e', 'df.n', 'df.s', 'df.t', 'df.v', 'df.x', 'df.y'\n","        'df[\"id\"]', 'col(\"id\")', \"col('c2')\", 'col(\"value\")', 'col(\"sketch\")', 'col(\"numbers\")', 'col(\"regexp\")',\n","        'df.ts', 'df.mapfield', 'df.padding', 'df.aad', 'df.intlist', 'df.unix_time', 'df.dt', 'df.structlist'\n","        ]]\n","    #print(f'-- DEBUG --:single_col_list: {single_col_list}')\n","    for item in single_col_list1:\n","      for delim in [')', ',', ' ']:\n","        if f'{standard_df_name}[' in item:\n","          code = code.replace(f'{item}{delim}', f'{standard_df_name}[\"COL_A\"]{delim}')\n","        elif f'{standard_df_name}.' in item:\n","          code = code.replace(f'{item}{delim}', f'{standard_df_name}.COL_A{delim}')\n","        elif 'col(' in item:\n","          code = code.replace(f'{item}{delim}', f'col(\"COL_A\"){delim}')\n","\n","    # Match single occurrences of columns manually\n","    single_col_list2 = [\n","        'id', 'a', 'c', 's', 'v', 'c1', 'c2', 'd2', 'cd', 'age', 'name', 'dt', 'date', 'values', 'value', 'val', 'data', 'desc_order', 'drank', 'pr', 'ntile', 'flag', 'distinct_cnt', 'sha2', 'str', 'cat', 'unix_time', 'ts',\n","        ]\n","    single_occ_columns = [f'\"{x}\"' for x in single_col_list2]\n","    single_occ_columns = single_occ_columns + [f\"'{x}'\" for x in single_col_list2]\n","    single_occ_ignore_fns = ['array_contains', 'array_position', 'sum', 'min', 'max', 'avg']\n","    for idx, item in enumerate(single_occ_columns):\n","      item_match = re.findall(item, code)\n","      if item in code and len(item_match) == 1 and fn_main not in ['array_contains', 'array_position', 'sum', 'min', 'max', 'avg']:\n","        code = code.replace(item, '\"COL_A\"')\n","\n","  if fn_main == 'applyInPandas':\n","    code = code.replace('groupby(\"id\"', 'groupby(\"COL_A\"')\n","  if fn_main == 'get':\n","    code = code.replace('col(\"index\")', 'col(\"COL_B\")')\n","    code = code.replace('\"index\"', '\"LIT_STR_0\"')\n","\n","  # Last minute fix for some replacements which did not work with generic code\n","  code = code.replace('age', 'COL_B') if fn_main == 'avg' else code\n","  code = code.replace('MY_DF.COL_A < MY_DF.COL_A', 'MY_DF.COL_A < MY_DF.COL_B').replace('MY_DF.COL_A > MY_DF.COL_A', 'MY_DF.COL_A > MY_DF.COL_B')   if fn_main == 'assert_true' else code\n","  code = code.replace('Hyukjin Kwon', 'LIT_STR_0')                                                                                                  if 'DataFrameWriter' in function or 'DataFrameReader' in function else code\n","  code = code.replace('MY_DF.Y, MY_DF.M, MY_DF.D', 'MY_DF.YEAR, MY_DF.MONTH, MY_DF.DAY')                                                            if fn_main == 'make_date' else code\n","  code = code.replace('substr(\"COL_A\", \"COL_B\", \"COL_A\")', 'substr(\"COL_A\", \"COL_B\", \"COL_C\")')                                                     if fn_main == 'substr' else code\n","  code = code.replace('\"COL_A\", \"COL_B\"', '\"COL_B\", \"COL_C\"')                                                                                       if fn_main == 'grouping_id' else code\n","  code = code.replace('col(\"COL_A\"), col(\"COL_B\")', 'col(\"COL_B\"), col(\"COL_C\")')                                                                   if fn_main == 'regexp_replace' else code\n","  code = code.replace('\"rnlt\"', '\"LIT_STR_0\"')                                                                                                      if fn_main == 'translate' else code\n","  code = code.replace('\"COL_B\", \"COL_A\"', '\"COL_B\", \"COL_C\"')                                                                                       if fn_main == 'printf' else code\n","\n","  code = code.replace('MY_DF.COL_C, MY_DF.COL_A', 'MY_DF.COL_C, MY_DF.COL_D')                                                                       if fn_main == 'aes_decrypt' else code\n","  code = code.replace('MY_DF.COL_C, MY_DF.COL_D, MY_DF.COL_A', 'MY_DF.COL_C, MY_DF.COL_D, MY_DF.COL_E')                                             if fn_main == 'aes_decrypt' else code\n","  code = code.replace('MY_DF.COL_C, MY_DF.COL_A', 'MY_DF.COL_C, MY_DF.COL_D')                                                                       if fn_main == 'try_aes_decrypt' else code\n","  code = code.replace('MY_DF.COL_C, MY_DF.COL_D, MY_DF.COL_A', 'MY_DF.COL_C, MY_DF.COL_D, MY_DF.COL_E')                                             if fn_main == 'try_aes_decrypt' else code\n","  code = code.replace(\"'COL_B', 'height'\", \"'COL_B', 'COL_A'\")                                             if fn_main == 'avg' else code\n","\n","  result = ignore_prefix +  code\n","  return result\n","\n","df_standardize_columns = df_standardize_literals.copy()\n","\n","# Standardize Column names calling the function with level 3 first then 2 and then 1\n","df_standardize_columns['code_snippet_1'] = df_standardize_columns.apply(lambda x: fn_standardize_col_names(x['code_snippet_1'], x['function'], 3), axis=1)\n","df_standardize_columns['code_snippet_1'] = df_standardize_columns.apply(lambda x: fn_standardize_col_names(x['code_snippet_1'], x['function'], 2), axis=1)\n","df_standardize_columns['code_snippet_1'] = df_standardize_columns.apply(lambda x: fn_standardize_col_names(x['code_snippet_1'], x['function'], 1), axis=1)\n","\n","\n","# UNCOMMENT TO DEBUG\n","df_standardize_columns[\n","    #(df_standardize_columns['function'].str.contains('pyspark.sql.DataFrameReader.option'))\n","    #(df_standardize_columns['code_snippet_1'].str.contains('col\\('))\n","    #(~df_standardize_columns['code_snippet_1'].str.contains('COL_A'))\n","    #(df_standardize_columns['code_snippet_1'].str.contains('COL_A'))\n","    #(~df_standardize_columns['function'].str.contains(r'DataFrameReader'))\n","    #& (~df_standardize_columns['function'].str.contains(r'DataFrameWriter'))\n","    (df_standardize_columns['code_snippet_1'].str.contains('group'))\n","    #& (~df_standardize_columns['code_snippet_1'].str.startswith('MY_DF.'))\n","#].info()\n","#].count()\n","].style.set_properties(**{'text-align': 'left'})\n","\n","#df_standardize_columns.style.set_properties(**{'text-align': 'left'})\n","df_standardize_columns.head().style.set_properties(**{'text-align': 'left'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"c3aFBaV_kz-z","executionInfo":{"status":"ok","timestamp":1710278126570,"user_tz":-330,"elapsed":897,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"1a077133-5fc5-4636-93c8-680d2d238456"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x791d941f91b0>"],"text/html":["<style type=\"text/css\">\n","#T_9fd35_row0_col0, #T_9fd35_row0_col1, #T_9fd35_row0_col2, #T_9fd35_row0_col3, #T_9fd35_row0_col4, #T_9fd35_row0_col5, #T_9fd35_row1_col0, #T_9fd35_row1_col1, #T_9fd35_row1_col2, #T_9fd35_row1_col3, #T_9fd35_row1_col4, #T_9fd35_row1_col5, #T_9fd35_row2_col0, #T_9fd35_row2_col1, #T_9fd35_row2_col2, #T_9fd35_row2_col3, #T_9fd35_row2_col4, #T_9fd35_row2_col5, #T_9fd35_row3_col0, #T_9fd35_row3_col1, #T_9fd35_row3_col2, #T_9fd35_row3_col3, #T_9fd35_row3_col4, #T_9fd35_row3_col5, #T_9fd35_row4_col0, #T_9fd35_row4_col1, #T_9fd35_row4_col2, #T_9fd35_row4_col3, #T_9fd35_row4_col4, #T_9fd35_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_9fd35\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_9fd35_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_9fd35_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_9fd35_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_9fd35_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_9fd35_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_9fd35_level0_col5\" class=\"col_heading level0 col5\" >code_snippet_1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_9fd35_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_9fd35_row0_col0\" class=\"data row0 col0\" >Pyspark code to Write a DataFrame into a CSV file</td>\n","      <td id=\"T_9fd35_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_9fd35_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_9fd35_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_9fd35_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_9fd35_row0_col5\" class=\"data row0 col5\" >MY_DF.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_9fd35_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_9fd35_row1_col0\" class=\"data row1 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_9fd35_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_9fd35_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_9fd35_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_9fd35_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_9fd35_row1_col5\" class=\"data row1 col5\" >MY_DF.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_9fd35_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_9fd35_row2_col0\" class=\"data row2 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.</td>\n","      <td id=\"T_9fd35_row2_col1\" class=\"data row2 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_9fd35_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_9fd35_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_9fd35_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_9fd35_row2_col5\" class=\"data row2 col5\" >MY_DF = spark.read.csv(MY_DIR, schema=MY_DF.schema, nullValue=\"LIT_STR_0\")</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_9fd35_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_9fd35_row3_col0\" class=\"data row3 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_9fd35_row3_col1\" class=\"data row3 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_9fd35_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_9fd35_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_9fd35_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_9fd35_row3_col5\" class=\"data row3 col5\" >MY_DF = spark.read.csv(MY_DIR, schema=MY_DF.schema, nullValue=\"LIT_STR_0\")</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_9fd35_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_9fd35_row4_col0\" class=\"data row4 col0\" >Pyspark code to Write a DataFrame into a JSON file</td>\n","      <td id=\"T_9fd35_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_9fd35_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_9fd35_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_9fd35_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_9fd35_row4_col5\" class=\"data row4 col5\" >MY_DF.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["<br>\n","<br>\n","<br>\n","\n","----\n","# <b>CODE/COMMENT STANDARDIZATION: PASS 1</b>\n","----\n","<br>"],"metadata":{"id":"ajWz2XfESPqG"}},{"cell_type":"code","source":["# HANDLE COMMENTS PASS 1: Manipulate comments to incorporate standards introduced in code\n","#                         Handles Zero or single occurrences of COL or LITERAL  i.e. functions passing 1 argument\n","\n","lit_search_id_dict = {\n","  'LIT_INT_0': 'integer',\n","  'LIT_INT_1': 'integer',\n","  'LIT_INT_2': 'integer',\n","  'LIT_INT_3': 'integer',\n","  'LIT_INT_4': 'integer',\n","  'LIT_INT_5': 'integer',\n","  'LIT_DEC_0': 'float or decimal',\n","  'LIT_DEC_1': 'float or decimal',\n","  'LIT_DEC_2': 'float or decimal',\n","  'LIT_DEC_3': 'float or decimal',\n","  'LIT_DEC_4': 'float or decimal',\n","  'LIT_DEC_5': 'float or decimal',\n","  'LIT_STR_0': 'string',\n","  'LIT_STR_1': 'string',\n","  'LIT_STR_2': 'string',\n","  'LIT_STR_3': 'string',\n","  'LIT_STR_4': 'string',\n","  'LIT_STR_5': 'string',\n","}\n","\n","delim_dict = {\n","  'array_join': '\",\"'\n","}\n","\n","# Function to handle comments for referenes to MY_DIR and LITERALS\n","def fn_standardize_comments_1(comment, code, function, category):\n","  fn_main = function.split('.')[-1].strip()\n","  filter_str = '=NA='\n","  replaced_flag = 'N'\n","\n","  # Handle some special cases for comments\n","  if not comment.strip().endswith('.'):\n","    comment = comment + '.'\n","\n","  comment = comment.replace('Aggregate function', '').replace('Collection function', '').replace('Window function', '')\n","  comment = comment.replace('sensitiveinformation', 'sensitive information').replace('partitionsby', 'partitions by').replace('valuethen', 'value then').replace('thegiven', 'the given').replace('andis', 'and is')\n","  comment = comment.replace('NULLvalue', 'NULL value').replace('andkey', 'and key').replace('thearray', 'the array').replace('ofthe', 'of the').replace('givencolumn', 'given column').replace('andreturns', 'and returns')\n","  comment = comment.replace('0or', '0 or').replace('returnsa', 'returns a').replace('representationof', 'representation of').replace('fallafter', 'fall after').replace('dateformat', 'date format').replace('aswell', 'as well')\n","  comment = comment.replace('columnbased', 'column based').replace('withnull', 'with null').replace('valuein', 'value in').replace('nullvalue', 'null value').replace('anddefault', 'and default').replace('arraymust', 'array must')\n","  comment = comment.replace('functionreturns', 'function returns').replace('arraymust', 'array must').replace('allN th', 'all N-th').replace('resultby', 'result by').replace('countof', 'count of').replace('exceptionwith', 'exception with')\n","  comment = comment.replace('mapof', 'map of').replace('which This', 'which ').replace('numberand', 'number and').replace('Euler s', 'Eulers').replace('descendingorder', 'descending order').replace('functiontakes', 'function takes')\n","  comment = comment.replace('stringrepresenting', 'string representing').replace('defaultlocale', 'default locale').replace('timestampsto', 'timestamps to').replace('matchedin', 'matched in').replace('resultas', 'result as')\n","  comment = comment.replace('definedper', 'defined per').replace('urlencoded', 'url encoded')\n","\n","  comment_orig = comment\n","\n","  if fn_main in delim_dict:\n","    comment = comment.replace('delimiter', 'delimiter ' + delim_dict[fn_main])\n","\n","  # Standardizes comments for DataFrameWriter and DataFrameReader functions\n","  if 'DataFrameWriter' in function or 'DataFrameReader' in function:\n","    comment = comment.replace('Hyukjin Kwon', 'LIT_STR_0')\n","    if 'MY_DIR' not in comment and 'MY_DIR' in code:\n","      comment = (comment + 'The data is written to file or directory MY_DIR')  if '.write' in code else comment\n","      comment = (comment + 'The data is read from file or directory MY_DIR')  if '.read' in code else comment\n","    filter_str = '1_MY_DIR'\n","    replaced_flag = 'Y'\n","\n","  # search code for occurrences of standardized column names and literals\n","  arg_match = re.findall(r\"(LIT_(INT|DEC|STR)_\\d+|COL_[A-Z]|YEAR|MONTH|DAY|HOUR|MIN|SEC)\", code)\n","  arg_match = list(dict.fromkeys([match[0] for match in arg_match])) # list(set([match[0] for match in arg_match]))\n","\n","  arg_replace_list = []\n","\n","  for item in arg_match:\n","    if 'COL_' in item or item in ['YEAR', 'MONTH', 'DAY', 'HOUR', 'MIN', 'SEC']:\n","      arg_replace_list.append(f\"column {item}\")\n","    else:\n","      arg_replace_list.append(f\"{lit_search_id_dict[item]} value {item}\")\n","\n","  df_suffix_str = \" in dataframe MY_DF\" if 'COL_' in code else ''\n","\n","  if len(arg_match) == 1 and replaced_flag == 'N': # and 'DataFrameWriter' not in function and 'DataFrameReader' not in function:\n","    comment = comment.replace('given columns', 'column')\n","    filter_str = '1_INIT'\n","\n","    if 'float' not in fn_main:\n","      comment = comment.replace('float', ' ')\n","    if fn_main == 'hash':\n","      comment = comment.replace('Pyspark code which Two or more columns', '.')\n","\n","    # Handles explode functions and replaces with the appropriate standard\n","    if ('explode' in fn_main or fn_main == 'inline') and replaced_flag == 'N':\n","      comment = re.sub('array or map', f\"array or map given by {arg_replace_list[0]}{df_suffix_str}\", comment, 1).split('.')[0]\n","      comment = re.sub('array of structs', f\"array of structs given by {arg_replace_list[0]}{df_suffix_str}\", comment, 1).split('.')[0]\n","      filter_str = '1_fn_explode_1'\n","      replaced_flag = 'Y'\n","    # Checks for the occurrence of the word column and replaces with the appropriate standard\n","    elif ('column' in comment.lower() or ' col ' in comment or ' col.' in comment) and replaced_flag == 'N':\n","      comment = comment.replace('numeric columns for each group', 'numeric column for each group')\n","      comment = comment.replace(' col ', ' column ').replace('col.', 'column.')\n","\n","      comment = re.sub('input column', f'input column {arg_replace_list[0]}{df_suffix_str}', comment, 1)\n","      comment = re.sub(r'column ', f\"column {arg_replace_list[0]}{df_suffix_str} \", comment, 1)                   if f\"{arg_replace_list[0]}\" not in comment else comment\n","      comment = re.sub(r'column\\.', f\"column {arg_replace_list[0]}{df_suffix_str}.\", comment, 1)                  if f\"{arg_replace_list[0]}\" not in comment else comment\n","\n","      comment = comment.replace('column column', 'column')\n","      comment = comment.replace('column integer', 'integer').replace('column float', 'float').replace('column string', 'string')\n","      comment = comment.replace('MY_DF col', 'MY_DF').replace('MY_DF name', 'MY_DF').replace('MY_DF id', 'MY_DF')\n","      comment = comment.replace('Column', f'{arg_replace_list[0]}{df_suffix_str}').replace('pyspark.sql', 'to the default pyspark TimestampType.')\n","      filter_str = '1_column_1'\n","      replaced_flag = 'Y'\n","    # Checks for the occurrence of the word value and replaces with the appropriate standard\n","    elif 'value' in comment and replaced_flag == 'N':\n","      if 'value.' in comment:\n","        comment = re.sub('value.', f\"value of {arg_replace_list[0]}{df_suffix_str}.\", comment, 1)                 if 'COL_' in arg_replace_list[0] else comment\n","        comment = re.sub('value.', f\"{arg_replace_list[0]}{df_suffix_str}.\", comment, 1)                          if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = comment.replace('value', 'positive value')                                                      if fn_main == 'positive' else comment\n","        filter_str = '1_value_1'\n","      elif 'given value' in comment:\n","        comment = re.sub('given value', f\"given value of {arg_replace_list[0]}{df_suffix_str}\", comment, 1)       if f\"{arg_replace_list[0]}\" not in comment else comment\n","        filter_str = '1_value_2'\n","      elif 'values' in comment:\n","        comment = re.sub('in a map', f\"in a map given by input {arg_replace_list[0]}{df_suffix_str}\", comment, 1) if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('col ', f\"{arg_replace_list[0]}{df_suffix_str} \", comment, 1)                            if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('col.', f\"{arg_replace_list[0]}{df_suffix_str}.\", comment, 1)                            if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('values of a group', f\"values in {arg_replace_list[0]}{df_suffix_str}\", comment, 1)      if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('values in a group', f\"values in {arg_replace_list[0]}{df_suffix_str}\", comment, 1)      if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('values', f\"values in {arg_replace_list[0]}{df_suffix_str}\", comment, 1)                 if f\"{arg_replace_list[0]}\" not in comment else comment\n","        filter_str = '1_value_3'\n","      elif 'value' in comment:\n","        comment =comment.replace('and returns the value as', 'value and returns')\n","        comment = re.sub('a predicate', f\"a predicate for each element in {arg_replace_list[0]}{df_suffix_str}\", comment, 1)    if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('being evaluated', f\"being evaluated for {arg_replace_list[0]}{df_suffix_str}\", comment, 1)            if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('timestamp string', f\"timestamp string in {arg_replace_list[0]}{df_suffix_str}\", comment, 1)           if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('string value', f\"string value in {arg_replace_list[0]}{df_suffix_str} using\", comment, 1)             if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('224 SHA 256 SHA 384 and SHA 512', f\"256 for {arg_replace_list[0]}{df_suffix_str} using\", comment, 1)  if f\"{arg_replace_list[0]}\" not in comment else comment\n","\n","        comment = re.sub('current row', f\"current row in {arg_replace_list[0]}{df_suffix_str} using\", comment, 1) if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('input using', f\"input {arg_replace_list[0]}{df_suffix_str} using\", comment, 1)          if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('bucket number', f\"bucket number of {arg_replace_list[0]}{df_suffix_str}\", comment, 1)   if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('the array', f\"{arg_replace_list[0]}{df_suffix_str}\", comment, 1)                        if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('value of string', f\"value of {arg_replace_list[0]}{df_suffix_str}\", comment, 1)         if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('expression in a group', f\"{arg_replace_list[0]}{df_suffix_str}\", comment, 1)            if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('argument ', f\"{arg_replace_list[0]}{df_suffix_str} \", comment, 1)                       if f\"{arg_replace_list[0]}\" and fn_main == 'rint' not in comment else comment\n","        comment = re.sub('the text', f\"the text in {arg_replace_list[0]}{df_suffix_str}\", comment, 1)             if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('same operation', f\"same operation on {arg_replace_list[0]}{df_suffix_str}\", comment, 1) if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('in a map', f\"in a map given by input {arg_replace_list[0]}{df_suffix_str}\", comment, 1) if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('col ', f\"{arg_replace_list[0]}{df_suffix_str} \", comment, 1)                            if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('col.', f\"{arg_replace_list[0]}{df_suffix_str}.\", comment, 1)                            if f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = re.sub('value', f\"value of {arg_replace_list[0]}{df_suffix_str},\", comment, 1)                  if f\"{arg_replace_list[0]}\" not in comment and 'no match is found' in comment else comment\n","        comment = re.sub('value', f\"value of {arg_replace_list[0]}{df_suffix_str},\", comment, 1)                  if f\"{arg_replace_list[0]}\" not in comment else comment\n","        filter_str = '1_value_4'\n","      replaced_flag = 'Y'\n","    # Checks for the occurrence of the word value and replaces with the appropriate standard\n","    elif 'argument' in comment and replaced_flag == 'N':\n","      if fn_main == 'log':\n","        comment = f\"Pyspark code which calculates the logarithm (base 2) of {arg_replace_list[0]}{df_suffix_str}\"       if f\"{arg_replace_list[0]}\" not in comment and '2.0' in code else comment\n","        comment = f\"Pyspark code which calculates the natural logarithm of {arg_replace_list[0]}{df_suffix_str}\"        if f\"{arg_replace_list[0]}\" not in comment and '2.0' not in code else comment\n","      if 'applyInPandas' in code:\n","        comment = f\"Pyspark code which normalizes the data on {arg_replace_list[0]}{df_suffix_str} using a custom function normalize using pandas\"                if f\"{arg_replace_list[0]}\" not in comment and 'normalize' in code else comment\n","        comment = f\"Pyspark code which calculates the mean of the data in {arg_replace_list[0]}{df_suffix_str} using a custom function normalize using pandas\"    if f\"{arg_replace_list[0]}\" not in comment and 'mean_func' in code else comment\n","        comment = f\"Pyspark code which calculates the sum of the data in {arg_replace_list[0]}{df_suffix_str} using a custom function normalize using pandas\"     if f\"{arg_replace_list[0]}\" not in comment and 'sum_func' in code else comment\n","        comment = f\"Pyspark code which applies the pandas function asof_join for {arg_replace_list[0]}{df_suffix_str}\"                                            if f\"{arg_replace_list[0]}\" not in comment and 'asof_join' in code else comment\n","\n","      comment = (comment.split('character set')[0] + 'character set UTF-8')                                             if f\"{arg_replace_list[0]}\" not in comment and 'UTF-8' in code else comment\n","      comment = re.sub('Splits a string', f\"Splits a string given by {arg_replace_list[0]}{df_suffix_str}\", comment, 1) if f\"{arg_replace_list[0]}\" not in comment else comment\n","      comment = re.sub('argument expr', f\"{arg_replace_list[0]}{df_suffix_str}\", comment, 1)                            if f\"{arg_replace_list[0]}\" not in comment else comment\n","      comment = re.sub('argument', f\"argument given by {arg_replace_list[0]}{df_suffix_str}\", comment, 1)               if f\"{arg_replace_list[0]}\" not in comment else comment\n","      comment = re.sub('argument', f\"argument given by {arg_replace_list[0]}{df_suffix_str}\", comment, 1)               if f\"{arg_replace_list[0]}\" not in comment else comment\n","      filter_str = '1_argument_1'\n","      replaced_flag = 'Y'\n","    # Checks for the occurrence of the word array and replaces with the appropriate standard\n","    elif 'array' in comment:\n","      comment = re.sub('input array', f'input array {arg_replace_list[0]}{df_suffix_str}', comment, 1)                  if 'input array' in comment else comment\n","      comment = re.sub('array\\.', f'array {arg_replace_list[0]}{df_suffix_str}.', comment, 1)                           if 'array.' in comment and f\"{arg_replace_list[0]}\" not in comment else comment\n","      comment = re.sub('elements\\.', f'elements in {arg_replace_list[0]}{df_suffix_str}.', comment, 1)                  if 'elements.' in comment and f\"{arg_replace_list[0]}\" not in comment else comment\n","      comment = re.sub('arrays\\.', f'arrays in {arg_replace_list[0]}{df_suffix_str}.', comment, 1)                      if 'arrays.' in comment and f\"{arg_replace_list[0]}\" not in comment else comment\n","      comment = re.sub(' map\\.', f' map in {arg_replace_list[0]}{df_suffix_str}.', comment, 1)                          if ' map.' in comment and f\"{arg_replace_list[0]}\" not in comment else comment\n","      filter_str = '1_array_1'\n","      replaced_flag = 'Y'\n","    # Checks for the occurrence of the word timestamp and replaces with the appropriate standard\n","    elif 'timestamp' in comment or category == 'Datetime Functions':\n","      for item in [\n","          'Parses the timestamp', 'Converts the timestamp', 'given date', 'given timestamp', 'of seconds', 'partition data', 'number of microseconds', 'number of milliseconds', 'time string', 'number of days', 'for date timestamp'\n","          ]:\n","        if item in comment and f\"{arg_replace_list[0]}\" not in comment:\n","          comment = re.sub(item, f\"{item.replace('string str', 'string')} in {arg_replace_list[0]}{df_suffix_str}\", comment, 1)\n","\n","      comment = re.sub('given time\\.', f'given time in {arg_replace_list[0]}{df_suffix_str}.', comment, 1)                                                              if 'given time.' in comment and f\"{arg_replace_list[0]}\" not in comment else comment\n","      comment = re.sub('truncated ', f'truncated from {arg_replace_list[0]}{df_suffix_str} ', comment, 1)                                                               if 'truncated ' in comment and f\"{arg_replace_list[0]}\" not in comment else comment\n","      comment = re.sub('a timestamp which', f'a timestamp in {arg_replace_list[0]}{df_suffix_str} which', comment, 1)                                                   if 'a timestamp which' in comment and f\"{arg_replace_list[0]}\" not in comment else comment\n","\n","      filter_str = '1_time_1'\n","      replaced_flag = 'Y'\n","    elif 'str ' in comment or 'str.' in comment:\n","      for item in ['from str', 'if str', 'in the string str', 'within the string str', 'strings in the str', 'all characters', 'Splits str', 'Decodes a str']:\n","        if item in comment and f\"{arg_replace_list[0]}\" not in comment:\n","          comment = re.sub(item, f\"{item.replace('string str', 'string')} in {arg_replace_list[0]}{df_suffix_str}\", comment, 1)\n","      filter_str = '1_str_1'\n","      replaced_flag = 'Y'\n","    else:\n","      comment = comment.replace('which Inverse of hex.', 'which returns the inverse of hex value.')\n","      in_list   = [\n","          'Converts an angle', 'a json string', 'a JSON string', 'in a group', 'ank of rows', 'in the sentence', 'string expression', 'for a string', 'Translates a string', 'XPath expression', 'function to each cogroup',\n","          'for each group'\n","          ]\n","      from_list = ['with duplicates', 'elements eliminated', 'lgConfigK arg']\n","      for_list  = ['starting at 1', 'binary data', 'result of SHA 1', 'with reflection', 'Compute aggregates', 'of hex value']\n","\n","      for item in in_list + from_list + for_list :\n","        verb = ''\n","        if item in comment and f\"{arg_replace_list[0]}\" not in comment:\n","          verb = 'in'     if item in in_list else verb\n","          verb = 'from'   if item in from_list else verb\n","          verb = 'for'    if item in in_list else verb\n","          comment = re.sub(item, f\"{item} {verb} {arg_replace_list[0]}{df_suffix_str}\", comment, 1)\n","      filter_str = '1_misc_1'\n","\n","      rplc_dict = {\n","\n","      }\n","      if f\"{arg_replace_list[0]}\" not in comment:\n","      #, , '', '', 'agg', 'apply', 'applyInPandasWithState', 'applyInPandas', 'avg'\n","        comment = f\"PySpark code which calculates the percentile rank of each row within the specified window partition based on the values in {arg_replace_list[0]}{df_suffix_str}.\" if fn_main == 'percent_rank' and f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = f\"Pyspark code which calculates the {'base-2 logarithm' if 'log(2' in code else  'natural logarithm'} of the values in {arg_replace_list[0]}{df_suffix_str}.\"       if fn_main == 'log' and f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = f\"PySpark code which calculates the variance of the values in {arg_replace_list[0]}{df_suffix_str}.\"                                                                if fn_main == 'variance' and f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = f\"PySpark code which calculates the standard deviation of the values in {arg_replace_list[0]}{df_suffix_str}.\"                                                      if 'std' in fn_main and f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = comment.replace('input.', f'input {arg_replace_list[0]}{df_suffix_str}.')                                                                                           if fn_main == 'typeof' and f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = f\"Pyspark code which groups the data by {arg_replace_list[0]}{df_suffix_str} and counts each group.\"                                                                if fn_main in ['agg', 'count'] and f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = f\"Pyspark code which applies a pandas user defined function 'normalize' to the {arg_replace_list[0]}{df_suffix_str}.\"                                               if fn_main == 'apply' and f\"{arg_replace_list[0]}\" not in comment else comment\n","        comment = comment.replace('each group of the current DataFrame', f'each group of values in {arg_replace_list[0]}{df_suffix_str}')                                             if fn_main == 'applyInPandas' and f\"{arg_replace_list[0]}\" not in comment else comment\n","\n","        filter_str = '1_misc_2'\n","\n","      replaced_flag = 'Y'\n","\n","\n","# each group of the current DataFrame  -- each group in\n","\n","  elif replaced_flag == 'N':\n","    if len(arg_match) == 0:\n","      filter_str = '0_COLS'\n","\n","  comment = comment if comment.strip().endswith('.') else f\"{comment.strip()}.\"\n","  result = [comment, filter_str, comment_orig]\n","  return result\n","\n","\n","df_standardize_comments_1 = df_standardize_columns.copy()\n","\n","# Standardize comments and add a filter column which will be useful for validations\n","df_standardize_comments_1['code_desc_1']      = df_standardize_columns.apply(lambda x: fn_standardize_comments_1(x['code_description'], x['code_snippet_1'], x['function'], x['Category'])[0], axis=1)\n","df_standardize_comments_1['filter']           = df_standardize_columns.apply(lambda x: fn_standardize_comments_1(x['code_description'], x['code_snippet_1'], x['function'], x['Category'])[1], axis=1)\n","df_standardize_comments_1['code_description'] = df_standardize_columns.apply(lambda x: fn_standardize_comments_1(x['code_description'], x['code_snippet_1'], x['function'], x['Category'])[2], axis=1)\n","\n","\n","# UNCOMMENT TO DEBUG\n","df_standardize_comments_1[\n","  #(df_standardize_comments_1['filter'].str.contains('1_INIT'))\n","\n","  (df_standardize_comments_1['filter'].str.contains('1_misc_2'))\n","\n","  #(df_standardize_comments_1['filter'].str.contains('=NA='))\n","#].info()\n","#].count()\n","][['function', 'Category', 'code_desc_1', 'code_description', 'code_snippet_1', 'code_snippet', 'filter']].style.set_properties(**{'text-align': 'left'})\n","#df_standardize_comments_1.style.set_properties(**{'text-align': 'left'})\n","\n","df_standardize_comments_1.head().style.set_properties(**{'text-align': 'left'})\n"],"metadata":{"id":"KAIf48ld6n9P","colab":{"base_uri":"https://localhost:8080/","height":484},"executionInfo":{"status":"ok","timestamp":1710278140912,"user_tz":-330,"elapsed":945,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"8971a858-05cb-4a89-c0da-8c7928477927"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x791d9446e560>"],"text/html":["<style type=\"text/css\">\n","#T_984b1_row0_col0, #T_984b1_row0_col1, #T_984b1_row0_col2, #T_984b1_row0_col3, #T_984b1_row0_col4, #T_984b1_row0_col5, #T_984b1_row0_col6, #T_984b1_row0_col7, #T_984b1_row1_col0, #T_984b1_row1_col1, #T_984b1_row1_col2, #T_984b1_row1_col3, #T_984b1_row1_col4, #T_984b1_row1_col5, #T_984b1_row1_col6, #T_984b1_row1_col7, #T_984b1_row2_col0, #T_984b1_row2_col1, #T_984b1_row2_col2, #T_984b1_row2_col3, #T_984b1_row2_col4, #T_984b1_row2_col5, #T_984b1_row2_col6, #T_984b1_row2_col7, #T_984b1_row3_col0, #T_984b1_row3_col1, #T_984b1_row3_col2, #T_984b1_row3_col3, #T_984b1_row3_col4, #T_984b1_row3_col5, #T_984b1_row3_col6, #T_984b1_row3_col7, #T_984b1_row4_col0, #T_984b1_row4_col1, #T_984b1_row4_col2, #T_984b1_row4_col3, #T_984b1_row4_col4, #T_984b1_row4_col5, #T_984b1_row4_col6, #T_984b1_row4_col7 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_984b1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_984b1_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_984b1_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_984b1_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_984b1_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_984b1_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_984b1_level0_col5\" class=\"col_heading level0 col5\" >code_snippet_1</th>\n","      <th id=\"T_984b1_level0_col6\" class=\"col_heading level0 col6\" >code_desc_1</th>\n","      <th id=\"T_984b1_level0_col7\" class=\"col_heading level0 col7\" >filter</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_984b1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_984b1_row0_col0\" class=\"data row0 col0\" >Pyspark code to Write a DataFrame into a CSV file.</td>\n","      <td id=\"T_984b1_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_984b1_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_984b1_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_984b1_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_984b1_row0_col5\" class=\"data row0 col5\" >MY_DF.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_984b1_row0_col6\" class=\"data row0 col6\" >Pyspark code to Write a DataFrame into a CSV file.The data is written to file or directory MY_DIR.</td>\n","      <td id=\"T_984b1_row0_col7\" class=\"data row0 col7\" >1_MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_984b1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_984b1_row1_col0\" class=\"data row1 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_984b1_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_984b1_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_984b1_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_984b1_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_984b1_row1_col5\" class=\"data row1 col5\" >MY_DF.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_984b1_row1_col6\" class=\"data row1 col6\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.The data is written to file or directory MY_DIR.</td>\n","      <td id=\"T_984b1_row1_col7\" class=\"data row1 col7\" >1_MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_984b1_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_984b1_row2_col0\" class=\"data row2 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.</td>\n","      <td id=\"T_984b1_row2_col1\" class=\"data row2 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_984b1_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_984b1_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_984b1_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_984b1_row2_col5\" class=\"data row2 col5\" >MY_DF = spark.read.csv(MY_DIR, schema=MY_DF.schema, nullValue=\"LIT_STR_0\")</td>\n","      <td id=\"T_984b1_row2_col6\" class=\"data row2 col6\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'LIT_STR_0'.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_984b1_row2_col7\" class=\"data row2 col7\" >1_MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_984b1_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_984b1_row3_col0\" class=\"data row3 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_984b1_row3_col1\" class=\"data row3 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_984b1_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_984b1_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_984b1_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_984b1_row3_col5\" class=\"data row3 col5\" >MY_DF = spark.read.csv(MY_DIR, schema=MY_DF.schema, nullValue=\"LIT_STR_0\")</td>\n","      <td id=\"T_984b1_row3_col6\" class=\"data row3 col6\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_984b1_row3_col7\" class=\"data row3 col7\" >1_MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_984b1_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_984b1_row4_col0\" class=\"data row4 col0\" >Pyspark code to Write a DataFrame into a JSON file.</td>\n","      <td id=\"T_984b1_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_984b1_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_984b1_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_984b1_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_984b1_row4_col5\" class=\"data row4 col5\" >MY_DF.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_984b1_row4_col6\" class=\"data row4 col6\" >Pyspark code to Write a DataFrame into a JSON file.The data is written to file or directory MY_DIR.</td>\n","      <td id=\"T_984b1_row4_col7\" class=\"data row4 col7\" >1_MY_DIR</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# HANDLE COMMENTS PASS 1: validate how many rows per filter category were standardized\n","\n","fn_display_header(\"HANDLE COMMENTS PASS 1: validate how many rows per filter category were standardized\")\n","\n","df_standardize_comments_1.groupby('filter').size().reset_index(name='count')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":575},"id":"jD_arYEH6pH6","executionInfo":{"status":"ok","timestamp":1710278146365,"user_tz":-330,"elapsed":793,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"62966848-2898-4475-eb09-7f7430b8239e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           HANDLE COMMENTS PASS 1: validate how many rows per filter category were standardized\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["            filter  count\n","0           0_COLS     37\n","1         1_MY_DIR     90\n","2     1_argument_1     13\n","3        1_array_1     17\n","4       1_column_1     78\n","5   1_fn_explode_1      5\n","6         1_misc_1     30\n","7         1_misc_2     14\n","8          1_str_1     29\n","9         1_time_1     46\n","10       1_value_1     14\n","11       1_value_2      4\n","12       1_value_3     23\n","13       1_value_4     27\n","14            =NA=    267"],"text/html":["\n","  <div id=\"df-321bf6c3-553c-44b9-a191-330249d109dd\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filter</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0_COLS</td>\n","      <td>37</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1_MY_DIR</td>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1_argument_1</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1_array_1</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1_column_1</td>\n","      <td>78</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1_fn_explode_1</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1_misc_1</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1_misc_2</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1_str_1</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1_time_1</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1_value_1</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1_value_2</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1_value_3</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1_value_4</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>=NA=</td>\n","      <td>267</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-321bf6c3-553c-44b9-a191-330249d109dd')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-321bf6c3-553c-44b9-a191-330249d109dd button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-321bf6c3-553c-44b9-a191-330249d109dd');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-43f9820f-72d0-43dd-b148-cb3c740f9cc3\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-43f9820f-72d0-43dd-b148-cb3c740f9cc3')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-43f9820f-72d0-43dd-b148-cb3c740f9cc3 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"df_standardize_comments_1\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"filter\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"1_time_1\",\n          \"1_value_2\",\n          \"0_COLS\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 65,\n        \"min\": 4,\n        \"max\": 267,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          46,\n          23,\n          37\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["<br>\n","<br>\n","<br>\n","\n","----\n","# <b>CODE/COMMENT STANDARDIZATION: PASS 2</b>\n","----\n","<br>"],"metadata":{"id":"6n1rHPBmScYe"}},{"cell_type":"code","source":["# HANDLE COMMENTS PASS 2: Manipulate comments to incorporate standards introduced in code\n","#                         Handles double occurrences of COL or LITERAL i.e. functions passing 2 arguments\n","\n","# Function to handle functions passing 2 arguments\n","def fn_standardize_comments_2(comment, code, function, category):\n","  fn_main = function.split('.')[-1].strip()\n","  filter_str = '=NA='\n","  replaced_flag = 'N'\n","\n","  # Handle some special cases for comments\n","  if not comment.strip().endswith('.'):\n","    comment = comment + '.'\n","\n","  comment = comment.replace('Aggregate function', '').replace('Collection function', '').replace('Window function', '')\n","  comment = comment.replace('thesame', 'the same').replace('valuein', 'value in').replace('columnPyspark', 'column Pyspark').replace('andcol2', 'and col2').replace('forcolumn', 'for column').replace('aswell', 'as well')\n","  comment = comment.replace('delimitedlist', 'delimited list').replace('pairsin', 'pairs in').replace('srcCol', 'source Col').replace('isreturned', 'is returned').replace('leftmost', 'left most').replace('rightmost', 'right most')\n","  comment = comment.replace('placeswith',  'places with').replace('sensitiveinformation', 'sensitive information')\n","\n","  comment_orig = comment\n","\n","  if fn_main in delim_dict:\n","    comment = comment.replace('delimiter', 'delimiter ' + delim_dict[fn_main])\n","\n","  # search code for occurrences of standardized column names and literals\n","  arg_match = re.findall(r\"(LIT_(INT|DEC|STR)_\\d+|COL_[A-Z]|YEAR|MONTH|DAY|HOUR|MIN|SEC)\", code)\n","  arg_match = list(dict.fromkeys([match[0] for match in arg_match]))\n","\n","  arg_replace_list = []\n","\n","  for item in arg_match:\n","    if 'COL_' in item or item in ['YEAR', 'MONTH', 'DAY', 'HOUR', 'MIN', 'SEC']:\n","      arg_replace_list.append(f\"column {item}\")\n","    else:\n","      arg_replace_list.append(f\"{lit_search_id_dict[item]} value {item}\")\n","\n","\n","\n","  exception_functions = ['']\n","  if len(arg_match) == 2 and replaced_flag == 'N':\n","    df_suffix_str = \" in dataframe MY_DF\"   #if 'COL_' in code else ''\n","\n","    if 'COL_' in arg_replace_list[0]:\n","      arg_replace_list[0] = arg_replace_list[0] + df_suffix_str\n","    if 'COL_' in arg_replace_list[1]:\n","      arg_replace_list[0] = arg_replace_list[0].replace(df_suffix_str, '')\n","      arg_replace_list[1] = arg_replace_list[1] + df_suffix_str\n","\n","    REPL_0 = arg_replace_list[0]\n","    REPL_1 = arg_replace_list[1]\n","\n","    #if 'float' not in fn_main:\n","    #  comment = comment.replace('float', ' ')\n","\n","    if 'dividend' in comment:\n","      comment = comment.replace('positive value of dividend mod divisor', 'positive mod value of dividend divisor')\n","      comment = comment.replace(\"dividend divisor\", f\"the result of division where the numerator is {REPL_0} and denominator is {REPL_1}\")\n","      filter_str = '2_ARG_1'\n","      replaced_flag = 'Y'\n","    elif 'Returns left right' in comment:\n","      comment = f\"Pyspark code which multiplies the value of {REPL_0} with {REPL_1}\"                        if f\"{arg_replace_list[0]}\" not in comment and fn_main == 'try_multiply' else comment\n","      if fn_main == 'try_subtract':\n","        comment = f\"Pyspark code which subtract the value of {REPL_1} from {REPL_0}\"                                                                                              if f\"{arg_replace_list[0]}\" not in comment and 'make_interval' not in code else comment\n","        comment = f\"Pyspark code which subtract the value of interval duration in {REPL_1} from {REPL_0}. Uses the make_interval function for deriving the interval in {REPL_1}\"  if f\"{arg_replace_list[0]}\" not in comment and 'make_interval' in code else comment\n","      filter_str = '2_ARG_2'\n","      replaced_flag = 'Y'\n","    elif fn_main == 'try_add':\n","      comment = f\"Pyspark code which adds the value of {REPL_0} to {REPL_1}\"                                                                                                  if f\"{arg_replace_list[0]}\" not in comment and 'make_interval' not in code else comment\n","      comment = f\"Pyspark code which adds the value of {REPL_0} to the interval duration in {REPL_1}. Uses the make_interval function for deriving the interval in {REPL_1}\"  if f\"{arg_replace_list[0]}\" not in comment and 'make_interval' in code else comment\n","      filter_str = '2_ARG_3'\n","      replaced_flag = 'Y'\n","    elif 'value in' in comment:\n","      comment = re.sub('given value in the given array', f\"element {REPL_1} in the array {REPL_0}\", comment, 1)         if f\"{arg_replace_list[0]}\" not in comment else comment\n","      comment = re.sub('value in a group', f\"value of {REPL_1} grouped by {REPL_0}\", comment, 1)                        if f\"{arg_replace_list[0]}\" not in comment else comment\n","      filter_str = '2_ARG_4'\n","      replaced_flag = 'Y'\n","    elif 'date_add' in code or 'add_months' in code or 'date_sub' in code or 'dateadd' in code:\n","      comment = f\"Pyspark code which adds the the number of months given by {REPL_1} to the date {REPL_0}\"              if f\"{arg_replace_list[0]}\" not in comment and 'add_months' in code else comment\n","      comment = f\"Pyspark code which adds the the number of days given by {REPL_1} to the date {REPL_0}\"                if f\"{arg_replace_list[0]}\" not in comment and 'date_add' in code else comment\n","      comment = f\"Pyspark code which adds the the number of days given by {REPL_1} to the date {REPL_0}\"                if f\"{arg_replace_list[0]}\" not in comment and 'dateadd' in code else comment\n","      comment = f\"Pyspark code which subtracts the the number of days given by {REPL_1} to the date {REPL_0}\"           if f\"{arg_replace_list[0]}\" not in comment and 'date_sub' in code else comment\n","      filter_str = '2_ARG_5'\n","      replaced_flag = 'Y'\n","    elif 'element of array' in comment:\n","      comment = comment.replace('array index', '')\n","      if 'get' in code:\n","        comment = re.sub('element of array', f\"element of array in {REPL_0}\", comment, 1)\n","        comment = re.sub('index', f\"index given by {REPL_1}.##\", comment, 1).split('##')[0]\n","      comment = re.sub('index', f\"index given by {REPL_1} in array or map {REPL_0}.##\", comment, 1).split('##')[0]      if f\"{arg_replace_list[0]}\" not in comment and 'element_at' in code else comment\n","      comment = f\"Pyspark code which adds the the number of months given by {REPL_1} to the date {REPL_0}\"              if f\"{arg_replace_list[0]}\" not in comment and 'add_months' in code else comment\n","      filter_str = '2_ARG_6'\n","      replaced_flag = 'Y'\n","    elif 'input column' in comment:\n","      comment = re.sub('input columns', f\"input columns {REPL_1} and {REPL_0}\", comment, 1)                             if f\"{arg_replace_list[0]}\" not in comment and 'input columns' in comment else comment\n","      if fn_main in ['assert_true']:\n","        comment = f\"Pyspark code which Returns null if the condition {REPL_0} < {REPL_1} is true.\"                      if '<' in code else comment\n","        comment = f\"Pyspark code which Returns null if the condition {REPL_0} < {REPL_1} is true.\"                      if '>' in code else comment\n","        for default_val in [', MY_DF.COL_A', \", 'error\", \", 'My error msg\"]:\n","          if default_val in code:\n","            default_val = default_val.replace(', ', '').replace(\"'\", '')\n","            default_val = f\"'{default_val}'\" if 'COL_' not in default_val else f\"in column {default_val}\"\n","            comment =  comment + f\" The function throws an exception with error message {default_val}.\"\n","      filter_str = '2_ARG_7'\n","      replaced_flag = 'Y'\n","    elif 'columns' in comment:\n","      comment = comment.replace('one column Pyspark code which Two or more columns', 'numeric columns')\n","      comment = re.sub('string columns', f\"string columns {REPL_0} and {REPL_1}\", comment, 1).replace('separator', \"separator '-'\")   if f\"{arg_replace_list[0]}\" not in comment and 'string columns' in comment else comment\n","      comment = re.sub('given columns', f\"given columns {REPL_0} and {REPL_1}\", comment, 1)                                           if f\"{arg_replace_list[0]}\" not in comment and 'given columns' in comment else comment\n","      if (\"'COL_B')\" in code or '\"COL_B\")' in code) and 'numeric columns for each group' in comment:\n","        comment = re.sub('each numeric columns', f\"numeric {REPL_1}\", comment, 1)\n","        comment = re.sub('for each group', f\"grouped by {REPL_0}\", comment, 1)\n","      else:\n","        comment = re.sub('each numeric columns', f\"numeric columns {REPL_0} and {REPL_1}\", comment, 1)\n","        comment = re.sub('for each group', f\"grouped by the entire dataframe MY_DF\", comment, 1)\n","      comment = re.sub('numeric columns', f\"numeric columns {REPL_0} and {REPL_1}\", comment, 1)     if f\"{arg_replace_list[0]}\" not in comment and fn_main == 'xxhash64' else comment\n","      filter_str = '2_ARG_8'\n","      replaced_flag = 'Y'\n","    elif 'col1' in comment:\n","      comment = comment.replace('col1', f\"{REPL_0}\").replace('col2', f\"{REPL_1}\")\n","      filter_str = '2_ARG_9'\n","      replaced_flag = 'Y'\n","    elif 'array' in comment:\n","      comment = comment.replace('array column', f\"array column from {REPL_0} and {REPL_1}\")\n","      if fn_main in ['array_contains']:\n","        array_verb = f\"{fn_main.split('_')[1]}s\".replace('ss', 's')\n","        comment = f\"Pyspark code which checks if the array {REPL_0} {array_verb} the value of {REPL_1}.\"\n","      elif fn_main in ['arrays_overlap']:\n","        array_verb = f\"{fn_main.split('_')[1]}s\".replace('ss', 's').replace('overlaps', 'overlaps or exists in')\n","        comment = f\"Pyspark code which checks if the value of {REPL_1} {array_verb} the array {REPL_0}.\"\n","      elif fn_main in ['array_insert', 'array_remove', 'array_prepend']:\n","        array_verb = f\"{fn_main.split('_')[1]}s\".replace('ss', 's')\n","        comment = f\"Pyspark code which {array_verb} the value of {REPL_1} the array {REPL_0}.\"\n","\n","      comment = comment.replace('input array', f\"input array {REPL_0} \")                                                              if\"{arg_replace_list[0]}\" not in comment and fn_main == 'array_sort' else comment\n","      comment = comment.replace('a column repeated count times', f\"a column {REPL_0} repeated the number of times in {REPL_1}\")       if f\"{arg_replace_list[0]}\" not in comment and fn_main == 'array_repeat' else comment\n","      comment = comment.replace('two arrays', f\"array {REPL_0} and array {REPL_1}\")                                                   if f\"{arg_replace_list[0]}\" not in comment and fn_main == 'map_from_arrays' else comment\n","      comment = comment.replace('two given arrays', f\"array {REPL_0} and array {REPL_1}\").replace('function', 'lambda function')      if f\"{arg_replace_list[0]}\" not in comment and 'element wise' in comment else comment\n","      comment = f\"Pyspark code which aggregates the {REPL_0} using initial accumulator {REPL_1} via a lambda function.\"               if f\"{arg_replace_list[0]}\" not in comment and fn_main == 'aggregate' else comment\n","      comment = f\"Pyspark code which computes a histogram for the numeric values in {REPL_0} with number of bins given by {REPL_1}.\"  if f\"{arg_replace_list[0]}\" not in comment and fn_main == 'histogram_numeric' else comment\n","      comment = f\"Pyspark code which Returns the substring of str in {REPL_0} that starts at position given by {REPL_1}.\"             if f\"{arg_replace_list[0]}\" not in comment and fn_main == 'substr' else comment\n","      comment = f\"Pyspark code which uses a reduce function to sum up the values of {REPL_0} using initial accumulator {REPL_1} via a lambda functon.\"  if f\"{arg_replace_list[0]}\" not in comment and fn_main == 'reduce' else comment\n","\n","      filter_str = '2_ARG_10'\n","      replaced_flag = 'Y'\n","    elif 'group' in comment:\n","      if '_value(' in code:\n","        comment = comment.replace('some value', 'any value').replace('of col for a group of rows', f'of {REPL_0} and {REPL_1}')\n","        comment = comment.replace('the values', f'the values in {REPL_1}').replace('in a group', f'grouped by {REPL_0}')                if f\"{arg_replace_list[0]}\" not in comment and 'median' in comment else comment\n","        comment = comment.replace('c1 n 1 grouping c2 n 2 grouping cn', f'c1 n 1 grouping c2 n 2 grouping cn')                          if f\"{arg_replace_list[0]}\" not in comment and 'median' in comment else comment\n","        filter_str = '2_ARG_11'\n","      elif 'groupBy' in code or 'groupby' in code:\n","        rplc_dict = {'min': 'minimum', 'min_udf': 'minimum', 'avg': 'average', 'max': 'maximum', 'sum': 'sum', 'median': 'median'}\n","        comment = f\"Pyspark code which groups by on {REPL_0 if 'groupBy()' not in code else 'entire dataset MY_DF'} and calculates the {[ rplc_dict[x] for x in rplc_dict.keys() if x + '(' in code][0]} of {REPL_1}.\" #+ comment\n","        filter_str = '2_ARG_11'\n","      elif 'regr_' in fn_main:\n","        rplc_dict = {\n","            'regr_avgx': 'average of the independent variable x for non-null pairs in a group',\n","            'regr_avgy': 'average of the dependent variable y for non-null pairs in a group',\n","            'regr_count': 'count non-null number pairs used to fit the regression line',\n","            'regr_intercept': 'y-intercept of the regression line',\n","            'regr_r2': 'coefficient of determination (R-squared) for the regression',\n","            'regr_slope': 'slope of the regression line',\n","            }\n","        if fn_main not in ['regr_sxx', 'regr_sxy', 'regr_syy']:\n","          comment = f\"Pyspark code which returns the {[ rplc_dict[x] for x in rplc_dict.keys() if x + '(' in code][0]} where x in the independent variable given by {REPL_0} and y is the dependent variable given by {REPL_1}.\" #+ '-----' + comment\n","        filter_str = '2_ARG_11' if fn_main not in ['regr_sxx', 'regr_sxy', 'regr_syy'] else '2_ARG_11_IGNORE'\n","      else:\n","        filter_str = '2_ARG_11_IGNORE'\n","\n","      replaced_flag = 'Y'\n","\n","    elif 'Math' in category:\n","      comment = comment.replace('given column', f'given columns {REPL_0} and {REPL_1}')\n","      comment = comment.replace('sqrt a 2 b 2', f'square root of the squared values of {REPL_0} and {REPL_1}')\n","      comment = comment.replace('first argument', f'first argument {REPL_0}').replace('second argument', f'second argument {REPL_1}')\n","      comment = comment.replace('given value', f'given value {REPL_0}').replace('numBits', f'number of bits given by {REPL_1} towards')     if 'numBits' in comment else comment\n","      comment = comment.replace('given value', f'{REPL_0} and {REPL_1}')                                                                    if 'numBits' not in comment else comment\n","      filter_str = '2_ARG_12_A'\n","      replaced_flag = 'Y'\n","\n","    elif 'Datetime' in category:\n","      comment = comment.replace('date1', f'{REPL_0}').replace('date2', f'{REPL_1}')\n","      comment = comment.split('from')[0] + f\"from columns {REPL_0} {'and ' + REPL_1 if 'WEEK' not in code else ',WEEK and ' + REPL_1} in dataframe MY_DF\".replace('column ', '')      if 'interval' in fn_main else comment\n","      comment = f\"Pyspark code which Returns the difference of the date columns {REPL_0} and {REPL_1}\"                                                                                if 'diff' in fn_main else comment\n","\n","      filter_str = '2_ARG_12_B'         if 'timestamp' not in fn_main else '2_ARG_12_B_IGNORE'\n","      replaced_flag = 'Y'\n","    elif 'Collection' in category:\n","      comment = comment.replace('column', f'column with {REPL_0} and {REPL_1}')\n","      comment = comment.replace('map contains the key', f'map column {REPL_0} contains the key value given by {REPL_1}')\n","      comment = comment.replace('given maps', f'given map columns {REPL_0} and {REPL_1}').replace('column ', '')\n","      comment = comment.replace('from start to stop incrementing by step', f'starting from value in {REPL_0} to the value in {REPL_1} incrementing by step 1 if not provided').split('.')[0]\n","      filter_str = '2_ARG_12_C'\n","      replaced_flag = 'Y'\n","    elif 'Aggregate' in category:\n","      if 'groupby' in code:\n","        rplc_dict = {'first': 'first value', 'last': 'last value'}\n","        comment = f\"Pyspark code which groups by {REPL_0} and calculates the {[ rplc_dict[x] for x in rplc_dict.keys() if x + '(' in code][0]} of {REPL_1}.\" #+ comment\n","      else:\n","        comment = comment.replace('percentile s', 'percentiles').replace('percentage s', 'percentages').replace('numeric column expr', f'numeric {REPL_0}')\n","      filter_str = '2_ARG_12_D'         if fn_main != 'hll_sketch_agg' else '2_ARG_12_D_IGNORE'\n","      replaced_flag = 'Y'\n","    elif 'String' in category:\n","      if 'boolean' in comment:\n","        comment = comment.split('.')[0] + f\" if {REPL_0} {fn_main} the value of {REPL_1}.\".replace('with', ' with')\n","        #filter_str = '2_ARG_12_E'\n","        replaced_flag = 'Y'\n","      elif 'substr' in comment:\n","        comment = comment.replace('substr column', 'substr ').replace('occurrence of substr', f'occurrence of {REPL_1}').replace(' in a string', f' in {REPL_0}').replace(' in str ', f' in {REPL_0}').replace('given string', f'given string {REPL_0}')\n","        comment = comment.replace('after position start', '').replace(' column after position pos', '')\n","        comment = comment.replace('from string str', f\"from {REPL_0}\").replace('delimiter delim', \"delimiter '.'\").replace('before count', 'before number of')\n","        filter_str = '2_ARG_12_E'\n","        replaced_flag = 'Y'\n","      else:\n","        comment = comment.replace('two given strings', f'two given strings in {REPL_0} and {REPL_1}')\n","        comment = comment.replace('from str', f'from {REPL_0} using {REPL_1} as the trim character.')                                                               if fn_main == 'btrim' else comment\n","        comment = comment.replace('string st', f'{REPL_0}').replace('list strArray', f'string Array given by {REPL_1}' )                                            if fn_main == 'find_in_set' else comment\n","        comment = f\"Pyspark code which pads the given string '#' to the {'left' if fn_main == 'lpad' else 'right'} of {REPL_0} up to a total width of {REPL_1}\"     if 'pad' in fn_main else comment\n","        comment = f\"Pyspark code which formats the number in {REPL_0} to a specific pattern rounded to the number of decimal places given by {REPL_1}.\"             if fn_main == 'format_number' else comment\n","        comment = comment.replace('the arguments', f'the {REPL_0} and {REPL_1} in the dataframe df')                                                                if fn_main == 'format_string' else comment\n","        comment = f'Pyspark code which returns the substring from the {fn_main} of {REPL_0} based on the length of the value in {REPL_1}.'                          if fn_main in ['left', 'right'] else comment\n","        comment = comment.replace('string column n times', f'the string in {REPL_0} the number of times given by {REPL_1}')                                         if fn_main == 'repeat' else comment\n","        comment = comment.replace('given string value', f'given string in {REPL_0} with {REPL_1} to replace upper-case characters with')                            if fn_main == 'mask' else comment\n","        comment = comment.replace('URL', f'URL where column {REPL_0} is the URL string and column {REPL_1} is extract part (like HOST, PATH, QUERY etc)')           if fn_main == 'parse_url' else comment\n","        comment = comment.replace('search with replace', f'{REPL_0} with {REPL_1}')                                                                                 if fn_main == 'replace' else comment\n","        if 'like' in fn_main:\n","          comment = f\"Pyspark code which checks if the value of {REPL_0} is (case {'sensitive' if fn_main == 'like' else 'insensitive'}) similar to any of the comma-separated string values in {REPL_1}. It returns a boolean column if a match is found. Default escape character is blank ''.\"\n","        filter_str = '2_ARG_12_F'\n","        replaced_flag = 'Y'\n","\n","    elif replaced_flag == 'N':\n","      comment = comment.replace('of input', f'of the input {REPL_0}').replace('with padding', f'with key {REPL_1}')                     if fn_main in ['aes_decrypt', 'aes_encrypt'] else comment\n","      comment = comment.replace('input bitmap', f'input bitmap of the input {REPL_0}')                                                  if fn_main == 'bitmap_count' else comment\n","      comment = comment.replace('specified position', f'specified position given by {REPL_1} from  {REPL_0}')                           if fn_main in ['bit_get', 'getbit'] else comment\n","      comment = comment.replace('which Returns', f'which evalutes the columns {REPL_0} and {REPL_1} and returns')                       if fn_main == 'equal_null' else comment\n","      comment = f\"Pyspark code which returns the value of {REPL_0} for the row number given by {REPL_1} in the window frame.\"           if fn_main == 'nth_value' else comment\n","      comment = f\"Pyspark code which returns the value of {REPL_0} for the previous row number given by {REPL_1} in the window frame.\"  if fn_main == 'lag' else comment\n","      filter_str = '2_ARG_13' if fn_main not in ['reflect'] else '2_ARG_13_IGNORE'\n","      replaced_flag = 'Y'\n","\n","\n","  comment = comment if comment.strip().endswith('.') else f\"{comment.strip()}.\"\n","  result = [comment, filter_str, comment_orig]\n","  return result\n","\n","\n","df_standardize_comments_2 = df_standardize_comments_1.copy()\n","\n","# Standardize Column names calling the function with level 3 first then 2 and then 1\n","df_standardize_comments_2['code_desc_1']      = df_standardize_comments_2.apply(\n","                                                    lambda x: fn_standardize_comments_2(x['code_description'], x['code_snippet_1'], x['function'], x['Category'])[0]   if x['filter'] == '=NA=' else x['code_desc_1']\n","                                                              , axis=1)\n","df_standardize_comments_2['filter']           = df_standardize_comments_2.apply(\n","                                                    lambda x: fn_standardize_comments_2(x['code_description'], x['code_snippet_1'], x['function'], x['Category'])[1]   if x['filter'] == '=NA=' else x['filter'], axis=1)\n","df_standardize_comments_2['code_description'] = df_standardize_comments_2.apply(\n","                                                    lambda x: fn_standardize_comments_2(x['code_description'], x['code_snippet_1'], x['function'], x['Category'])[2]   , axis=1)\n","\n","# UNCOMMENT TO DEBUG\n","df_standardize_comments_2[\n","  #(df_standardize_comments_2['filter'].str.contains('2_ARG_1'))\n","  #(df_standardize_comments_2['filter'].str.contains('2_ARG_10'))\n","  #(df_standardize_comments_2['filter'].str.contains('2_ARG_12_F'))\n","\n","  (df_standardize_comments_2['filter'].str.contains('2_ARG_13'))\n","\n","  #(df_standardize_comments_2['filter'].str.contains('=NA='))\n","#].info()\n","#].count()\n","#][['filter', 'code_desc_1', 'code_snippet_1', 'code_description', 'code_snippet', 'function', 'Category' ]].style.set_properties(**{'text-align': 'left'})\n","][['function', 'Category', 'code_desc_1', 'code_snippet_1', 'code_snippet', 'filter' ]].style.set_properties(**{'text-align': 'left'})\n","\n","#df_standardize_comments_2.style.set_properties(**{'text-align': 'left'})\n","\n","df_standardize_comments_2.head().style.set_properties(**{'text-align': 'left'})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":484},"id":"tp852le_IiXj","executionInfo":{"status":"ok","timestamp":1710278161141,"user_tz":-330,"elapsed":879,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"2f5201b4-c9b3-410c-ebaf-dd372990e906"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x791d9446ea10>"],"text/html":["<style type=\"text/css\">\n","#T_cfba6_row0_col0, #T_cfba6_row0_col1, #T_cfba6_row0_col2, #T_cfba6_row0_col3, #T_cfba6_row0_col4, #T_cfba6_row0_col5, #T_cfba6_row0_col6, #T_cfba6_row0_col7, #T_cfba6_row1_col0, #T_cfba6_row1_col1, #T_cfba6_row1_col2, #T_cfba6_row1_col3, #T_cfba6_row1_col4, #T_cfba6_row1_col5, #T_cfba6_row1_col6, #T_cfba6_row1_col7, #T_cfba6_row2_col0, #T_cfba6_row2_col1, #T_cfba6_row2_col2, #T_cfba6_row2_col3, #T_cfba6_row2_col4, #T_cfba6_row2_col5, #T_cfba6_row2_col6, #T_cfba6_row2_col7, #T_cfba6_row3_col0, #T_cfba6_row3_col1, #T_cfba6_row3_col2, #T_cfba6_row3_col3, #T_cfba6_row3_col4, #T_cfba6_row3_col5, #T_cfba6_row3_col6, #T_cfba6_row3_col7, #T_cfba6_row4_col0, #T_cfba6_row4_col1, #T_cfba6_row4_col2, #T_cfba6_row4_col3, #T_cfba6_row4_col4, #T_cfba6_row4_col5, #T_cfba6_row4_col6, #T_cfba6_row4_col7 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_cfba6\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_cfba6_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_cfba6_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_cfba6_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_cfba6_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_cfba6_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_cfba6_level0_col5\" class=\"col_heading level0 col5\" >code_snippet_1</th>\n","      <th id=\"T_cfba6_level0_col6\" class=\"col_heading level0 col6\" >code_desc_1</th>\n","      <th id=\"T_cfba6_level0_col7\" class=\"col_heading level0 col7\" >filter</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_cfba6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_cfba6_row0_col0\" class=\"data row0 col0\" >Pyspark code to Write a DataFrame into a CSV file.</td>\n","      <td id=\"T_cfba6_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_cfba6_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_cfba6_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_cfba6_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_cfba6_row0_col5\" class=\"data row0 col5\" >MY_DF.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_cfba6_row0_col6\" class=\"data row0 col6\" >Pyspark code to Write a DataFrame into a CSV file.The data is written to file or directory MY_DIR.</td>\n","      <td id=\"T_cfba6_row0_col7\" class=\"data row0 col7\" >1_MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cfba6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_cfba6_row1_col0\" class=\"data row1 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_cfba6_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_cfba6_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_cfba6_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_cfba6_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_cfba6_row1_col5\" class=\"data row1 col5\" >MY_DF.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_cfba6_row1_col6\" class=\"data row1 col6\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.The data is written to file or directory MY_DIR.</td>\n","      <td id=\"T_cfba6_row1_col7\" class=\"data row1 col7\" >1_MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cfba6_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_cfba6_row2_col0\" class=\"data row2 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.</td>\n","      <td id=\"T_cfba6_row2_col1\" class=\"data row2 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_cfba6_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_cfba6_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_cfba6_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_cfba6_row2_col5\" class=\"data row2 col5\" >MY_DF = spark.read.csv(MY_DIR, schema=MY_DF.schema, nullValue=\"LIT_STR_0\")</td>\n","      <td id=\"T_cfba6_row2_col6\" class=\"data row2 col6\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'LIT_STR_0'.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_cfba6_row2_col7\" class=\"data row2 col7\" >1_MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cfba6_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_cfba6_row3_col0\" class=\"data row3 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_cfba6_row3_col1\" class=\"data row3 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_cfba6_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_cfba6_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_cfba6_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_cfba6_row3_col5\" class=\"data row3 col5\" >MY_DF = spark.read.csv(MY_DIR, schema=MY_DF.schema, nullValue=\"LIT_STR_0\")</td>\n","      <td id=\"T_cfba6_row3_col6\" class=\"data row3 col6\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_cfba6_row3_col7\" class=\"data row3 col7\" >1_MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cfba6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_cfba6_row4_col0\" class=\"data row4 col0\" >Pyspark code to Write a DataFrame into a JSON file.</td>\n","      <td id=\"T_cfba6_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_cfba6_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_cfba6_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_cfba6_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_cfba6_row4_col5\" class=\"data row4 col5\" >MY_DF.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_cfba6_row4_col6\" class=\"data row4 col6\" >Pyspark code to Write a DataFrame into a JSON file.The data is written to file or directory MY_DIR.</td>\n","      <td id=\"T_cfba6_row4_col7\" class=\"data row4 col7\" >1_MY_DIR</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# HANDLE COMMENTS PASS 2: validate how many rows per filter category were standardized\n","\n","fn_display_header(\"HANDLE COMMENTS PASS 2: validate how many rows per filter category were standardized\")\n","\n","df_standardize_comments_2.groupby('filter').size().reset_index(name='count')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Gj2p3YNptIzL","executionInfo":{"status":"ok","timestamp":1710278167234,"user_tz":-330,"elapsed":802,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"c966c06c-2a06-46d2-e29e-e915aafff4f3"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           HANDLE COMMENTS PASS 2: validate how many rows per filter category were standardized\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["               filter  count\n","0              0_COLS     37\n","1            1_MY_DIR     90\n","2        1_argument_1     13\n","3           1_array_1     17\n","4          1_column_1     78\n","5      1_fn_explode_1      5\n","6            1_misc_1     30\n","7            1_misc_2     14\n","8             1_str_1     29\n","9            1_time_1     46\n","10          1_value_1     14\n","11          1_value_2      4\n","12          1_value_3     23\n","13          1_value_4     27\n","14            2_ARG_1      3\n","15           2_ARG_10     19\n","16           2_ARG_11     25\n","17    2_ARG_11_IGNORE      4\n","18         2_ARG_12_A      8\n","19         2_ARG_12_B     11\n","20  2_ARG_12_B_IGNORE      2\n","21         2_ARG_12_C      8\n","22         2_ARG_12_D      7\n","23  2_ARG_12_D_IGNORE      2\n","24         2_ARG_12_E      6\n","25         2_ARG_12_F     18\n","26           2_ARG_13     10\n","27    2_ARG_13_IGNORE      1\n","28            2_ARG_2      7\n","29            2_ARG_3      5\n","30            2_ARG_4      9\n","31            2_ARG_5     12\n","32            2_ARG_6     11\n","33            2_ARG_7      6\n","34            2_ARG_8     11\n","35            2_ARG_9     11\n","36               =NA=     71"],"text/html":["\n","  <div id=\"df-02ce9fbd-6857-425d-8628-ac559eebfbb0\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filter</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0_COLS</td>\n","      <td>37</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1_MY_DIR</td>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1_argument_1</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1_array_1</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1_column_1</td>\n","      <td>78</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1_fn_explode_1</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1_misc_1</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1_misc_2</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1_str_1</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1_time_1</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1_value_1</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1_value_2</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1_value_3</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1_value_4</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2_ARG_1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2_ARG_10</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2_ARG_11</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2_ARG_11_IGNORE</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2_ARG_12_A</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>2_ARG_12_B</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>2_ARG_12_B_IGNORE</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>2_ARG_12_C</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>2_ARG_12_D</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>2_ARG_12_D_IGNORE</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>2_ARG_12_E</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>2_ARG_12_F</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>2_ARG_13</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>2_ARG_13_IGNORE</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>2_ARG_2</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>2_ARG_3</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>2_ARG_4</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>2_ARG_5</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>2_ARG_6</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>2_ARG_7</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>2_ARG_8</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>2_ARG_9</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>=NA=</td>\n","      <td>71</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02ce9fbd-6857-425d-8628-ac559eebfbb0')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-02ce9fbd-6857-425d-8628-ac559eebfbb0 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-02ce9fbd-6857-425d-8628-ac559eebfbb0');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-bd6f8582-fd87-4425-8911-e7d8a29ac40d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bd6f8582-fd87-4425-8911-e7d8a29ac40d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-bd6f8582-fd87-4425-8911-e7d8a29ac40d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"df_standardize_comments_2\",\n  \"rows\": 37,\n  \"fields\": [\n    {\n      \"column\": \"filter\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 37,\n        \"samples\": [\n          \"2_ARG_11_IGNORE\",\n          \"1_value_4\",\n          \"1_column_1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21,\n        \"min\": 1,\n        \"max\": 90,\n        \"num_unique_values\": 27,\n        \"samples\": [\n          29,\n          3,\n          46\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["<br>\n","<br>\n","<br>\n","\n","----\n","# <b>CODE/COMMENT STANDARDIZATION: PASS 3</b>\n","----\n","<br>"],"metadata":{"id":"cjk6P8bVSgyV"}},{"cell_type":"code","source":["# HANDLE COMMENTS PASS 3: Manipulate comments to incorporate standards introduced in code\n","#                         Handles tripple occurrences of COL or LITERAL i.e. functions passing 3 arguments\n","\n","# Function to handle functions passing 2 arguments\n","def fn_standardize_comments_2(comment, code, function, category):\n","  fn_main = function.split('.')[-1].strip()\n","  filter_str = '=NA='\n","  replaced_flag = 'N'\n","\n","  # Handle some special cases for comments\n","  if not comment.strip().endswith('.'):\n","    comment = comment + '.'\n","\n","  comment = comment.replace('Aggregate function', '').replace('Collection function', '').replace('Window function', '')\n","  comment = comment.replace('resultby',  'result by').replace('anddefault', 'and default').replace('sensitiveinformation', 'sensitive information')\n","\n","  comment_orig = comment\n","\n","  if fn_main in delim_dict:\n","    comment = comment.replace('delimiter', 'delimiter ' + delim_dict[fn_main])\n","\n","  # search code for occurrences of standardized column names and literals\n","  arg_match = re.findall(r\"(LIT_(INT|DEC|STR)_\\d+|COL_[A-Z]|YEAR|MONTH|DAY|HOUR|MIN|SEC)\", code)\n","  arg_match = list(dict.fromkeys([match[0] for match in arg_match]))\n","\n","  arg_replace_list = []\n","\n","  for item in arg_match:\n","    if 'COL_' in item or item in ['YEAR', 'MONTH', 'DAY', 'HOUR', 'MIN', 'SEC']:\n","      arg_replace_list.append(f\"column {item}\")\n","    else:\n","      arg_replace_list.append(f\"{lit_search_id_dict[item]} value {item}\")\n","\n","\n","\n","  exception_functions = ['']\n","  if len(arg_match) == 3 and replaced_flag == 'N':\n","    df_suffix_str = \" in dataframe MY_DF\"   #if 'COL_' in code else ''\n","\n","    if 'COL_' in arg_replace_list[0]:\n","      arg_replace_list[0] = arg_replace_list[0] + df_suffix_str\n","    if 'COL_' in arg_replace_list[1]:\n","      arg_replace_list[0] = arg_replace_list[0].replace(df_suffix_str, '')\n","      arg_replace_list[1] = arg_replace_list[1] + df_suffix_str\n","    if 'COL_' in arg_replace_list[2]:\n","      arg_replace_list[1] = arg_replace_list[1].replace(df_suffix_str, '')\n","      arg_replace_list[2] = arg_replace_list[2] + df_suffix_str\n","\n","    REPL_0 = arg_replace_list[0]\n","    REPL_1 = arg_replace_list[1]\n","    REPL_2 = arg_replace_list[2]\n","\n","    if replaced_flag == 'N' :\n","      comment = comment.replace(\"dividend divisor\", f\"the result of division where the numerator is {REPL_0} and denominator is {REPL_1} {df_suffix_str}\")                            if 'dividend' in comment else comment\n","      comment = comment.replace('column', f'{REPL_0}').replace('one base to another.', f'base in {REPL_1} to base in {REPL_2}.')                                                      if fn_main == 'conv' else comment\n","      comment = comment.replace('two given strings', f'two given strings in {REPL_0} and {REPL_1} where {REPL_2} gives the maximum edit distance')                                    if fn_main == 'levenshtein' else comment\n","      comment = f'Pyspark code which returns an array containing all the elements in {REPL_0} with starting index as {REPL_1} and length {REPL_2}'                                    if fn_main == 'slice' else comment\n","      comment = comment.replace('input columns', f\"input columns {REPL_0}, {REPL_1} and {REPL_2}\")                                                                                    if fn_main == 'concat' else comment\n","      comment = comment.replace('adds an item', f'inserts value in {REPL_2}').replace('a given array', f\"array {REPL_0}\").replace('array index', f'array index given by {REPL_1}')    if fn_main == 'array_insert' else comment\n","      comment = f'Pyspark code which creates an interval column by combining the values from the DAY, HOUR and MIN columns {df_suffix_str}.'                                          if fn_main == 'make_dt_interval' else comment\n","      comment = f'Pyspark code which creates an interval column by combining the values from the YEAR, MONTH, WEEK and DAY columns {df_suffix_str}.'                                  if fn_main == 'make_interval' else comment\n","      comment = f'Pyspark code which creates a new column with dates built from the values in the YEAR, MONTH and DAY columns {df_suffix_str}.'                                       if fn_main == 'make_date' else comment\n","      comment = comment.replace('from start to stop incrementing by step', f'starting from {REPL_0} to {REPL_1} incrementing by step given by {REPL_2}')                              if fn_main == 'sequence' else comment\n","      comment = f'Pyspark code which creates a new array column by merging the values from the specified columns {REPL_0}, {REPL_1} and {REPL_2}'                                     if fn_main == 'arrays_zip' else comment\n","      comment = comment.replace('a column with the given esp confidence and seed', f'{REPL_0} with the given esp confidence given by {REPL_1} and seed given by {REPL_2}')            if fn_main == 'count_min_sketch' else comment\n","      comment = comment.replace('col2', f'{REPL_1}').replace('col1', f'{REPL_0}').replace('col3', f'{REPL_2}')                                                                        if fn_main == 'nvl2' else comment\n","      comment = comment.replace('a column', f'{REPL_1}').replace('aggregation', f' aggregation on {REPL_2} grouped by values in {REPL_1}')                                            if fn_main == 'pivot' else comment\n","      comment = f\"Pyspark code which Compute the sum of {REPL_2} for each value of {REPL_0} pivoted on {REPL_1}.\"                                                                     if fn_main == 'pivot' and 'which Compute' in comment else comment\n","      comment = comment.replace('in the array', f'in the array {REPL_0}')                                                                                                             if fn_main == 'reduce' else comment\n","      comment = comment.replace('the value', f'the value in {REPL_0}').replace('offset rows', f'offset rows given by {REPL_1}').replace('default', f'default to {REPL_2}')            if fn_main == 'lead' else comment\n","      comment = f\"Returns the n-th input where n is given by {REPL_0}. Returns {REPL_1} when n is 1 and {REPL_2} when n is 2\"                                                         if fn_main == 'elt' else comment\n","      comment = comment.replace('URL.', f'URL where {REPL_0} is the URL string and {REPL_1} is extract part (like HOST, PATH, QUERY etc) with the key as {REPL_2}')                   if fn_main == 'parse_url' else comment\n","      comment = comment.replace('arguments', f'arguments {REPL_1} and {REPL_2}').replace('printf style', f'printf style given by the format in {REPL_0}')                             if fn_main == 'printf' else comment\n","      comment = comment.replace('specified string', f'specified string {REPL_0}').replace('regexp with replacement', f'regular expression in {REPL_1} with replacement in {REPL_2}')  if fn_main == 'regexp_replace' else comment\n","      comment = comment.replace('occurrences of', f'occurrences of {REPL_0}').replace('search with replace', f'of search string in {REPL_1} with replacement in {REPL_2}')            if fn_main == 'replace' else comment\n","      comment = comment.replace('Splits str', f'splits string in {REPL_0}').replace('delimiter', f'delimiter {REPL_1}').replace(' part', f' part given by {REPL_2}')                  if fn_main == 'split_part' else comment\n","      comment = f\"Pyspark code which Returns the substring of string in {REPL_0} that starts at position given by {REPL_1} and is of length given by {REPL_2}\"                        if fn_main in ['substr', 'substring'] else comment\n","      comment = f\"Pyspark code which overlay the specified portion of value in {REPL_0} with replace string in {REPL_1} starting from byte position {REPL_2}.\"                        if fn_main == 'overlay' else comment\n","      comment = comment.replace('a string', f'a string in {REPL_0}').replace('language', f'language given by {REPL_1}').replace('country', f'country given by {REPL_2}')              if fn_main == 'sentences' else comment\n","      comment = comment.replace('A function', '').replace('the srcCol', f'in {REPL_0}').replace('in matching', f'in {REPL_1} with {REPL_2}').split('.')[0]                            if fn_main == 'translate' else comment\n","      comment = comment.replace('of input', f'of the input {REPL_0}').replace('using AES in mode', f'using mode in {REPL_1}').replace('with padding', f'with key {REPL_2}')           if fn_main in ['aes_decrypt', 'try_aes_decrypt'] else comment\n","\n","      comment = comment.replace('given array or map', f'array or map given by {REPL_2} {df_suffix_str}') + f\". It includes {REPL_0} and {REPL_1} in the explode process.\"                   if 'explode' in fn_main else comment\n","      comment = comment.replace('given string value', f'given string in {REPL_0} with {REPL_1} to replace upper-case characters with and {REPL_2} to replace lower-case characters with')   if fn_main == 'mask' else comment\n","\n","      if fn_main in ['max_by', 'min_by']:\n","        rplc_dict = {'max_by': 'maximum', 'min_by': 'minimum'}\n","        comment = f\"Pyspark code which groups by on {REPL_0} and calculates the {[ rplc_dict[x] for x in rplc_dict.keys() if x + '(' in code][0]} of {REPL_1} and {REPL_2}.\" #+ comment\n","\n","      filter_str = '3_ARG_1' if fn_main not in ['aggregate', 'grouping_id'] else '3_ARG_1_IGNORE'\n","      replaced_flag = 'Y'\n","\n","  comment = comment if comment.strip().endswith('.') else f\"{comment.strip()}.\"\n","  result = [comment, filter_str, comment_orig]\n","  return result\n","\n","\n","df_standardize_comments_3 = df_standardize_comments_2.copy()\n","\n","# Standardize Column names calling the function with level 3 first then 2 and then 1\n","df_standardize_comments_3['code_desc_1']      = df_standardize_comments_3.apply(\n","                                                    lambda x: fn_standardize_comments_2(x['code_description'], x['code_snippet_1'], x['function'], x['Category'])[0]  if x['filter'] == '=NA=' else x['code_desc_1'], axis=1)\n","df_standardize_comments_3['filter']           = df_standardize_comments_3.apply(\n","                                                    lambda x: fn_standardize_comments_2(x['code_description'], x['code_snippet_1'], x['function'], x['Category'])[1]  if x['filter'] == '=NA=' else x['filter'], axis=1)\n","df_standardize_comments_3['code_description'] = df_standardize_comments_3.apply(\n","                                                    lambda x: fn_standardize_comments_2(x['code_description'], x['code_snippet_1'], x['function'], x['Category'])[2]  , axis=1)\n","\n","# UNCOMMENT TO DEBUG\n","df_standardize_comments_3[\n","  (df_standardize_comments_3['filter'].str.contains('3_ARG'))\n","  #(df_standardize_comments_3['filter'].str.contains('=NA='))\n","#].info()\n","#].count()\n","#][['filter', 'code_desc_1', 'code_snippet_1', 'code_description', 'code_snippet', 'function', 'Category' ]].style.set_properties(**{'text-align': 'left'})\n","][['function', 'Category', 'code_desc_1', 'code_snippet_1', 'code_snippet', 'filter' ]].style.set_properties(**{'text-align': 'left'})\n","\n","#df_standardize_comments_3.style.set_properties(**{'text-align': 'left'})\n","\n","df_standardize_comments_3.head().style.set_properties(**{'text-align': 'left'})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":484},"id":"xJOsivN6roy4","executionInfo":{"status":"ok","timestamp":1710278186877,"user_tz":-330,"elapsed":55,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"9040342c-dbbd-437a-cc7a-35b620abeed2"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x791d9446ea70>"],"text/html":["<style type=\"text/css\">\n","#T_f4437_row0_col0, #T_f4437_row0_col1, #T_f4437_row0_col2, #T_f4437_row0_col3, #T_f4437_row0_col4, #T_f4437_row0_col5, #T_f4437_row0_col6, #T_f4437_row0_col7, #T_f4437_row1_col0, #T_f4437_row1_col1, #T_f4437_row1_col2, #T_f4437_row1_col3, #T_f4437_row1_col4, #T_f4437_row1_col5, #T_f4437_row1_col6, #T_f4437_row1_col7, #T_f4437_row2_col0, #T_f4437_row2_col1, #T_f4437_row2_col2, #T_f4437_row2_col3, #T_f4437_row2_col4, #T_f4437_row2_col5, #T_f4437_row2_col6, #T_f4437_row2_col7, #T_f4437_row3_col0, #T_f4437_row3_col1, #T_f4437_row3_col2, #T_f4437_row3_col3, #T_f4437_row3_col4, #T_f4437_row3_col5, #T_f4437_row3_col6, #T_f4437_row3_col7, #T_f4437_row4_col0, #T_f4437_row4_col1, #T_f4437_row4_col2, #T_f4437_row4_col3, #T_f4437_row4_col4, #T_f4437_row4_col5, #T_f4437_row4_col6, #T_f4437_row4_col7 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_f4437\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_f4437_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_f4437_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_f4437_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_f4437_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_f4437_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_f4437_level0_col5\" class=\"col_heading level0 col5\" >code_snippet_1</th>\n","      <th id=\"T_f4437_level0_col6\" class=\"col_heading level0 col6\" >code_desc_1</th>\n","      <th id=\"T_f4437_level0_col7\" class=\"col_heading level0 col7\" >filter</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_f4437_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_f4437_row0_col0\" class=\"data row0 col0\" >Pyspark code to Write a DataFrame into a CSV file.</td>\n","      <td id=\"T_f4437_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_f4437_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_f4437_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_f4437_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_f4437_row0_col5\" class=\"data row0 col5\" >MY_DF.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_f4437_row0_col6\" class=\"data row0 col6\" >Pyspark code to Write a DataFrame into a CSV file.The data is written to file or directory MY_DIR.</td>\n","      <td id=\"T_f4437_row0_col7\" class=\"data row0 col7\" >1_MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_f4437_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_f4437_row1_col0\" class=\"data row1 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_f4437_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_f4437_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_f4437_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_f4437_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_f4437_row1_col5\" class=\"data row1 col5\" >MY_DF.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_f4437_row1_col6\" class=\"data row1 col6\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.The data is written to file or directory MY_DIR.</td>\n","      <td id=\"T_f4437_row1_col7\" class=\"data row1 col7\" >1_MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_f4437_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_f4437_row2_col0\" class=\"data row2 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.</td>\n","      <td id=\"T_f4437_row2_col1\" class=\"data row2 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_f4437_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_f4437_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_f4437_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_f4437_row2_col5\" class=\"data row2 col5\" >MY_DF = spark.read.csv(MY_DIR, schema=MY_DF.schema, nullValue=\"LIT_STR_0\")</td>\n","      <td id=\"T_f4437_row2_col6\" class=\"data row2 col6\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'LIT_STR_0'.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_f4437_row2_col7\" class=\"data row2 col7\" >1_MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_f4437_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_f4437_row3_col0\" class=\"data row3 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_f4437_row3_col1\" class=\"data row3 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_f4437_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_f4437_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_f4437_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_f4437_row3_col5\" class=\"data row3 col5\" >MY_DF = spark.read.csv(MY_DIR, schema=MY_DF.schema, nullValue=\"LIT_STR_0\")</td>\n","      <td id=\"T_f4437_row3_col6\" class=\"data row3 col6\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_f4437_row3_col7\" class=\"data row3 col7\" >1_MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_f4437_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_f4437_row4_col0\" class=\"data row4 col0\" >Pyspark code to Write a DataFrame into a JSON file.</td>\n","      <td id=\"T_f4437_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_f4437_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_f4437_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_f4437_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_f4437_row4_col5\" class=\"data row4 col5\" >MY_DF.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_f4437_row4_col6\" class=\"data row4 col6\" >Pyspark code to Write a DataFrame into a JSON file.The data is written to file or directory MY_DIR.</td>\n","      <td id=\"T_f4437_row4_col7\" class=\"data row4 col7\" >1_MY_DIR</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["# HANDLE COMMENTS PASS 3: validate how many rows per filter category were standardized\n","\n","fn_display_header(\"HANDLE COMMENTS PASS 3: validate how many rows per filter category were standardized\")\n","\n","df_standardize_comments_3.groupby('filter').size().reset_index(name='count')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"bolOz3fitrFB","executionInfo":{"status":"ok","timestamp":1710278190815,"user_tz":-330,"elapsed":990,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"0a7ddbbc-bb06-4a65-e194-3d1b8b5f149d"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           HANDLE COMMENTS PASS 3: validate how many rows per filter category were standardized\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["               filter  count\n","0              0_COLS     37\n","1            1_MY_DIR     90\n","2        1_argument_1     13\n","3           1_array_1     17\n","4          1_column_1     78\n","5      1_fn_explode_1      5\n","6            1_misc_1     30\n","7            1_misc_2     14\n","8             1_str_1     29\n","9            1_time_1     46\n","10          1_value_1     14\n","11          1_value_2      4\n","12          1_value_3     23\n","13          1_value_4     27\n","14            2_ARG_1      3\n","15           2_ARG_10     19\n","16           2_ARG_11     25\n","17    2_ARG_11_IGNORE      4\n","18         2_ARG_12_A      8\n","19         2_ARG_12_B     11\n","20  2_ARG_12_B_IGNORE      2\n","21         2_ARG_12_C      8\n","22         2_ARG_12_D      7\n","23  2_ARG_12_D_IGNORE      2\n","24         2_ARG_12_E      6\n","25         2_ARG_12_F     18\n","26           2_ARG_13     10\n","27    2_ARG_13_IGNORE      1\n","28            2_ARG_2      7\n","29            2_ARG_3      5\n","30            2_ARG_4      9\n","31            2_ARG_5     12\n","32            2_ARG_6     11\n","33            2_ARG_7      6\n","34            2_ARG_8     11\n","35            2_ARG_9     11\n","36            3_ARG_1     41\n","37     3_ARG_1_IGNORE      2\n","38               =NA=     28"],"text/html":["\n","  <div id=\"df-fee5c385-0c4c-482f-9d0e-47621412b25d\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filter</th>\n","      <th>count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0_COLS</td>\n","      <td>37</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1_MY_DIR</td>\n","      <td>90</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1_argument_1</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1_array_1</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1_column_1</td>\n","      <td>78</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1_fn_explode_1</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1_misc_1</td>\n","      <td>30</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1_misc_2</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1_str_1</td>\n","      <td>29</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1_time_1</td>\n","      <td>46</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1_value_1</td>\n","      <td>14</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1_value_2</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1_value_3</td>\n","      <td>23</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1_value_4</td>\n","      <td>27</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2_ARG_1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2_ARG_10</td>\n","      <td>19</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2_ARG_11</td>\n","      <td>25</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2_ARG_11_IGNORE</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2_ARG_12_A</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>2_ARG_12_B</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>2_ARG_12_B_IGNORE</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>2_ARG_12_C</td>\n","      <td>8</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>2_ARG_12_D</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>2_ARG_12_D_IGNORE</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>2_ARG_12_E</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>2_ARG_12_F</td>\n","      <td>18</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>2_ARG_13</td>\n","      <td>10</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>2_ARG_13_IGNORE</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>2_ARG_2</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>2_ARG_3</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>2_ARG_4</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>2_ARG_5</td>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>2_ARG_6</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>2_ARG_7</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>2_ARG_8</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>2_ARG_9</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>3_ARG_1</td>\n","      <td>41</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>3_ARG_1_IGNORE</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>=NA=</td>\n","      <td>28</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fee5c385-0c4c-482f-9d0e-47621412b25d')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-fee5c385-0c4c-482f-9d0e-47621412b25d button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-fee5c385-0c4c-482f-9d0e-47621412b25d');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-62001e78-3173-49b8-9b6d-cfe9d772ceb4\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-62001e78-3173-49b8-9b6d-cfe9d772ceb4')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-62001e78-3173-49b8-9b6d-cfe9d772ceb4 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"df_standardize_comments_3\",\n  \"rows\": 39,\n  \"fields\": [\n    {\n      \"column\": \"filter\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 39,\n        \"samples\": [\n          \"2_ARG_7\",\n          \"3_ARG_1\",\n          \"1_column_1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19,\n        \"min\": 1,\n        \"max\": 90,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          46,\n          12,\n          29\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["<br>\n","<br>\n","<br>\n","\n","----\n","# <b>FINALIZE: APPEND PRE AND POST STANDARDIZATION DATASETS</b>\n","----\n","<br>"],"metadata":{"id":"3Yh2nPPsS3YE"}},{"cell_type":"code","source":["# RECREATE THE PRE STANDARDIZATION DATASET WITH DESIRED COLUMNS\n","\n","df_pre_standardization = df_standardize_comments_3[['code_description', 'code_snippet', 'import_line', 'Category', 'function']].copy()\n","\n","# Add a column to identify the original records\n","df_pre_standardization['origin_str'] = 'original'\n","\n","fn_display_header('PRE-STANDADIZATION COLUMN/COUNT DETAILS')\n","df_pre_standardization.info()\n","\n","fn_display_header('DISPLAY FEW ROWS IN PRE-STANDADIZATION DATASET')\n","df_pre_standardization.head().style.set_properties(**{'text-align': 'left'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":609},"id":"DNmbmbSIro5h","executionInfo":{"status":"ok","timestamp":1710278197766,"user_tz":-330,"elapsed":722,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"a6a0fda9-73c2-4cf4-87a3-d9190fe02f90"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           PRE-STANDADIZATION COLUMN/COUNT DETAILS\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 694 entries, 0 to 693\n","Data columns (total 6 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   code_description  694 non-null    object\n"," 1   code_snippet      694 non-null    object\n"," 2   import_line       62 non-null     object\n"," 3   Category          694 non-null    object\n"," 4   function          694 non-null    object\n"," 5   origin_str        694 non-null    object\n","dtypes: object(6)\n","memory usage: 32.7+ KB\n","--------------------------------------------------------------------------------\n","           DISPLAY FEW ROWS IN PRE-STANDADIZATION DATASET\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x791d9404b220>"],"text/html":["<style type=\"text/css\">\n","#T_cdd5a_row0_col0, #T_cdd5a_row0_col1, #T_cdd5a_row0_col2, #T_cdd5a_row0_col3, #T_cdd5a_row0_col4, #T_cdd5a_row0_col5, #T_cdd5a_row1_col0, #T_cdd5a_row1_col1, #T_cdd5a_row1_col2, #T_cdd5a_row1_col3, #T_cdd5a_row1_col4, #T_cdd5a_row1_col5, #T_cdd5a_row2_col0, #T_cdd5a_row2_col1, #T_cdd5a_row2_col2, #T_cdd5a_row2_col3, #T_cdd5a_row2_col4, #T_cdd5a_row2_col5, #T_cdd5a_row3_col0, #T_cdd5a_row3_col1, #T_cdd5a_row3_col2, #T_cdd5a_row3_col3, #T_cdd5a_row3_col4, #T_cdd5a_row3_col5, #T_cdd5a_row4_col0, #T_cdd5a_row4_col1, #T_cdd5a_row4_col2, #T_cdd5a_row4_col3, #T_cdd5a_row4_col4, #T_cdd5a_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_cdd5a\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_cdd5a_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_cdd5a_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_cdd5a_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_cdd5a_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_cdd5a_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_cdd5a_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_cdd5a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_cdd5a_row0_col0\" class=\"data row0 col0\" >Pyspark code to Write a DataFrame into a CSV file.</td>\n","      <td id=\"T_cdd5a_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_cdd5a_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_cdd5a_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_cdd5a_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_cdd5a_row0_col5\" class=\"data row0 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cdd5a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_cdd5a_row1_col0\" class=\"data row1 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_cdd5a_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_cdd5a_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_cdd5a_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_cdd5a_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_cdd5a_row1_col5\" class=\"data row1 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cdd5a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_cdd5a_row2_col0\" class=\"data row2 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.</td>\n","      <td id=\"T_cdd5a_row2_col1\" class=\"data row2 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_cdd5a_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_cdd5a_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_cdd5a_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_cdd5a_row2_col5\" class=\"data row2 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cdd5a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_cdd5a_row3_col0\" class=\"data row3 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_cdd5a_row3_col1\" class=\"data row3 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_cdd5a_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_cdd5a_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_cdd5a_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_cdd5a_row3_col5\" class=\"data row3 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cdd5a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_cdd5a_row4_col0\" class=\"data row4 col0\" >Pyspark code to Write a DataFrame into a JSON file.</td>\n","      <td id=\"T_cdd5a_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_cdd5a_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_cdd5a_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_cdd5a_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_cdd5a_row4_col5\" class=\"data row4 col5\" >original</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# KEEP STANDARDIZATION DATASET WITH DESIRED COLUMNS ONLY. RENAME DERIVED COLUMNS\n","\n","df_standard_comments = df_standardize_comments_3[\n","    (~df_standardize_comments_3['filter'].str.contains('IGNORE'))\n","    & (~df_standardize_comments_3['filter'].str.contains('=NA='))\n","    & (df_standardize_comments_3['code_desc_1'].str.contains('(LIT_(INT|DEC|STR)_\\d+|COL_[A-Z]|YEAR|MONTH|DAY|HOUR|MIN|SEC)'))\n","].copy()\n","\n","# Keep only necessary columns\n","df_standard_comments = df_standard_comments[['code_desc_1', 'code_snippet_1', 'import_line', 'Category', 'function']]\n","\n","# Rename derived columns to original names\n","df_standard_comments.rename(columns={'code_desc_1': 'code_description', 'code_snippet_1': 'code_snippet'}, inplace=True)\n","\n","# Add a column to identify the original records\n","df_standard_comments['origin_str'] = 'standardization'\n","\n","fn_display_header('STANDARDIZATION DATASET COLUMN/COUNT DETAILS')\n","df_standard_comments.info()\n","\n","fn_display_header('DISPLAY FEW ROWS IN PRE-STANDADIZATION DATASET')\n","df_standard_comments.head().style.set_properties(**{'text-align': 'left'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":643},"id":"tlXSi0euro8c","executionInfo":{"status":"ok","timestamp":1710278203362,"user_tz":-330,"elapsed":509,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"b89c4975-015c-44a5-dc56-06e93a623848"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           STANDARDIZATION DATASET COLUMN/COUNT DETAILS\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 521 entries, 2 to 693\n","Data columns (total 6 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   code_description  521 non-null    object\n"," 1   code_snippet      521 non-null    object\n"," 2   import_line       49 non-null     object\n"," 3   Category          521 non-null    object\n"," 4   function          521 non-null    object\n"," 5   origin_str        521 non-null    object\n","dtypes: object(6)\n","memory usage: 28.5+ KB\n","--------------------------------------------------------------------------------\n","           DISPLAY FEW ROWS IN PRE-STANDADIZATION DATASET\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x791d9404a260>"],"text/html":["<style type=\"text/css\">\n","#T_694f0_row0_col0, #T_694f0_row0_col1, #T_694f0_row0_col2, #T_694f0_row0_col3, #T_694f0_row0_col4, #T_694f0_row0_col5, #T_694f0_row1_col0, #T_694f0_row1_col1, #T_694f0_row1_col2, #T_694f0_row1_col3, #T_694f0_row1_col4, #T_694f0_row1_col5, #T_694f0_row2_col0, #T_694f0_row2_col1, #T_694f0_row2_col2, #T_694f0_row2_col3, #T_694f0_row2_col4, #T_694f0_row2_col5, #T_694f0_row3_col0, #T_694f0_row3_col1, #T_694f0_row3_col2, #T_694f0_row3_col3, #T_694f0_row3_col4, #T_694f0_row3_col5, #T_694f0_row4_col0, #T_694f0_row4_col1, #T_694f0_row4_col2, #T_694f0_row4_col3, #T_694f0_row4_col4, #T_694f0_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_694f0\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_694f0_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_694f0_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_694f0_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_694f0_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_694f0_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_694f0_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_694f0_level0_row0\" class=\"row_heading level0 row0\" >2</th>\n","      <td id=\"T_694f0_row0_col0\" class=\"data row0 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'LIT_STR_0'.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_694f0_row0_col1\" class=\"data row0 col1\" >MY_DF = spark.read.csv(MY_DIR, schema=MY_DF.schema, nullValue=\"LIT_STR_0\")</td>\n","      <td id=\"T_694f0_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_694f0_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_694f0_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_694f0_row0_col5\" class=\"data row0 col5\" >standardization</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_694f0_level0_row1\" class=\"row_heading level0 row1\" >14</th>\n","      <td id=\"T_694f0_row1_col0\" class=\"data row1 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'LIT_STR_0', and 'header' option set to `True`.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_694f0_row1_col1\" class=\"data row1 col1\" >MY_DF = spark.read.load(MY_DIR, schema=MY_DF.schema, format=\"csv\", nullValue=\"LIT_STR_0\", header=True)</td>\n","      <td id=\"T_694f0_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_694f0_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_694f0_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.load</td>\n","      <td id=\"T_694f0_row1_col5\" class=\"data row1 col5\" >standardization</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_694f0_level0_row2\" class=\"row_heading level0 row2\" >18</th>\n","      <td id=\"T_694f0_row2_col0\" class=\"data row2 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'LIT_STR_0'.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_694f0_row2_col1\" class=\"data row2 col1\" >MY_DF = spark.read.schema(MY_DF.schema).option(\"nullValue\", \"LIT_STR_0\").format('csv').load(MY_DIR)</td>\n","      <td id=\"T_694f0_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_694f0_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_694f0_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.option</td>\n","      <td id=\"T_694f0_row2_col5\" class=\"data row2 col5\" >standardization</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_694f0_level0_row3\" class=\"row_heading level0 row3\" >22</th>\n","      <td id=\"T_694f0_row3_col0\" class=\"data row3 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'LIT_STR_0', and 'header' option set to `True`.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_694f0_row3_col1\" class=\"data row3 col1\" >MY_DF = spark.read.options(nullValue=\"LIT_STR_0\",header=True).format('csv').load(MY_DIR)</td>\n","      <td id=\"T_694f0_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_694f0_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_694f0_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.options</td>\n","      <td id=\"T_694f0_row3_col5\" class=\"data row3 col5\" >standardization</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_694f0_level0_row4\" class=\"row_heading level0 row4\" >40</th>\n","      <td id=\"T_694f0_row4_col0\" class=\"data row4 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'LIT_STR_0'.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_694f0_row4_col1\" class=\"data row4 col1\" >MY_DF = spark.read.schema(MY_DF.schema).format(\"csv\").option(\"nullValue\", \"LIT_STR_0\").load(MY_DIR)</td>\n","      <td id=\"T_694f0_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_694f0_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_694f0_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameWriter.csv</td>\n","      <td id=\"T_694f0_row4_col5\" class=\"data row4 col5\" >standardization</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":[" # CONCATENATE ORIGINAL AND STANDARDIZED DATASETS\n","\n","# Concatenate\n","df_final = pd.concat([df_pre_standardization, df_standard_comments], ignore_index=True)\n","\n","fn_display_header('STANDARDIZATION DATASET COLUMN/COUNT DETAILS')\n","df_final.info()\n","\n","fn_display_header('DISPLAY FEW ROWS IN PRE-STANDADIZATION DATASET')\n","df_final.head().style.set_properties(**{'text-align': 'left'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":609},"id":"8m2fXF3Z4SM1","executionInfo":{"status":"ok","timestamp":1710278208521,"user_tz":-330,"elapsed":39,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"78260ab9-4462-4246-cfbd-39941c722fe4"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           STANDARDIZATION DATASET COLUMN/COUNT DETAILS\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1215 entries, 0 to 1214\n","Data columns (total 6 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   code_description  1215 non-null   object\n"," 1   code_snippet      1215 non-null   object\n"," 2   import_line       111 non-null    object\n"," 3   Category          1215 non-null   object\n"," 4   function          1215 non-null   object\n"," 5   origin_str        1215 non-null   object\n","dtypes: object(6)\n","memory usage: 57.1+ KB\n","--------------------------------------------------------------------------------\n","           DISPLAY FEW ROWS IN PRE-STANDADIZATION DATASET\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x791d9bffb730>"],"text/html":["<style type=\"text/css\">\n","#T_d04ab_row0_col0, #T_d04ab_row0_col1, #T_d04ab_row0_col2, #T_d04ab_row0_col3, #T_d04ab_row0_col4, #T_d04ab_row0_col5, #T_d04ab_row1_col0, #T_d04ab_row1_col1, #T_d04ab_row1_col2, #T_d04ab_row1_col3, #T_d04ab_row1_col4, #T_d04ab_row1_col5, #T_d04ab_row2_col0, #T_d04ab_row2_col1, #T_d04ab_row2_col2, #T_d04ab_row2_col3, #T_d04ab_row2_col4, #T_d04ab_row2_col5, #T_d04ab_row3_col0, #T_d04ab_row3_col1, #T_d04ab_row3_col2, #T_d04ab_row3_col3, #T_d04ab_row3_col4, #T_d04ab_row3_col5, #T_d04ab_row4_col0, #T_d04ab_row4_col1, #T_d04ab_row4_col2, #T_d04ab_row4_col3, #T_d04ab_row4_col4, #T_d04ab_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_d04ab\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_d04ab_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_d04ab_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_d04ab_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_d04ab_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_d04ab_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_d04ab_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_d04ab_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_d04ab_row0_col0\" class=\"data row0 col0\" >Pyspark code to Write a DataFrame into a CSV file.</td>\n","      <td id=\"T_d04ab_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_d04ab_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_d04ab_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_d04ab_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_d04ab_row0_col5\" class=\"data row0 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d04ab_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_d04ab_row1_col0\" class=\"data row1 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_d04ab_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_d04ab_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_d04ab_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_d04ab_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_d04ab_row1_col5\" class=\"data row1 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d04ab_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_d04ab_row2_col0\" class=\"data row2 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.</td>\n","      <td id=\"T_d04ab_row2_col1\" class=\"data row2 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_d04ab_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_d04ab_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_d04ab_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_d04ab_row2_col5\" class=\"data row2 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d04ab_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_d04ab_row3_col0\" class=\"data row3 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_d04ab_row3_col1\" class=\"data row3 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_d04ab_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_d04ab_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_d04ab_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_d04ab_row3_col5\" class=\"data row3 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d04ab_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_d04ab_row4_col0\" class=\"data row4 col0\" >Pyspark code to Write a DataFrame into a JSON file.</td>\n","      <td id=\"T_d04ab_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_d04ab_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_d04ab_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_d04ab_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_d04ab_row4_col5\" class=\"data row4 col5\" >original</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# WRITE FINAL DATASET TO CSV AND DOWNLOAD\n","\n","DOWNLOAD_FLAG = 'N'\n","if DOWNLOAD_FLAG == 'Y':\n","  df_final.to_csv('ETL_P3_manual_data_standardization.csv')\n","\n","  from google.colab import files\n","  files.download('ETL_P3_manual_data_standardization.csv')"],"metadata":{"id":"YeOSxOaXrpAa","executionInfo":{"status":"ok","timestamp":1710278218270,"user_tz":-330,"elapsed":8,"user":{"displayName":"SK G","userId":"17874234191554613076"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VmOxP2IXKKRR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"hpGBTsPwKKXf"},"execution_count":null,"outputs":[]}]}