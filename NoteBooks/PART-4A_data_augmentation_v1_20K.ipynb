{"cells":[{"cell_type":"markdown","metadata":{"id":"jdYDazNza3x0"},"source":["# DATA AUGMENTAION\n","\n","----------------------------\n","#### GIST OF CHANGES DONE IN THIS NOTEBOOK\n","----------------------------\n","-   Need for data augmentation arose because the initial webscraped data volume was lower than expected\n","\n","-   Standardized codes and comments will be augmented with the below approaches\n","    - <b>ADDITIONAL STANDARDIZATION</b> Existing rows were enhanced\n","      - All column level operations will be enhance to include the alias(col) code since the ML model will be trained to generate column level codes for column level operations.\n","      - Removed first() function from column level operations as it is a aggregation function\n","      - Enhanced comments to describe the sort process if present in code\n","      - Removed show(), head(), printSchema() as these are display functions  \n","    -   <b>TEXT AUGMENTATION</b>  -- DISCARDED --\n","      - SUMY and T5 Summarizers were found to be lacking in changing the text descriptions enough to consider as alternate descriptions. This is probably because of the small text size and these summarizers do not use synonyyms to summarize.\n","      - Synonym replacement works but the replacements with the basic nltk code seem to be uncommon words which are less likely to be used in real world scenarios. While this had potential and can be fine tuned for replacing only certain keywords in the code descriptions, but in the interest of time, this approach is also not considered and can be taken up in future.\n","    - <b>MANUAL AUGMENTATION</b> New rows were added.\n","      - Some function options like sort, read and write etc were exploded to have few possible arguments in the code. i.e. write mode can be append or ignore and hence this two rows were added for these possibilities.\n","      - Standardized columns like COL_A, COL_B, MY_ALIAS_A etc will replaced with random values in both code and comments.\n","      - Standarized Literals like LIT_STR_1, LIT_DEC_1, LIT_INT_1 will replaced with random values in both code and comments.\n","\n","    -   <b>NOTE</b>\n","      - This notebook can augment data exponentially. However, the base webscraped data is quite small due to which, augmenting some scenarios significanly might increase the data size, but it could also lead to overfitting the basic nature of the descriptions do not change a lot.\n","      - Initially agumented data up to 40K was used for training but due to compute constraints and Collab limits, the trainings were cumbersome to tune and refine.\n","      - Datasets with 20K and eventually a dataset size of 11K was selected as it was found to be less time consuming to train and tune.\n","      - Since all the selected models are ideally supposed to be trained with a huge dataset significantly, selecting a smaller subset of the training data means that we assume the risk of inaccuracies and overfitting of the models.\n","\n","----------------------------\n","<br>\n","<br>\n"]},{"cell_type":"markdown","metadata":{"id":"9bzhkOdCsNDJ"},"source":["<br>\n","\n","# Import/Install Necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mTYySyCN_Z9H","executionInfo":{"status":"ok","timestamp":1721677992365,"user_tz":-330,"elapsed":14376,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"ec210090-0d52-4046-df82-131af57a973d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sumy in /usr/local/lib/python3.10/dist-packages (0.11.0)\n","Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.6.2)\n","Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.1.20)\n","Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.31.0)\n","Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.10/dist-packages (from sumy) (24.6.1)\n","Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (3.0.4)\n","Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2024.5.15)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.7.4)\n","Requirement already satisfied: googletrans==4.0.0-rc1 in /usr/local/lib/python3.10/dist-packages (4.0.0rc1)\n","Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.10/dist-packages (from googletrans==4.0.0-rc1) (0.13.3)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.7.4)\n","Requirement already satisfied: hstspreload in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.7.1)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n","Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.4)\n","Requirement already satisfied: idna==2.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2.10)\n","Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.5.0)\n","Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.1)\n","Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (0.9.0)\n","Requirement already satisfied: h2==3.* in /usr/local/lib/python3.10/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.2.0)\n","Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (5.2.0)\n","Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1) (3.0.0)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":10}],"source":["# IMPORT/INSTALL\n","\n","!pip install sumy\n","!pip install googletrans==4.0.0-rc1\n","\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OfPB61Gz8VfY"},"outputs":[],"source":["# IMPORT NECESSARY PACKAGES\n","import io\n","import re\n","import pandas as pd\n","from tqdm import tqdm\n","from google.colab import drive, files\n","from nltk.corpus import stopwords\n","\n","import warnings\n","warnings.simplefilter(action='ignore')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pEiTi6z_8rTF"},"outputs":[],"source":["# DISPLAY FUNCTIONS\n","def fn_display_header(msg):\n","  print('-' * 80)\n","  print(' ' * 10, msg)\n","  print('-' * 80)\n","\n","def fn_display_message(msg):\n","  print(msg)"]},{"cell_type":"markdown","metadata":{"id":"CLY2Hn2eC86g"},"source":["<br>\n","\n","# READ INPUT DATA"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":810},"id":"AJTTKnDD3UA8","executionInfo":{"status":"ok","timestamp":1721677992367,"user_tz":-330,"elapsed":26,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"0e6fe67a-d936-4a1a-ce12-2f26de6eb5e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           Display Source Dataframe Metadata\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1215 entries, 0 to 1214\n","Data columns (total 6 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   code_description  1215 non-null   object\n"," 1   code_snippet      1215 non-null   object\n"," 2   import_line       111 non-null    object\n"," 3   Category          1215 non-null   object\n"," 4   function          1215 non-null   object\n"," 5   origin_str        1215 non-null   object\n","dtypes: object(6)\n","memory usage: 57.1+ KB\n","--------------------------------------------------------------------------------\n","           Drop Duplicates code_snippet and code_description\n","--------------------------------------------------------------------------------\n","\n","\n"," --> Display Count Before deleting records: 1215\n"," --> Display Count After dropping duplicates: 1195\n","\n","\n","--------------------------------------------------------------------------------\n","           Display df_init Dataframe\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7eb95bf0a170>"],"text/html":["<style type=\"text/css\">\n","#T_faaa9_row0_col0, #T_faaa9_row0_col1, #T_faaa9_row0_col2, #T_faaa9_row0_col3, #T_faaa9_row0_col4, #T_faaa9_row0_col5, #T_faaa9_row1_col0, #T_faaa9_row1_col1, #T_faaa9_row1_col2, #T_faaa9_row1_col3, #T_faaa9_row1_col4, #T_faaa9_row1_col5, #T_faaa9_row2_col0, #T_faaa9_row2_col1, #T_faaa9_row2_col2, #T_faaa9_row2_col3, #T_faaa9_row2_col4, #T_faaa9_row2_col5, #T_faaa9_row3_col0, #T_faaa9_row3_col1, #T_faaa9_row3_col2, #T_faaa9_row3_col3, #T_faaa9_row3_col4, #T_faaa9_row3_col5, #T_faaa9_row4_col0, #T_faaa9_row4_col1, #T_faaa9_row4_col2, #T_faaa9_row4_col3, #T_faaa9_row4_col4, #T_faaa9_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_faaa9\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_faaa9_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_faaa9_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_faaa9_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_faaa9_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_faaa9_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_faaa9_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_faaa9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_faaa9_row0_col0\" class=\"data row0 col0\" >Pyspark code to Write a DataFrame into a CSV file.</td>\n","      <td id=\"T_faaa9_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_faaa9_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_faaa9_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_faaa9_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_faaa9_row0_col5\" class=\"data row0 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_faaa9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_faaa9_row1_col0\" class=\"data row1 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_faaa9_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_faaa9_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_faaa9_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_faaa9_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_faaa9_row1_col5\" class=\"data row1 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_faaa9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_faaa9_row2_col0\" class=\"data row2 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.</td>\n","      <td id=\"T_faaa9_row2_col1\" class=\"data row2 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_faaa9_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_faaa9_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_faaa9_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_faaa9_row2_col5\" class=\"data row2 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_faaa9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_faaa9_row3_col0\" class=\"data row3 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_faaa9_row3_col1\" class=\"data row3 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_faaa9_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_faaa9_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_faaa9_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_faaa9_row3_col5\" class=\"data row3 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_faaa9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_faaa9_row4_col0\" class=\"data row4 col0\" >Pyspark code to Write a DataFrame into a JSON file.</td>\n","      <td id=\"T_faaa9_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_faaa9_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_faaa9_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_faaa9_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_faaa9_row4_col5\" class=\"data row4 col5\" >original</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":13}],"source":["# READ WEB SCRAPED DATA INTO DATAFRAME AND DISPLAY DETAILS\n","\n","df_raw = pd.read_csv('ETL_P3_manual_data_standardization_v1.csv').drop('Unnamed: 0', axis=1)\n","\n","fn_display_header('Display Source Dataframe Metadata')\n","df_raw.info()\n","\n","# Drop Duplicates and NULL code_snippet or code_description\n","fn_display_header('Drop Duplicates code_snippet and code_description')\n","fn_display_message(f'\\n\\n --> Display Count Before deleting records: {df_raw.shape[0]}')\n","df_init = df_raw.drop_duplicates()\n","fn_display_message(f' --> Display Count After dropping duplicates: {df_init.shape[0]}\\n\\n')\n","\n","fn_display_header('Display df_init Dataframe')\n","#df_raw.style.set_properties(**{'text-align': 'left'})\n","df_init.head().style.set_properties(**{'text-align': 'left'})\n"]},{"cell_type":"markdown","metadata":{"id":"SE81pGSNM2rR"},"source":["<br>\n","<br>\n","<br>\n","\n","----\n","# <b>ADDITIONAL STANDARDIZATION</b>\n","----\n"]},{"cell_type":"markdown","metadata":{"id":"xNR9sebLrXhJ"},"source":["<br>\n","\n","# Standardize: Standardize Aliases"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"DNmbmbSIro5h","executionInfo":{"status":"ok","timestamp":1721677992367,"user_tz":-330,"elapsed":20,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"ec6ee2e1-591c-4722-c988-a64c0709fc91"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7eb95c0cf640>"],"text/html":["<style type=\"text/css\">\n","#T_d6c5d_row0_col0, #T_d6c5d_row0_col1, #T_d6c5d_row0_col2, #T_d6c5d_row0_col3, #T_d6c5d_row0_col4, #T_d6c5d_row0_col5, #T_d6c5d_row0_col6, #T_d6c5d_row1_col0, #T_d6c5d_row1_col1, #T_d6c5d_row1_col2, #T_d6c5d_row1_col3, #T_d6c5d_row1_col4, #T_d6c5d_row1_col5, #T_d6c5d_row1_col6, #T_d6c5d_row2_col0, #T_d6c5d_row2_col1, #T_d6c5d_row2_col2, #T_d6c5d_row2_col3, #T_d6c5d_row2_col4, #T_d6c5d_row2_col5, #T_d6c5d_row2_col6, #T_d6c5d_row3_col0, #T_d6c5d_row3_col1, #T_d6c5d_row3_col2, #T_d6c5d_row3_col3, #T_d6c5d_row3_col4, #T_d6c5d_row3_col5, #T_d6c5d_row3_col6, #T_d6c5d_row4_col0, #T_d6c5d_row4_col1, #T_d6c5d_row4_col2, #T_d6c5d_row4_col3, #T_d6c5d_row4_col4, #T_d6c5d_row4_col5, #T_d6c5d_row4_col6 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_d6c5d\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_d6c5d_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_d6c5d_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_d6c5d_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_d6c5d_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_d6c5d_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_d6c5d_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","      <th id=\"T_d6c5d_level0_col6\" class=\"col_heading level0 col6\" >alias_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_d6c5d_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_d6c5d_row0_col0\" class=\"data row0 col0\" >Pyspark code to Write a DataFrame into a CSV file.</td>\n","      <td id=\"T_d6c5d_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_d6c5d_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_d6c5d_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_d6c5d_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_d6c5d_row0_col5\" class=\"data row0 col5\" >original</td>\n","      <td id=\"T_d6c5d_row0_col6\" class=\"data row0 col6\" ></td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d6c5d_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_d6c5d_row1_col0\" class=\"data row1 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_d6c5d_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_d6c5d_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_d6c5d_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_d6c5d_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_d6c5d_row1_col5\" class=\"data row1 col5\" >original</td>\n","      <td id=\"T_d6c5d_row1_col6\" class=\"data row1 col6\" ></td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d6c5d_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_d6c5d_row2_col0\" class=\"data row2 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.</td>\n","      <td id=\"T_d6c5d_row2_col1\" class=\"data row2 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_d6c5d_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_d6c5d_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_d6c5d_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_d6c5d_row2_col5\" class=\"data row2 col5\" >original</td>\n","      <td id=\"T_d6c5d_row2_col6\" class=\"data row2 col6\" ></td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d6c5d_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_d6c5d_row3_col0\" class=\"data row3 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_d6c5d_row3_col1\" class=\"data row3 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_d6c5d_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_d6c5d_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_d6c5d_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_d6c5d_row3_col5\" class=\"data row3 col5\" >original</td>\n","      <td id=\"T_d6c5d_row3_col6\" class=\"data row3 col6\" ></td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_d6c5d_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_d6c5d_row4_col0\" class=\"data row4 col0\" >Pyspark code to Write a DataFrame into a JSON file.</td>\n","      <td id=\"T_d6c5d_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_d6c5d_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_d6c5d_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_d6c5d_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_d6c5d_row4_col5\" class=\"data row4 col5\" >original</td>\n","      <td id=\"T_d6c5d_row4_col6\" class=\"data row4 col6\" ></td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":14}],"source":["# ENHANCE COMMENTS TO INCLUDE ALIASES IF APPLICABLE\n","\n","# function to handle alias comments.\n","def fn_handle_alias_comments(comment, code, function, category):\n","  fn_main = function.split('.')[-1].strip()\n","\n","  if not comment.strip().endswith('.'):\n","      comment = comment + '.'\n","\n","  alias_str = ''\n","  replaced_flag = 'N'\n","  ignore_replace_categories = ['Aggregate Functions', 'Grouping', 'Input/Output']\n","\n","  # Remove first() from code in non aggregation operations.\n","  if category not in ignore_replace_categories:\n","    if '.first()' in code:\n","      code = code.strip().replace('.first()', '')\n","\n","    # Add alias(col) to the code where it was not present. This is done only for column level functions/code.\n","    # Also enhance the comments to include details about the alias colummn\n","    suffix_replace = ''\n","    for item in ['.first()', '.collect()', '.createOrReplace()', '.show(truncate=False)']:\n","      if code.strip().endswith(item):\n","        code = code.strip().replace(item, '')     if 'ALIAS_' not in code else code\n","        suffix_replace = item\n","\n","    if code.strip().endswith(')'):\n","      comment = comment + f\"The result is applied for all rows in MY_DF and stored in the column MY_ALIAS_A in the target dataframe.\"     if 'ALIAS_' not in comment else comment\n","      code = code.strip()[:-1] + \".alias(MY_ALIAS_A))\" + suffix_replace                                                                   if 'ALIAS_' not in code else code\n","      alias_str = 'MY_ALIAS_A'\n","\n","  # Enhance the comments to include details about the alias colummn\n","  alias_match = re.findall(r'alias\\((MY_ALIAS_[A-Z])+\\)', code)\n","\n","  ignore_functions = ['get_json_object', 'reduce']\n","  if replaced_flag == 'N':\n","    if len(alias_match) > 0 and fn_main not in ignore_functions:\n","      alias_str = alias_match[0]\n","      comment = comment + f\"The result is stored in the column {alias_match[0]} in the target dataframe.\"       if 'ALIAS_' not in comment else comment\n","\n","    elif 'MY_ALIAS_A, MY_ALIAS_B' in code:\n","      alias_str = 'MY_ALIAS_A, MY_ALIAS_B'\n","      comment = comment + f\"The result is stored in the columns MY_ALIAS_A and MY_ALIAS_B in the target dataframe.\"      if 'ALIAS_' not in comment else comment\n","\n","  result = [comment, code, alias_str]\n","  return result\n","\n","\n","\n","df_handle_aliases = df_init.copy()\n","\n","# Derive code_snippet and code_description\n","df_handle_aliases['code_description'] = df_handle_aliases.apply(lambda x: fn_handle_alias_comments(x['code_description'], x['code_snippet'], x['function'], x['Category'])[0] if x['origin_str'] != 'original' else x['code_description'], axis=1)\n","df_handle_aliases['code_snippet']     = df_handle_aliases.apply(lambda x: fn_handle_alias_comments(x['code_description'], x['code_snippet'], x['function'], x['Category'])[1] if x['origin_str'] != 'original' else x['code_snippet'], axis=1)\n","df_handle_aliases['alias_str']         = df_handle_aliases.apply(lambda x: fn_handle_alias_comments(x['code_description'], x['code_snippet'], x['function'], x['Category'])[2] if x['origin_str'] != 'original' else '', axis=1)\n","\n","\n","# UNCOMMENT FILTERS AS DESIRED TO DEBUG\n","df_handle_aliases[\n","    #(df_handle_aliases['code_snippet'].str.contains('alias\\(')) &\n","    (~df_handle_aliases['origin_str'].str.contains('original'))\n","    #(~df_handle_aliases['alias_str'].str.contains('MY_ALIAS_')) & (df_handle_aliases['code_snippet'] != df_handle_aliases['code_snippet1'] )\n","    #(~df_handle_aliases['alias_str'].str.contains('MY_ALIAS_')) & (df_handle_aliases['code_description'] != df_handle_aliases['code_description'] )\n","    & (df_handle_aliases['alias_str'].str.contains('MY_ALIAS_'))\n","    #(~df_handle_aliases['alias_str'].str.contains('MY_ALIAS_'))\n","#].count()  # ~ 406 aliases replaced\n","][['code_description', 'alias_str', 'code_snippet', 'function', 'Category']].style.set_properties(**{'text-align': 'left'})\n","#][['origin_str', 'code_description', 'code_snippet', 'code_description1', 'code_snippet1', 'Category']].style.set_properties(**{'text-align': 'left'})\n","\n","df_handle_aliases.head().style.set_properties(**{'text-align': 'left'})"]},{"cell_type":"markdown","metadata":{"id":"46A8gCKAdSYw"},"source":["<br>\n","\n","# Standardize: Remove display functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"tlXSi0euro8c","executionInfo":{"status":"ok","timestamp":1721677992367,"user_tz":-330,"elapsed":19,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"8bb7c210-2937-4f0e-eb0d-410f24aa6994"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7eb95c0cf370>"],"text/html":["<style type=\"text/css\">\n","#T_1145b_row0_col0, #T_1145b_row0_col1, #T_1145b_row0_col2, #T_1145b_row0_col3, #T_1145b_row0_col4, #T_1145b_row0_col5, #T_1145b_row1_col0, #T_1145b_row1_col1, #T_1145b_row1_col2, #T_1145b_row1_col3, #T_1145b_row1_col4, #T_1145b_row1_col5, #T_1145b_row2_col0, #T_1145b_row2_col1, #T_1145b_row2_col2, #T_1145b_row2_col3, #T_1145b_row2_col4, #T_1145b_row2_col5, #T_1145b_row3_col0, #T_1145b_row3_col1, #T_1145b_row3_col2, #T_1145b_row3_col3, #T_1145b_row3_col4, #T_1145b_row3_col5, #T_1145b_row4_col0, #T_1145b_row4_col1, #T_1145b_row4_col2, #T_1145b_row4_col3, #T_1145b_row4_col4, #T_1145b_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_1145b\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_1145b_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_1145b_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_1145b_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_1145b_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_1145b_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_1145b_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_1145b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_1145b_row0_col0\" class=\"data row0 col0\" >Pyspark code to Write a DataFrame into a CSV file.</td>\n","      <td id=\"T_1145b_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_1145b_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_1145b_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_1145b_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_1145b_row0_col5\" class=\"data row0 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1145b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_1145b_row1_col0\" class=\"data row1 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_1145b_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_1145b_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_1145b_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_1145b_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_1145b_row1_col5\" class=\"data row1 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1145b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_1145b_row2_col0\" class=\"data row2 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.</td>\n","      <td id=\"T_1145b_row2_col1\" class=\"data row2 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_1145b_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_1145b_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_1145b_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_1145b_row2_col5\" class=\"data row2 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1145b_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_1145b_row3_col0\" class=\"data row3 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_1145b_row3_col1\" class=\"data row3 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_1145b_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_1145b_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_1145b_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_1145b_row3_col5\" class=\"data row3 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_1145b_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_1145b_row4_col0\" class=\"data row4 col0\" >Pyspark code to Write a DataFrame into a JSON file.</td>\n","      <td id=\"T_1145b_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_1145b_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_1145b_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_1145b_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_1145b_row4_col5\" class=\"data row4 col5\" >original</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":15}],"source":["# REMOVE DISPLAY FUNCTIONS FROM CODE\n","\n","# Function to remove display functions like show(), head(),  printSchema()\n","def fn_remove_display_functions(comment, code, function, category):\n","  fn_main = function.split('.')[-1].strip()\n","\n","  if not comment.strip().endswith('.'):\n","      comment = comment + '.'\n","\n","  display_functions = ['.show()', '.show(truncate=False)', '.head()',  '.printSchema()']\n","\n","  # Remove the display functions\n","  for item in display_functions:\n","    code = code.strip().replace(item, '')\n","\n","  result = code\n","  return result\n","\n","\n","df_handle_display_fn = df_handle_aliases[['code_description', 'code_snippet', 'import_line', 'Category', 'function', 'origin_str']].copy()\n","\n","# Call function to remove display functions from code\n","df_handle_display_fn['code_snippet']     = df_handle_aliases.apply(lambda x: fn_remove_display_functions(x['code_description'], x['code_snippet'], x['function'], x['Category']), axis=1)\n","\n","# UNCOMMENT AS DESIRED TO DEBUG\n","df_handle_display_fn[\n","    (df_handle_display_fn['code_snippet'].str.contains('show\\('))\n","    #(df_handle_display_fn['code_snippet'].str.contains('head\\('))\n","    #(df_handle_display_fn['code_snippet'].str.contains('printSchema\\('))\n","    #(df_handle_display_fn['code_snippet'] == df_handle_display_fn['code_snippet1'] )\n","#].count()  # 54 shows() | 7 head | 16 printSchema\n","].style.set_properties(**{'text-align': 'left'})\n","\n","df_handle_display_fn.head().style.set_properties(**{'text-align': 'left'})\n"]},{"cell_type":"markdown","metadata":{"id":"lAeAV8gldbEB"},"source":["<br>\n","\n","# Standardize: Sort Function comments"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"id":"SaiA1yibMOBN","executionInfo":{"status":"ok","timestamp":1721677992367,"user_tz":-330,"elapsed":17,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"9ab64d12-7cfd-4a07-cee6-08b761a60b59"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7eb95bf48520>"],"text/html":["<style type=\"text/css\">\n","#T_cc745_row0_col0, #T_cc745_row0_col1, #T_cc745_row0_col2, #T_cc745_row0_col3, #T_cc745_row0_col4, #T_cc745_row0_col5, #T_cc745_row1_col0, #T_cc745_row1_col1, #T_cc745_row1_col2, #T_cc745_row1_col3, #T_cc745_row1_col4, #T_cc745_row1_col5, #T_cc745_row2_col0, #T_cc745_row2_col1, #T_cc745_row2_col2, #T_cc745_row2_col3, #T_cc745_row2_col4, #T_cc745_row2_col5, #T_cc745_row3_col0, #T_cc745_row3_col1, #T_cc745_row3_col2, #T_cc745_row3_col3, #T_cc745_row3_col4, #T_cc745_row3_col5, #T_cc745_row4_col0, #T_cc745_row4_col1, #T_cc745_row4_col2, #T_cc745_row4_col3, #T_cc745_row4_col4, #T_cc745_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_cc745\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_cc745_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_cc745_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_cc745_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_cc745_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_cc745_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_cc745_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_cc745_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_cc745_row0_col0\" class=\"data row0 col0\" >Pyspark code to Write a DataFrame into a CSV file.</td>\n","      <td id=\"T_cc745_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_cc745_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_cc745_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_cc745_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_cc745_row0_col5\" class=\"data row0 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cc745_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_cc745_row1_col0\" class=\"data row1 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_cc745_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_cc745_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_cc745_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_cc745_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_cc745_row1_col5\" class=\"data row1 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cc745_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_cc745_row2_col0\" class=\"data row2 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.</td>\n","      <td id=\"T_cc745_row2_col1\" class=\"data row2 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_cc745_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_cc745_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_cc745_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_cc745_row2_col5\" class=\"data row2 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cc745_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_cc745_row3_col0\" class=\"data row3 col0\" >Pyspark code which Loads a CSV file and returns the result as a DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_cc745_row3_col1\" class=\"data row3 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_cc745_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_cc745_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_cc745_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_cc745_row3_col5\" class=\"data row3 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cc745_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_cc745_row4_col0\" class=\"data row4 col0\" >Pyspark code to Write a DataFrame into a JSON file.</td>\n","      <td id=\"T_cc745_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_cc745_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_cc745_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_cc745_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_cc745_row4_col5\" class=\"data row4 col5\" >original</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":16}],"source":["# HANDLE COMMENTS FOR SORT FUNCTIONS IN CODE\n","\n","# Function to remove display functions like show(), head(),  printSchema()\n","def fn_handle_sort(comment, code, function, category):\n","  fn_main = function.split('.')[-1].strip()\n","  result = ''\n","\n","  if not comment.strip().endswith('.'):\n","      comment = comment + '.'\n","\n","  sort_match = re.findall(r\"\\.sort\\((.*?)\\)\", code)\n","  if len(sort_match) > 0:\n","    if '(' not in sort_match[0]:\n","      comment = comment + f\"The data is sorted on the column {sort_match[0]} in ascending order.\"\n","\n","  result = comment\n","  return result\n","\n","\n","df_handle_sort = df_handle_display_fn.copy()\n","\n","# Call function to remove display functions from code\n","df_handle_sort['code_description']     = df_handle_sort.apply(lambda x: fn_handle_sort(x['code_description'], x['code_snippet'], x['function'], x['Category']), axis=1)\n","\n","# UNCOMMENT AS DESIRED TO DEBUG\n","df_handle_sort[\n","    (df_handle_sort['code_snippet'].str.contains('\\.sort\\('))\n","    #(df_handle_sort['code_description'] != df_handle_sort['code_description1'])\n","].count()  # 29 sort() replaced\n","#].style.set_properties(**{'text-align': 'left'})\n","\n","df_handle_sort.head().style.set_properties(**{'text-align': 'left'})"]},{"cell_type":"markdown","metadata":{"id":"d0j2pqE50Hwt"},"source":["<br>\n","\n","# Standardize: Comments with MY_DIR references"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":312},"id":"b8tfxUsk0HDx","executionInfo":{"status":"ok","timestamp":1721677993036,"user_tz":-330,"elapsed":684,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"48f5e0e1-34d6-4e90-b915-2b80fc52d28d"},"outputs":[{"output_type":"stream","name":"stdout","text":[" --> Display Count Before dropping duplicates - 1195\n"," --> Display Count After dropping duplicates - 1177\n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7eb95bed7460>"],"text/html":["<style type=\"text/css\">\n","#T_dc528_row0_col0, #T_dc528_row0_col1, #T_dc528_row0_col2, #T_dc528_row0_col3, #T_dc528_row0_col4, #T_dc528_row0_col5, #T_dc528_row0_col6, #T_dc528_row1_col0, #T_dc528_row1_col1, #T_dc528_row1_col2, #T_dc528_row1_col3, #T_dc528_row1_col4, #T_dc528_row1_col5, #T_dc528_row1_col6, #T_dc528_row2_col0, #T_dc528_row2_col1, #T_dc528_row2_col2, #T_dc528_row2_col3, #T_dc528_row2_col4, #T_dc528_row2_col5, #T_dc528_row2_col6, #T_dc528_row3_col0, #T_dc528_row3_col1, #T_dc528_row3_col2, #T_dc528_row3_col3, #T_dc528_row3_col4, #T_dc528_row3_col5, #T_dc528_row3_col6, #T_dc528_row4_col0, #T_dc528_row4_col1, #T_dc528_row4_col2, #T_dc528_row4_col3, #T_dc528_row4_col4, #T_dc528_row4_col5, #T_dc528_row4_col6 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_dc528\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_dc528_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_dc528_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_dc528_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_dc528_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_dc528_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_dc528_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","      <th id=\"T_dc528_level0_col6\" class=\"col_heading level0 col6\" >delete</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_dc528_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_dc528_row0_col0\" class=\"data row0 col0\" >Pyspark code to overwrite a DataFrame into a CSV file.The data is overwritten to file or directory MY_DIR .</td>\n","      <td id=\"T_dc528_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_dc528_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_dc528_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_dc528_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_dc528_row0_col5\" class=\"data row0 col5\" >original</td>\n","      <td id=\"T_dc528_row0_col6\" class=\"data row0 col6\" >N</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_dc528_level0_row1\" class=\"row_heading level0 row1\" >2</th>\n","      <td id=\"T_dc528_row1_col0\" class=\"data row1 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_dc528_row1_col1\" class=\"data row1 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_dc528_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_dc528_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_dc528_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_dc528_row1_col5\" class=\"data row1 col5\" >original</td>\n","      <td id=\"T_dc528_row1_col6\" class=\"data row1 col6\" >N</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_dc528_level0_row2\" class=\"row_heading level0 row2\" >4</th>\n","      <td id=\"T_dc528_row2_col0\" class=\"data row2 col0\" >Pyspark code to overwrite a DataFrame into a JSON file.The data is overwritten to file or directory MY_DIR .</td>\n","      <td id=\"T_dc528_row2_col1\" class=\"data row2 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_dc528_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_dc528_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_dc528_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_dc528_row2_col5\" class=\"data row2 col5\" >original</td>\n","      <td id=\"T_dc528_row2_col6\" class=\"data row2 col6\" >N</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_dc528_level0_row3\" class=\"row_heading level0 row3\" >5</th>\n","      <td id=\"T_dc528_row3_col0\" class=\"data row3 col0\" >Pyspark code which Specifies the input data source format.The data is overwritten to file or directory MY_DIR .</td>\n","      <td id=\"T_dc528_row3_col1\" class=\"data row3 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_dc528_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_dc528_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_dc528_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_dc528_row3_col5\" class=\"data row3 col5\" >original</td>\n","      <td id=\"T_dc528_row3_col6\" class=\"data row3 col6\" >N</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_dc528_level0_row4\" class=\"row_heading level0 row4\" >6</th>\n","      <td id=\"T_dc528_row4_col0\" class=\"data row4 col0\" >Pyspark code to Read the JSON file as a DataFrame.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_dc528_row4_col1\" class=\"data row4 col1\" >df = spark.read.format('json').load(MY_DIR)</td>\n","      <td id=\"T_dc528_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_dc528_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_dc528_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_dc528_row4_col5\" class=\"data row4 col5\" >original</td>\n","      <td id=\"T_dc528_row4_col6\" class=\"data row4 col6\" >N</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":17}],"source":["# HANDLE COMMENTS FOR CODE WITH MY_DIR REFERENCES\n","\n","# Function to remove display functions like show(), head(),  printSchema()\n","def fn_handle_my_dir(comment, code, function, category):\n","  fn_main = function.split('.')[-1].strip()\n","  result = ''\n","  delete = 'N'\n","\n","  if not comment.strip().endswith('.'):\n","      comment = comment + '.'\n","\n","  # On Manul validation, these codes are duplicates but have incorrect code descriptions\n","  if len(comment.strip().split('.')) > 2 and 'MY_DIR' in code and 'LIT_' not in comment:\n","    delete = '-Y-'\n","\n","  if 'MY_DIR' in code:\n","    suffix = ''\n","    if '.option(\"header\", True' in code:\n","      suffix = 'with a header record'\n","    if 'directory MY_DIR' not in comment and 'write' in code:\n","      comment = comment + f'The data is written to file or directory MY_DIR {suffix}.'\n","      comment = comment.replace('Write', 'overwrite').replace('written', 'overwritten')       if 'overwrite' in code else comment\n","    if 'directory MY_DIR' not in comment and 'read' in code:\n","      comment = comment + f'The data is read from file or directory MY_DIR.'\n","  if 'MY_DIR' in comment:\n","    if 'overwrite' in code and 'overwrit' not in comment:\n","      comment = comment.replace(' written', ' overwritten')\n","\n","  code = code.replace('AnalysisException:', '')\n","  result = [comment, code, delete]\n","  return result\n","\n","\n","df_handle_my_dir = df_handle_sort.copy()\n","\n","# Call function to remove display functions from code\n","df_handle_my_dir['delete']              = df_handle_my_dir.apply(lambda x: fn_handle_my_dir(x['code_description'], x['code_snippet'], x['function'], x['Category'])[2], axis=1)\n","df_handle_my_dir['code_description']   = df_handle_my_dir.apply(lambda x: fn_handle_my_dir(x['code_description'], x['code_snippet'], x['function'], x['Category'])[0], axis=1)\n","df_handle_my_dir['code_snippet']        = df_handle_my_dir.apply(lambda x: fn_handle_my_dir(x['code_description'], x['code_snippet'], x['function'], x['Category'])[1], axis=1)\n","\n","\n","# Drop unwanted records -- these codes are duplicates but have incorrect code descriptions\n","fn_display_message(f' --> Display Count Before dropping duplicates - {df_handle_my_dir.shape[0]}')\n","df_handle_my_dir = df_handle_my_dir[df_handle_my_dir['delete'] != '-Y-']\n","fn_display_message(f' --> Display Count After dropping duplicates - {df_handle_my_dir.shape[0]}')\n","\n","# UNCOMMENT AS DESIRED TO DEBUG\n","df_handle_my_dir[\n","    (df_handle_my_dir['code_snippet'].str.contains('MY_DIR'))\n","    #& (df_handle_my_dir['delete'] == 'Y' )\n","    #(df_handle_my_dir['code_description'] != df_handle_my_dir['code_description1'])\n","#].count()  # 77 MY_DIR\n","#][['code_description', 'code_snippet1', 'code_description1', 'code_snippet', 'delete']].style.set_properties(**{'text-align': 'left'})\n","].style.set_properties(**{'text-align': 'left'})\n","\n","df_handle_my_dir.head().style.set_properties(**{'text-align': 'left'})\n"]},{"cell_type":"markdown","metadata":{"id":"--Kk6aPAfyW-"},"source":["<br>\n","\n","# CREATE FINAL STANDARDIZED DATASET"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sqTdgnRfd3FH","executionInfo":{"status":"ok","timestamp":1721677993037,"user_tz":-330,"elapsed":34,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"94c53f15-f43d-4302-9e32-03184cb0001f"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           Display df_final_standardized COLUMN/COUNT Details\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","Index: 1177 entries, 0 to 1214\n","Data columns (total 6 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   code_description  1177 non-null   object\n"," 1   code_snippet      1177 non-null   object\n"," 2   import_line       110 non-null    object\n"," 3   Category          1177 non-null   object\n"," 4   function          1177 non-null   object\n"," 5   origin_str        1177 non-null   object\n","dtypes: object(6)\n","memory usage: 64.4+ KB\n"]}],"source":["# CREATE FINAL STANDARDIZED DATASET\n","\n","df_final_standardized = df_handle_my_dir[['code_description', 'code_snippet', 'import_line', 'Category', 'function', 'origin_str']].copy()\n","\n","fn_display_header('Display df_final_standardized COLUMN/COUNT Details')\n","df_final_standardized.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FTJery6Bp9I1"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"vuDDjmGeMhfS"},"source":["<br>\n","<br>\n","<br>\n","\n","----\n","# <b>AUGMENTATION SECTION: BASIC </b>\n","----\n"]},{"cell_type":"markdown","metadata":{"id":"vtnz8b-Srhuv"},"source":["<br>\n","\n","# Augmentation: display functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":557},"id":"YeOSxOaXrpAa","executionInfo":{"status":"ok","timestamp":1721677993037,"user_tz":-330,"elapsed":31,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"38f08368-e379-43ba-d292-b7cda9f42d2f"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           Display Augmented Dataframe information: df_augment_display_fn\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 43 entries, 0 to 42\n","Data columns (total 6 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   code_description  43 non-null     object\n"," 1   code_snippet      43 non-null     object\n"," 2   import_line       43 non-null     object\n"," 3   Category          43 non-null     object\n"," 4   function          43 non-null     object\n"," 5   origin_str        43 non-null     object\n","dtypes: object(6)\n","memory usage: 2.1+ KB\n","--------------------------------------------------------------------------------\n","           Display Augmented Dataframe rows: df_augment_display_fn\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7eb95bf48fd0>"],"text/html":["<style type=\"text/css\">\n","#T_ee50f_row0_col0, #T_ee50f_row0_col1, #T_ee50f_row0_col2, #T_ee50f_row0_col3, #T_ee50f_row0_col4, #T_ee50f_row0_col5, #T_ee50f_row1_col0, #T_ee50f_row1_col1, #T_ee50f_row1_col2, #T_ee50f_row1_col3, #T_ee50f_row1_col4, #T_ee50f_row1_col5, #T_ee50f_row2_col0, #T_ee50f_row2_col1, #T_ee50f_row2_col2, #T_ee50f_row2_col3, #T_ee50f_row2_col4, #T_ee50f_row2_col5, #T_ee50f_row3_col0, #T_ee50f_row3_col1, #T_ee50f_row3_col2, #T_ee50f_row3_col3, #T_ee50f_row3_col4, #T_ee50f_row3_col5, #T_ee50f_row4_col0, #T_ee50f_row4_col1, #T_ee50f_row4_col2, #T_ee50f_row4_col3, #T_ee50f_row4_col4, #T_ee50f_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_ee50f\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_ee50f_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_ee50f_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_ee50f_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_ee50f_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_ee50f_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_ee50f_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_ee50f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_ee50f_row0_col0\" class=\"data row0 col0\" >Pyspark code which displays the top 20 rows in the dataframe MY_DF as a list of row objects. This is the default option.</td>\n","      <td id=\"T_ee50f_row0_col1\" class=\"data row0 col1\" >MY_DF.show()</td>\n","      <td id=\"T_ee50f_row0_col2\" class=\"data row0 col2\" ></td>\n","      <td id=\"T_ee50f_row0_col3\" class=\"data row0 col3\" >Display Function</td>\n","      <td id=\"T_ee50f_row0_col4\" class=\"data row0 col4\" >show</td>\n","      <td id=\"T_ee50f_row0_col5\" class=\"data row0 col5\" >Augmented Display</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ee50f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_ee50f_row1_col0\" class=\"data row1 col0\" >Pyspark code which displays the top 5 rows in the dataframe MY_DF.</td>\n","      <td id=\"T_ee50f_row1_col1\" class=\"data row1 col1\" >MY_DF.show(23)</td>\n","      <td id=\"T_ee50f_row1_col2\" class=\"data row1 col2\" ></td>\n","      <td id=\"T_ee50f_row1_col3\" class=\"data row1 col3\" >Display Function</td>\n","      <td id=\"T_ee50f_row1_col4\" class=\"data row1 col4\" >show</td>\n","      <td id=\"T_ee50f_row1_col5\" class=\"data row1 col5\" >Augmented Display</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ee50f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_ee50f_row2_col0\" class=\"data row2 col0\" >Pyspark code which displays the top 10 rows in the dataframe MY_DF.</td>\n","      <td id=\"T_ee50f_row2_col1\" class=\"data row2 col1\" >MY_DF.show(39)</td>\n","      <td id=\"T_ee50f_row2_col2\" class=\"data row2 col2\" ></td>\n","      <td id=\"T_ee50f_row2_col3\" class=\"data row2 col3\" >Display Function</td>\n","      <td id=\"T_ee50f_row2_col4\" class=\"data row2 col4\" >show</td>\n","      <td id=\"T_ee50f_row2_col5\" class=\"data row2 col5\" >Augmented Display</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ee50f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_ee50f_row3_col0\" class=\"data row3 col0\" >Pyspark code which displays the top 15 rows in the dataframe MY_DF.</td>\n","      <td id=\"T_ee50f_row3_col1\" class=\"data row3 col1\" >MY_DF.show(55)</td>\n","      <td id=\"T_ee50f_row3_col2\" class=\"data row3 col2\" ></td>\n","      <td id=\"T_ee50f_row3_col3\" class=\"data row3 col3\" >Display Function</td>\n","      <td id=\"T_ee50f_row3_col4\" class=\"data row3 col4\" >show</td>\n","      <td id=\"T_ee50f_row3_col5\" class=\"data row3 col5\" >Augmented Display</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_ee50f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_ee50f_row4_col0\" class=\"data row4 col0\" >Pyspark code which displays the top 50 rows in the dataframe MY_DF.</td>\n","      <td id=\"T_ee50f_row4_col1\" class=\"data row4 col1\" >MY_DF.show(101)</td>\n","      <td id=\"T_ee50f_row4_col2\" class=\"data row4 col2\" ></td>\n","      <td id=\"T_ee50f_row4_col3\" class=\"data row4 col3\" >Display Function</td>\n","      <td id=\"T_ee50f_row4_col4\" class=\"data row4 col4\" >show</td>\n","      <td id=\"T_ee50f_row4_col5\" class=\"data row4 col5\" >Augmented Display</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":19}],"source":["# AUGMENT/HANDLE DISPLAY COMMENTS\n","\n","display_functions = ['.show()', '.show(truncate=False)', '.head()',  '.printSchema()']\n","\n","# Create a blank dataframe to start with\n","df_augment_display_fn = pd.DataFrame(columns=df_final_standardized.columns)\n","\n","# Defaults\n","df_name = 'MY_DF'\n","import_line = ''\n","category = 'Display Function'\n","origin_str = 'Augmented Display'\n","disp_val = ''\n","\n","display_fn_array = []\n","\n","display_ctrl_dict = {\n","    'show'        : f\"Pyspark code which displays the top 20 rows in the dataframe {df_name} as a list of row objects. This is the default option.\",\n","    'head'        : f\"Pyspark code which displays the top 20 rows in the dataframe {df_name} in tabular format. This is the default option.\",\n","    'printSchema' : f\"Pyspark code which displays the schema or structure of the dataframe {df_name}. It includes culumn names, data types and whether column is nullable.\"\n","}\n","\n","for disp_fn, disp_comment in display_ctrl_dict.items():\n","\n","  # Augment show() and head()\n","  if disp_fn in ['show', 'head']:\n","    display_fn_array.append({\n","        'code_description': disp_comment,\n","        'code_snippet'    : f\"{df_name}.{disp_fn}()\",\n","        'import_line'     : import_line,\n","        'Category'        : category,\n","        'function'        : disp_fn,\n","        'origin_str'      : origin_str,\n","      })\n","\n","    add = 7\n","    for disp_val in [5, 10, 15, 50, 75, 100, 125, 150, 175, 200, 223, 258, 273, 299, 350, 371, 446, 500, 750, 1000]:\n","      add = add + 11\n","      display_fn_array.append({\n","        'code_description': f\"Pyspark code which displays the top {disp_val} rows in the dataframe {df_name}.\",\n","        'code_snippet'    : f\"{df_name}.{disp_fn}({disp_val + add if disp_fn == 'show' else disp_val})\",\n","        'import_line'     : import_line,\n","        'Category'        : category,\n","        'function'        : disp_fn,\n","        'origin_str'      : origin_str,\n","      })\n","\n","  # Augment printSchema()\n","  if disp_fn == 'printSchema':\n","    display_fn_array.append({\n","        'code_description': disp_comment,\n","        'code_snippet'    : f\"{df_name}.{disp_fn}()\",\n","        'import_line'     : import_line,\n","        'Category'        : category,\n","        'function'        : disp_fn,\n","        'origin_str'      : origin_str,\n","      })\n","\n","# Convert the list of dictionaries to a DataFrame\n","df_augment_display_fn = pd.DataFrame(display_fn_array)\n","\n","fn_display_header('Display Augmented Dataframe information: df_augment_display_fn')\n","df_augment_display_fn.info()\n","\n","fn_display_header('Display Augmented Dataframe rows: df_augment_display_fn')\n","df_augment_display_fn.head().style.set_properties(**{'text-align': 'left'})\n"]},{"cell_type":"markdown","metadata":{"id":"219_fgYrdqRv"},"source":["<br>\n","\n","# Augmentation: sort functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":749},"id":"a5yZJjyAYLfG","executionInfo":{"status":"ok","timestamp":1721677993037,"user_tz":-330,"elapsed":27,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"cc8d00cb-d701-4fa4-c7ae-d79a1b90a1ee"},"outputs":[{"output_type":"stream","name":"stdout","text":[" --> Display Count of iterations - 29\n","--------------------------------------------------------------------------------\n","           Display Augmented Dataframe information: df_augment_sort_fn\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 58 entries, 0 to 57\n","Data columns (total 6 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   code_description  58 non-null     object\n"," 1   code_snippet      58 non-null     object\n"," 2   import_line       0 non-null      object\n"," 3   Category          58 non-null     object\n"," 4   function          58 non-null     object\n"," 5   origin_str        58 non-null     object\n","dtypes: object(6)\n","memory usage: 2.8+ KB\n","--------------------------------------------------------------------------------\n","           Display Augmented Dataframe rows: df_augment_sort_fn\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7eb95c100970>"],"text/html":["<style type=\"text/css\">\n","#T_80a21_row0_col0, #T_80a21_row0_col1, #T_80a21_row0_col2, #T_80a21_row0_col3, #T_80a21_row0_col4, #T_80a21_row0_col5, #T_80a21_row1_col0, #T_80a21_row1_col1, #T_80a21_row1_col2, #T_80a21_row1_col3, #T_80a21_row1_col4, #T_80a21_row1_col5, #T_80a21_row2_col0, #T_80a21_row2_col1, #T_80a21_row2_col2, #T_80a21_row2_col3, #T_80a21_row2_col4, #T_80a21_row2_col5, #T_80a21_row3_col0, #T_80a21_row3_col1, #T_80a21_row3_col2, #T_80a21_row3_col3, #T_80a21_row3_col4, #T_80a21_row3_col5, #T_80a21_row4_col0, #T_80a21_row4_col1, #T_80a21_row4_col2, #T_80a21_row4_col3, #T_80a21_row4_col4, #T_80a21_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_80a21\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_80a21_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_80a21_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_80a21_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_80a21_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_80a21_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_80a21_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_80a21_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_80a21_row0_col0\" class=\"data row0 col0\" >Pyspark code which Compute aggregates and returns the result as a DataFrame. The available aggregate functions can be built in aggregation functions such as avg max min sum count group aggregate pandas UDFs created with pyspark.The data is sorted on the column \"name\" in ascending order.</td>\n","      <td id=\"T_80a21_row0_col1\" class=\"data row0 col1\" >df.groupBy(df.name).agg({\"*\": \"count\"}).sort(\"name\", ascending=True)</td>\n","      <td id=\"T_80a21_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_80a21_row0_col3\" class=\"data row0 col3\" >Grouping</td>\n","      <td id=\"T_80a21_row0_col4\" class=\"data row0 col4\" >pyspark.sql.GroupedData.agg</td>\n","      <td id=\"T_80a21_row0_col5\" class=\"data row0 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_80a21_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_80a21_row1_col0\" class=\"data row1 col0\" >Pyspark code which Compute aggregates and returns the result as a DataFrame. The available aggregate functions can be built in aggregation functions such as avg max min sum count group aggregate pandas UDFs created with pyspark.The data is sorted on the column \"name\" in descending order.</td>\n","      <td id=\"T_80a21_row1_col1\" class=\"data row1 col1\" >df.groupBy(df.name).agg({\"*\": \"count\"}).sort(\"name\", ascending=False)</td>\n","      <td id=\"T_80a21_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_80a21_row1_col3\" class=\"data row1 col3\" >Grouping</td>\n","      <td id=\"T_80a21_row1_col4\" class=\"data row1 col4\" >pyspark.sql.GroupedData.agg</td>\n","      <td id=\"T_80a21_row1_col5\" class=\"data row1 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_80a21_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_80a21_row2_col0\" class=\"data row2 col0\" >Pyspark code which Compute aggregates and returns the result as a DataFrame. The available aggregate functions can be built in aggregation functions such as avg max min sum count group aggregate pandas UDFs created with pyspark.The data is sorted on the column \"name\" in ascending order.</td>\n","      <td id=\"T_80a21_row2_col1\" class=\"data row2 col1\" >df.groupBy(df.name).agg(sf.min(df.age)).sort(\"name\", ascending=True)</td>\n","      <td id=\"T_80a21_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_80a21_row2_col3\" class=\"data row2 col3\" >Grouping</td>\n","      <td id=\"T_80a21_row2_col4\" class=\"data row2 col4\" >pyspark.sql.GroupedData.agg</td>\n","      <td id=\"T_80a21_row2_col5\" class=\"data row2 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_80a21_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_80a21_row3_col0\" class=\"data row3 col0\" >Pyspark code which Compute aggregates and returns the result as a DataFrame. The available aggregate functions can be built in aggregation functions such as avg max min sum count group aggregate pandas UDFs created with pyspark.The data is sorted on the column \"name\" in descending order.</td>\n","      <td id=\"T_80a21_row3_col1\" class=\"data row3 col1\" >df.groupBy(df.name).agg(sf.min(df.age)).sort(\"name\", ascending=False)</td>\n","      <td id=\"T_80a21_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_80a21_row3_col3\" class=\"data row3 col3\" >Grouping</td>\n","      <td id=\"T_80a21_row3_col4\" class=\"data row3 col4\" >pyspark.sql.GroupedData.agg</td>\n","      <td id=\"T_80a21_row3_col5\" class=\"data row3 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_80a21_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_80a21_row4_col0\" class=\"data row4 col0\" >Pyspark code which Compute aggregates and returns the result as a DataFrame. The available aggregate functions can be built in aggregation functions such as avg max min sum count group aggregate pandas UDFs created with pyspark.The data is sorted on the column \"name\" in ascending order.</td>\n","      <td id=\"T_80a21_row4_col1\" class=\"data row4 col1\" >df.groupBy(df.name).agg(min_udf(df.age)).sort(\"name\", ascending=True)</td>\n","      <td id=\"T_80a21_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_80a21_row4_col3\" class=\"data row4 col3\" >Grouping</td>\n","      <td id=\"T_80a21_row4_col4\" class=\"data row4 col4\" >pyspark.sql.GroupedData.agg</td>\n","      <td id=\"T_80a21_row4_col5\" class=\"data row4 col5\" >original</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":20}],"source":["# AUGMENT CODES USING SORT\n","\n","df_augment_sort_fn = pd.DataFrame(columns=df_final_standardized.columns)\n","\n","df_iter_sort = df_final_standardized[(df_final_standardized['code_snippet'].str.contains('\\.sort\\(')) & (df_final_standardized['code_description'].str.contains('in ascending order\\.'))]\n","\n","fn_display_message(f' --> Display Count of iterations - {df_iter_sort.shape[0]}')\n","\n","new_rows = [] # Create a list to store new rows as dictionaries\n","for index, row in df_iter_sort.iterrows():\n","  for item in [', ascending=True)', ', ascending=False)']:\n","    code = row['code_snippet'][:-1] + item\n","    comment = row['code_description'].replace('ascending', 'descending')       if 'False' in item else row['code_description']\n","\n","    # Append a dictionary representing the new row to the list\n","    new_rows.append({\n","      'code_description': comment,\n","      'code_snippet': code,\n","      'import_line': row['import_line'],\n","      'Category': row['Category'],\n","      'function': row['function'],\n","      'origin_str': row['origin_str']\n","    })\n","\n","# Concatenate the new rows to the DataFrame\n","df_augment_sort_fn = pd.concat([df_augment_sort_fn, pd.DataFrame(new_rows)], ignore_index=True)\n","\n","fn_display_header('Display Augmented Dataframe information: df_augment_sort_fn')\n","df_augment_sort_fn.info()\n","\n","fn_display_header('Display Augmented Dataframe rows: df_augment_sort_fn')\n","df_augment_sort_fn.head().style.set_properties(**{'text-align': 'left'})"]},{"cell_type":"markdown","metadata":{"id":"1PafGpwLpmNr"},"source":["<br>\n","\n","# Augmentation: Miscellaneous functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":612},"id":"1dev-Td3cYUi","executionInfo":{"status":"ok","timestamp":1721677993038,"user_tz":-330,"elapsed":25,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"fd7a04c1-950b-4990-88d6-1656b7c8c492"},"outputs":[{"output_type":"stream","name":"stdout","text":[" --> Display Count of iterations - 37\n"," --> Display Count Before dropping duplicates - 234\n"," --> Display Count After dropping duplicates - 117\n","--------------------------------------------------------------------------------\n","           Display Augmented Dataframe information: df_augment_misc_fn\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","Index: 117 entries, 0 to 230\n","Data columns (total 6 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   code_description  117 non-null    object\n"," 1   code_snippet      117 non-null    object\n"," 2   import_line       0 non-null      object\n"," 3   Category          117 non-null    object\n"," 4   function          117 non-null    object\n"," 5   origin_str        117 non-null    object\n","dtypes: object(6)\n","memory usage: 6.4+ KB\n","--------------------------------------------------------------------------------\n","           Display Augmented Dataframe rows: df_augment_misc_fn\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7eb95bed7d90>"],"text/html":["<style type=\"text/css\">\n","#T_85511_row0_col0, #T_85511_row0_col1, #T_85511_row0_col2, #T_85511_row0_col3, #T_85511_row0_col4, #T_85511_row0_col5, #T_85511_row1_col0, #T_85511_row1_col1, #T_85511_row1_col2, #T_85511_row1_col3, #T_85511_row1_col4, #T_85511_row1_col5, #T_85511_row2_col0, #T_85511_row2_col1, #T_85511_row2_col2, #T_85511_row2_col3, #T_85511_row2_col4, #T_85511_row2_col5, #T_85511_row3_col0, #T_85511_row3_col1, #T_85511_row3_col2, #T_85511_row3_col3, #T_85511_row3_col4, #T_85511_row3_col5, #T_85511_row4_col0, #T_85511_row4_col1, #T_85511_row4_col2, #T_85511_row4_col3, #T_85511_row4_col4, #T_85511_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_85511\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_85511_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_85511_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_85511_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_85511_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_85511_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_85511_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_85511_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_85511_row0_col0\" class=\"data row0 col0\" >Pyspark code to overwrite a DataFrame into a CSV file.The data is appended to file or directory MY_DIR .</td>\n","      <td id=\"T_85511_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"append\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_85511_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_85511_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_85511_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_85511_row0_col5\" class=\"data row0 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_85511_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_85511_row1_col0\" class=\"data row1 col0\" >Pyspark code to overwrite a DataFrame into a CSV file.The write is ignored if the file MY_DIR already exists .</td>\n","      <td id=\"T_85511_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"ignore\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_85511_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_85511_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_85511_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_85511_row1_col5\" class=\"data row1 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_85511_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_85511_row2_col0\" class=\"data row2 col0\" >Pyspark code to overwrite a DataFrame into a CSV file.Throws an error if the file MY_DIR already exists .</td>\n","      <td id=\"T_85511_row2_col1\" class=\"data row2 col1\" >df.write.mode(\"error\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_85511_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_85511_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_85511_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_85511_row2_col5\" class=\"data row2 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_85511_level0_row3\" class=\"row_heading level0 row3\" >6</th>\n","      <td id=\"T_85511_row3_col0\" class=\"data row3 col0\" >Pyspark code to overwrite a DataFrame into a JSON file.The data is appended to file or directory MY_DIR .</td>\n","      <td id=\"T_85511_row3_col1\" class=\"data row3 col1\" >df.write.mode(\"append\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_85511_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_85511_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_85511_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_85511_row3_col5\" class=\"data row3 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_85511_level0_row4\" class=\"row_heading level0 row4\" >7</th>\n","      <td id=\"T_85511_row4_col0\" class=\"data row4 col0\" >Pyspark code to overwrite a DataFrame into a JSON file.The write is ignored if the file MY_DIR already exists .</td>\n","      <td id=\"T_85511_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"ignore\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_85511_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_85511_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_85511_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_85511_row4_col5\" class=\"data row4 col5\" >original</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":21}],"source":["# MISCELLANEOUS AUGMENTATION: FOR MY_DIR REFERENCES USING VRAIOUS APPLICABLE ARGUMENT OPTIONS\n","\n","df_augment_misc_fn = pd.DataFrame(columns=df_final_standardized.columns)\n","\n","df_iter_misc = df_final_standardized[\n","    (df_final_standardized['code_snippet'].str.contains('option\\(\"header\", True'))\n","    | ( (df_final_standardized['code_snippet'].str.contains('mode')) & (df_final_standardized['code_snippet'].str.contains('overwrite')) )\n","    ]\n","\n","fn_display_message(f' --> Display Count of iterations - {df_iter_misc.shape[0]}')\n","\n","new_rows = [] # Create a list to store new rows as dictionaries\n","for index, row in df_iter_misc.iterrows():\n","  for item in [', ascending=True)', ', ascending=False)']:\n","    result_list = []\n","\n","    if '.option(\"header\", True' in row['code_snippet']:\n","      # pass 1: replace options with blank to mimic default\n","      code = row['code_snippet'].replace('.option(\"header\", True)', '')\n","      comment = row['code_description'].replace(\"with a header record\", \"with a header record by default\").replace(\"with header columns\", \"with header columns by default\")\n","      result_list.append([comment, code])\n","\n","      # pass 2: replace True with False\n","      code = row['code_snippet'].replace('True', 'False')\n","      comment = row['code_description'].replace(\"with a header record\", \"without a header record\").replace(\"with header columns\", \"without header columns\")\n","      result_list.append([comment, code])\n","\n","    if 'mode' in row['code_snippet'] and 'overwrite' in row['code_snippet']:\n","      # pass 1: replace overwrite with append\n","      code = row['code_snippet'].replace('overwrite', 'append')\n","      comment = row['code_description'].replace(\"overwritten\", \"appended\")\n","      result_list.append([comment, code])\n","\n","      # pass 2: replace overwrite with ignore\n","      code = row['code_snippet'].replace('overwrite', 'ignore')\n","      comment = row['code_description'].replace(\"The data is overwritten to file or directory MY_DIR\", \"The write is ignored if the file MY_DIR already exists\")\n","      result_list.append([comment, code])\n","\n","      # pass 3: replace overwrite with error\n","      code = row['code_snippet'].replace('overwrite', 'error')\n","      comment = row['code_description'].replace(\"The data is overwritten to file or directory MY_DIR\", \"Throws an error if the file MY_DIR already exists\")\n","      result_list.append([comment, code])\n","\n","    # Build list of dictionaries representing new rows\n","    for item in result_list:\n","      new_rows.append({\n","      'code_description': item[0],\n","      'code_snippet': item[1],\n","      'import_line': row['import_line'],\n","      'Category': row['Category'],\n","      'function': row['function'],\n","      'origin_str': row['origin_str']\n","      })\n","\n","# Concatenate the new rows to the DataFrame using pd.concat()\n","df_augment_misc_fn = pd.concat([df_augment_misc_fn, pd.DataFrame(new_rows)], ignore_index=True)\n","\n","# Drop Duplicates\n","fn_display_message(f' --> Display Count Before dropping duplicates - {df_augment_misc_fn.shape[0]}')\n","df_augment_misc_fn = df_augment_misc_fn.drop_duplicates()\n","fn_display_message(f' --> Display Count After dropping duplicates - {df_augment_misc_fn.shape[0]}')\n","\n","fn_display_header('Display Augmented Dataframe information: df_augment_misc_fn')\n","df_augment_misc_fn.info()\n","\n","fn_display_header('Display Augmented Dataframe rows: df_augment_misc_fn')\n","df_augment_misc_fn.head().style.set_properties(**{'text-align': 'left'})"]},{"cell_type":"markdown","metadata":{"id":"IbwApGlxQ7Ni"},"source":["<br>\n","\n","# CONCATENATE AUGMENTED DATASETS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":611},"id":"lg_yWT8scYXr","executionInfo":{"status":"ok","timestamp":1721677993038,"user_tz":-330,"elapsed":22,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"e8e9a1c4-972f-4af8-dcb6-5fdcaff5ef74"},"outputs":[{"output_type":"stream","name":"stdout","text":[" --> Display Count Before dropping duplicates - 1395\n"," --> Display Count After dropping duplicates - 1395\n","--------------------------------------------------------------------------------\n","           Display Augmented Dataframe information: df_concat_pass_1\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1395 entries, 0 to 1394\n","Data columns (total 6 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   code_description  1395 non-null   object\n"," 1   code_snippet      1395 non-null   object\n"," 2   import_line       153 non-null    object\n"," 3   Category          1395 non-null   object\n"," 4   function          1395 non-null   object\n"," 5   origin_str        1395 non-null   object\n","dtypes: object(6)\n","memory usage: 65.5+ KB\n","--------------------------------------------------------------------------------\n","           Display Augmented Dataframe rows: df_concat_pass_1\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7eb95bf1a2c0>"],"text/html":["<style type=\"text/css\">\n","#T_5bdcd_row0_col0, #T_5bdcd_row0_col1, #T_5bdcd_row0_col2, #T_5bdcd_row0_col3, #T_5bdcd_row0_col4, #T_5bdcd_row0_col5, #T_5bdcd_row1_col0, #T_5bdcd_row1_col1, #T_5bdcd_row1_col2, #T_5bdcd_row1_col3, #T_5bdcd_row1_col4, #T_5bdcd_row1_col5, #T_5bdcd_row2_col0, #T_5bdcd_row2_col1, #T_5bdcd_row2_col2, #T_5bdcd_row2_col3, #T_5bdcd_row2_col4, #T_5bdcd_row2_col5, #T_5bdcd_row3_col0, #T_5bdcd_row3_col1, #T_5bdcd_row3_col2, #T_5bdcd_row3_col3, #T_5bdcd_row3_col4, #T_5bdcd_row3_col5, #T_5bdcd_row4_col0, #T_5bdcd_row4_col1, #T_5bdcd_row4_col2, #T_5bdcd_row4_col3, #T_5bdcd_row4_col4, #T_5bdcd_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_5bdcd\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_5bdcd_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_5bdcd_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_5bdcd_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_5bdcd_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_5bdcd_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_5bdcd_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_5bdcd_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_5bdcd_row0_col0\" class=\"data row0 col0\" >Pyspark code to overwrite a DataFrame into a CSV file.The data is overwritten to file or directory MY_DIR .</td>\n","      <td id=\"T_5bdcd_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_5bdcd_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_5bdcd_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_5bdcd_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_5bdcd_row0_col5\" class=\"data row0 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_5bdcd_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_5bdcd_row1_col0\" class=\"data row1 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_5bdcd_row1_col1\" class=\"data row1 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_5bdcd_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_5bdcd_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_5bdcd_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_5bdcd_row1_col5\" class=\"data row1 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_5bdcd_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_5bdcd_row2_col0\" class=\"data row2 col0\" >Pyspark code to overwrite a DataFrame into a JSON file.The data is overwritten to file or directory MY_DIR .</td>\n","      <td id=\"T_5bdcd_row2_col1\" class=\"data row2 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_5bdcd_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_5bdcd_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_5bdcd_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_5bdcd_row2_col5\" class=\"data row2 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_5bdcd_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_5bdcd_row3_col0\" class=\"data row3 col0\" >Pyspark code which Specifies the input data source format.The data is overwritten to file or directory MY_DIR .</td>\n","      <td id=\"T_5bdcd_row3_col1\" class=\"data row3 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_5bdcd_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_5bdcd_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_5bdcd_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_5bdcd_row3_col5\" class=\"data row3 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_5bdcd_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_5bdcd_row4_col0\" class=\"data row4 col0\" >Pyspark code to Read the JSON file as a DataFrame.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_5bdcd_row4_col1\" class=\"data row4 col1\" >df = spark.read.format('json').load(MY_DIR)</td>\n","      <td id=\"T_5bdcd_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_5bdcd_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_5bdcd_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_5bdcd_row4_col5\" class=\"data row4 col5\" >original</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":22}],"source":["# CONCATENATE AUGMENTED DATASETS - PASS 1\n","\n","df_concat_pass_1 = pd.concat([df_final_standardized, df_augment_display_fn, df_augment_sort_fn, df_augment_misc_fn], ignore_index=True)\n","\n","# Drop Duplicates\n","fn_display_message(f' --> Display Count Before dropping duplicates - {df_concat_pass_1.shape[0]}')\n","df_concat_pass_1 = df_concat_pass_1.drop_duplicates()\n","fn_display_message(f' --> Display Count After dropping duplicates - {df_concat_pass_1.shape[0]}')\n","\n","# Fix some common syntax error\n","df_concat_pass_1['code_description']  = df_concat_pass_1['code_description'].apply(lambda x: x.replace('..', '.').replace('  ', ' ').replace('. .', '.'))\n","\n","fn_display_header('Display Augmented Dataframe information: df_concat_pass_1')\n","df_concat_pass_1.info()\n","\n","fn_display_header('Display Augmented Dataframe rows: df_concat_pass_1')\n","df_concat_pass_1.head().style.set_properties(**{'text-align': 'left'})\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ulXanKlGp9ql"},"source":["<br>\n","<br>\n","<br>\n","\n","----\n","# <b>TEXT AUGMENTATION</b>\n","----"]},{"cell_type":"markdown","metadata":{"id":"kDsAvtEprPUc"},"source":["<br>\n","\n","# Text Augmentation: SUMY"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":242},"id":"a_4WCmfPp87j","executionInfo":{"status":"ok","timestamp":1721677994433,"user_tz":-330,"elapsed":1413,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"6a70febe-a1d0-43f6-d513-f66c406f884d"},"outputs":[{"output_type":"stream","name":"stderr","text":["1395it [00:01, 840.05it/s] \n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7eb95af42aa0>"],"text/html":["<style type=\"text/css\">\n","#T_713ab_row0_col0, #T_713ab_row0_col1, #T_713ab_row1_col0, #T_713ab_row1_col1, #T_713ab_row2_col0, #T_713ab_row2_col1, #T_713ab_row3_col0, #T_713ab_row3_col1, #T_713ab_row4_col0, #T_713ab_row4_col1 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_713ab\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_713ab_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_713ab_level0_col1\" class=\"col_heading level0 col1\" >sumy_summary</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_713ab_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_713ab_row0_col0\" class=\"data row0 col0\" >Pyspark code to overwrite a DataFrame into a CSV file.The data is overwritten to file or directory MY_DIR .</td>\n","      <td id=\"T_713ab_row0_col1\" class=\"data row0 col1\" >Pyspark code to overwrite a DataFrame into a CSV file The data is overwritten to file or directory MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_713ab_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_713ab_row1_col0\" class=\"data row1 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_713ab_row1_col1\" class=\"data row1 col1\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon' The data is read from file or directory MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_713ab_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_713ab_row2_col0\" class=\"data row2 col0\" >Pyspark code to overwrite a DataFrame into a JSON file.The data is overwritten to file or directory MY_DIR .</td>\n","      <td id=\"T_713ab_row2_col1\" class=\"data row2 col1\" >Pyspark code to overwrite a DataFrame into a JSON file The data is overwritten to file or directory MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_713ab_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_713ab_row3_col0\" class=\"data row3 col0\" >Pyspark code which Specifies the input data source format.The data is overwritten to file or directory MY_DIR .</td>\n","      <td id=\"T_713ab_row3_col1\" class=\"data row3 col1\" >Pyspark code which Specifies the input data source format The data is overwritten to file or directory MY_DIR</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_713ab_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_713ab_row4_col0\" class=\"data row4 col0\" >Pyspark code to Read the JSON file as a DataFrame.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_713ab_row4_col1\" class=\"data row4 col1\" >Pyspark code to Read the JSON file as a DataFrame The data is read from file or directory MY_DIR</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":23}],"source":["# CODE_DESCRIPTION AUGMENTATION TECHNIQUE - SUMY\n","\n","df_aug_sumy = df_concat_pass_1.copy()\n","\n","from sumy.parsers.plaintext import PlaintextParser\n","from sumy.nlp.tokenizers import Tokenizer\n","from sumy.summarizers.lex_rank import LexRankSummarizer\n","\n","# Parse the code_description column as plain text\n","sumy_summaries = []\n","for index, row in tqdm(df_aug_sumy.iterrows()):\n","\n","  # Concatenate all lines in the code_description column\n","  code_description = \"\\n\".join(row[\"code_description\"].split('.'))\n","\n","  sumy_parser = PlaintextParser.from_string(code_description, Tokenizer(\"english\"))\n","\n","  # LexRankSummarizer: LexRank is an unsupervised approach inspired by algorithms like PageRank and HITS (Hypertext Induced Topic Selection).\n","  # It ranks sentences based on their cosine similarity and degree centrality in the sentence graph\n","  sumy_summarizer = LexRankSummarizer()\n","  sumy_summary = sumy_summarizer(sumy_parser.document, 2)\n","\n","  sumy_summary = '. '.join([str(sentence) for sentence in sumy_summary])\n","  sumy_summaries.append(sumy_summary)\n","\n","df_aug_sumy['sumy_summary'] = sumy_summaries\n","\n","df_aug_sumy[\n","    df_aug_sumy['code_description'] != df_aug_sumy['sumy_summary']\n","#].style.set_properties(**{'text-align': 'left'})\n","][[ 'code_description', 'sumy_summary']].head().style.set_properties(**{'text-align': 'left'})\n"]},{"cell_type":"markdown","metadata":{"id":"NGA8LWPGq0fK"},"source":["<br>\n","\n","# Text Augmentation: T5"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":672,"referenced_widgets":["40828875e01c4671868188f6318510fd","f7e3eb0f5a2b431d9e9751cee920500d","309272fd4888469a82b93864ca4d3b3e","9a024194a0444461a21cb7195bb2eb6d","964e2de3e856430a8d824f1f3b8bce94","b0410281c6a248f6b824bdb3db29d5ed","22fef2e55d1b4f598fa378e05e9d64c9","b5deba20d4494686b6dfcee43f8dfc0c","864dba3355394cbfad5dfb67248974a8","9b5bfe6310364aa7aea95af57adcd5bc","39039d0d971548a0943aeb93b043b380","925d89769bf54542871565c74fd3a48e","4e9b9d3ebe4a4888b42c85cb72ba253d","cefd9d9633894a59af0cf779f0c832bf","c4e8aa05e79447d0bf45bfafcd058248","97893a742dd348e29afb2fb4a0d40398","7cbc3d659591443babb88f893d2eddb3","5ad365d7e94d474dab96835a4f3a9fff","6f7a16b0e58d45f39248c12a85ce87de","a1d28ca097da47638f8b6886100174b7","5123260de25d4030a06f832d0c7c7d58","b387312bdc2943c1a3be194f798be56b","eadc957be9e641c6b8120cab8ebd3d3d","9adc4d67bb95427c878fce963c8b8a6d","a57cdca15ed14e6eb6364261314a76b2","67ce391fe08547eeb89b6db141fc775c","d1396a2b84aa41cdad589245e99fffef","446f23c078be47128bd2dc14ba25d6ac","b030d6373939432481b1d9967468dd3b","718a1da8808e4bd6b761d5fcf6847926","be999b9854c6442f99699ce0ac13ecf7","c0e31d7bd38d45f8811dae303ac707c6","053144cd1f9c4d52830ae484787ae15b","8da5a25691da48cb880c8dd92a792ede","f6b73ec913a04f358211bcfdc5839682","bada275ba2b2420faf40c26bbad6e60c","c7cf6a64876a4006b4291704357844d0","fb240a121fb349ce8a3425513c987d21","7fe514d52379467587acf4cb880aaed3","7b5cdd3b3e5b4bc797c6664396df9363","7eb0b150e13e49e098cc5172e678535c","b7921536188343b7843ab233f0b16540","3a00bf065fb14589a39a8c19df22143e","9486dda6a37f49348964bf4abeaadfb7","0a618c38c6394c53b6f95cab276d2a2a","7555c11df1b048fbb351eda4564e6186","cab4c4fa100442a6a3c7a48775c82e82","bca5b7b988084e9d8f48acaf92cb7a4d","37dde5b1795c465db8277e8bfbeef074","10c0c9531986464aa1cc966e930cd05a","3f1e5d6329454f25bdf58aebb9686c8e","eb3693713eb7426dba1d3540ab0f7382","410fd5b29dc1456b82c0ee9c0c9568e8","03c25f7ae75a4b7692aeb017fcd7b970","891c82bb0e674ff9b05f5bec35a2b9b4","ca1dbb4a207b4b8a8e19219db8023b1d","a1649040d6d1483eb2f3b639c91d54df","2c4db98f4ef047e694ade3efb11c171c","280df210ef5c4834997675f3fa1446f1","c5b99384e2e04facb80b867d4ab40fbf","e6d662c36dca467c8f36d7e460a4355f","04d513b8dcf64c3c9eb313eb89c3b7ad","0208c990b3444367ba4e5b7295166778","5bdba1f73f0e468d80afa43a8c47e9d2","0267f6b843b9477fb271392981689d23","69579fade96846c5be6e89ae0bfd3c71"]},"id":"odmByd-X1Y--","executionInfo":{"status":"ok","timestamp":1721678037093,"user_tz":-330,"elapsed":42666,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"28ca92a1-9faa-4f4f-ce18-0cd78ee0a889"},"outputs":[{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40828875e01c4671868188f6318510fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"925d89769bf54542871565c74fd3a48e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eadc957be9e641c6b8120cab8ebd3d3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8da5a25691da48cb880c8dd92a792ede"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a618c38c6394c53b6f95cab276d2a2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca1dbb4a207b4b8a8e19219db8023b1d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           Display the T5 Summaries for the first 5 rows in dataset\n","--------------------------------------------------------------------------------\n","------ 0 ------\n","text    : Pyspark code to overwrite a DataFrame into a CSV file.The data is overwritten to file or directory MY_DIR .\n","summarry: Pyspark code to overwrite a DataFrame into a CSV file.The data is overwritten to file or directory MY_DIR .\n","------ 1 ------\n","text    : Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.The data is read from file or directory MY_DIR.\n","summarry: Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.The data is read from file or directory MY_DIR.\n","------ 2 ------\n","text    : Pyspark code to overwrite a DataFrame into a JSON file.The data is overwritten to file or directory MY_DIR .\n","summarry: Pyspark code to overwrite a DataFrame into a JSON file.The data is overwritten to file or directory MY_DIR .\n","------ 3 ------\n","text    : Pyspark code which Specifies the input data source format.The data is overwritten to file or directory MY_DIR .\n","summarry: Pyspark code which Specifies the input data source format.The data is overwritten to file or directory MY_DIR .\n","------ 4 ------\n","text    : Pyspark code to Read the JSON file as a DataFrame.The data is read from file or directory MY_DIR.\n","summarry: Pyspark code to Read the JSON file as a DataFrame.The data is read from file or directory MY_DIR.\n"]}],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","df_aug_t5_summarizer = df_concat_pass_1.copy()\n","\n","def fn_summarize_text(text, model, tokenizer, max_length=512, num_beams=3):\n","    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=max_length, truncation=True)\n","    summary_ids = model.generate(inputs, max_length=50, num_beams=num_beams)\n","    summarized_text = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","    return summarized_text\n","\n","t5_summaries = []\n","cnt = 5\n","for index, row in df_aug_t5_summarizer.head(cnt).iterrows():\n","  model=T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n","  tokenizer=T5Tokenizer.from_pretrained(\"t5-small\")\n","\n","  text = row['code_description']\n","  summary = fn_summarize_text(text, model, tokenizer)\n","  t5_summaries.append([text, summary])\n","\n","fn_display_header(f\"Display the T5 Summaries for the first {cnt} rows in dataset\")\n","for idx, item in enumerate(t5_summaries):\n","  print(f\"------ {idx} ------\")\n","  print(f\"text    : {item[0]}\")\n","  print(f\"summarry: {item[0]}\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jdz0zycJv9yA"},"source":["<br>\n","\n","# Text Augmentation: Synonym Replacement"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t9dyVPRTv9Rg","executionInfo":{"status":"ok","timestamp":1721678039220,"user_tz":-330,"elapsed":1741,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"9666e1bc-1223-4613-fdd5-46a7950a08ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["------ 1 ------\n","text    : Pyspark code which returns a reversed string or an array with reverse order of elements in column COL_A in dataframe MY_DF.The result is applied for all rows in MY_DF and stored in the column MY_ALIAS_A in the target dataframe.\n","summarry: Pyspark code which returns a reversed string or an array with reverse order of element in chromatographycolumn COL_A in dataframe MY_DF.The result is applied for all quarrel in MY_DF and stored in the chromatographycolumn MY_ALIAS_A in the target dataframe.\n","Pyspark code which Returns a new row for each element with position in the regalia or map precondition by column COL_C in dataframe MY_DF in dataframe MY_DF.Unlike posexplode if the regalia map is null or empty then the row null null is produced.. It includes column COL_A and column COL_B in the blowup process.The result is applied for all rows in MY_DF and stored in the column MY_ALIAS_A in the target dataframe.\n","------ 1 ------\n","text    : Pyspark code which Returns a new row for each element with position in the array or map given by column COL_C in dataframe MY_DF in dataframe MY_DF.Unlike posexplode if the array map is null or empty then the row null null is produced.. It includes column COL_A and column COL_B in the explode process.The result is applied for all rows in MY_DF and stored in the column MY_ALIAS_A in the target dataframe.\n","summarry: Pyspark code which Returns a new row for each element with position in the regalia or map precondition by column COL_C in dataframe MY_DF in dataframe MY_DF.Unlike posexplode if the regalia map is null or empty then the row null null is produced.. It includes column COL_A and column COL_B in the blowup process.The result is applied for all rows in MY_DF and stored in the column MY_ALIAS_A in the target dataframe.\n"]}],"source":["from nltk.corpus import wordnet as wn\n","from nltk.corpus import stopwords\n","import random\n","\n","def get_synonyms(word, pos=None, max_synonyms=5):\n","    synonyms = set()\n","    count = 0\n","    for syn in wn.synsets(word, pos=pos):\n","        for lemma in syn.lemmas():\n","            synonym = lemma.name().replace(\"_\", \" \").replace(\"-\", \"\").lower()\n","            synonym = \"\".join([char for char in synonym if char in \"abcdefghijklmnopqrstuvwxyz\"])\n","            synonyms.add(synonym)\n","            count += 1\n","            if count >= max_synonyms:\n","                break\n","    if word in synonyms:\n","        synonyms.remove(word)\n","    return list(synonyms)\n","\n","# Replace synonyms in your text\n","def synonym_replacement(words, n):\n","    words = words.split()\n","    new_words = words.copy()\n","    random_word_list = list(set([word for word in words if word not in stopwords.words('english')]))\n","    random.shuffle(random_word_list)\n","    num_replaced = 0\n","    for random_word in random_word_list:\n","        synonyms = get_synonyms(random_word)\n","        if len(synonyms) >= 1:\n","            synonym = random.choice(list(synonyms))\n","            new_words = [synonym if word == random_word else word for word in new_words]\n","            num_replaced += 1\n","            if num_replaced >= n:\n","                break\n","    sentence = ' '.join(new_words)\n","    return sentence\n","\n","# Example 1\n","original_text = \"Pyspark code which returns a reversed string or an array with reverse order of elements in column COL_A in dataframe MY_DF.The result is applied for all rows in MY_DF and stored in the column MY_ALIAS_A in the target dataframe.\"\n","augmented_text = synonym_replacement(original_text, n=3)  # Replace 3 words with synonyms\n","print(f\"------ 1 ------\")\n","print(f\"text    : {original_text}\")\n","print(f\"summarry: {augmented_text}\")\n","\n","# Example 2\n","original_text = \"Pyspark code which Returns a new row for each element with position in the array or map given by column COL_C in dataframe MY_DF in dataframe MY_DF.Unlike posexplode if the array map is null or empty then the row null null is produced.. It includes column COL_A and column COL_B in the explode process.The result is applied for all rows in MY_DF and stored in the column MY_ALIAS_A in the target dataframe.\"\n","augmented_text = synonym_replacement(original_text, n=3)  # Replace 3 words with synonyms\n","print(augmented_text)\n","print(f\"------ 1 ------\")\n","print(f\"text    : {original_text}\")\n","print(f\"summarry: {augmented_text}\")\n"]},{"cell_type":"markdown","metadata":{"id":"x_AmUmn5SQDz"},"source":["<br>\n","<br>\n","<br>\n","\n","----\n","# <b>AUGMENTATION SECTION: EXPLODE ON STANDARDIZED IDENTIFIERS</b>\n","----\n"]},{"cell_type":"markdown","metadata":{"id":"HYMZ16DWrufA"},"source":["<br>\n","\n","# Augmentation: Explode on MY_DIR"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":382,"status":"ok","timestamp":1721678054187,"user":{"displayName":"SK G","userId":"17874234191554613076"},"user_tz":-330},"id":"sMSWAfUTenph","outputId":"ac72dcc8-90b4-44b7-aa1e-613eb62e5a9a"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           Explode on MY_DIR\n","--------------------------------------------------------------------------------\n"," --> Display Rows with MY_DIR within code: 194\n"," --> Display Count Before Explode: 194\n"," --> Display Count After Explode: 3578\n"]}],"source":["# Augmentation: Explode on MY_DIR\n","\n","def fn_explode_my_dir(comment, code):\n","\n","  random_write_dir_names = [\n","      'result_dir', 'output_folder', 'output_path', 'output_location', 'output_directory', 'output_dest', 'output_archive', 'output_store', 'output_dump',\n","      #'output_export', 'output_dir', 'out_directory', 'out_path', 'out_loc', 'out_location', 'out_destination', 'out_dir'\n","      ]\n","  random_read_dir_names  = [\n","      'data_in', 'input_data', 'input_folder', 'input_source', 'input_path', 'input_archive', 'input_store', 'input_dump', 'input_source', 'input_location',\n","      #'input_loc', 'archive_dir', 'archive_directory', 'inp_loc', 'inp_src', 'file_location', 'file_dir'\n","      ]\n","\n","  result_array = []\n","\n","  if '.read.' in code:\n","    for item in random_read_dir_names:\n","      # Pass 1 with upper case replacements\n","      result_array.append(comment.replace('MY_DIR', item.upper()) + '--delim--' + code.replace('MY_DIR', item.upper()))\n","\n","      # Pass 2 with lower case replacements\n","      result_array.append(comment.replace('MY_DIR', item) + '--delim--' + code.replace('MY_DIR', item))\n","\n","  elif '.write.' in code:\n","    for item in random_write_dir_names:\n","      # Pass 1 with upper case replacements\n","      result_array.append(comment.replace('MY_DIR', item.upper()) + '--delim--' + code.replace('MY_DIR', item.upper()))\n","\n","      # Pass 2 with lower case replacements\n","      result_array.append(comment.replace('MY_DIR', item) + '--delim--' + code.replace('MY_DIR', item))\n","\n","  result = result_array\n","  return result\n","\n","fn_display_header(\"Explode on MY_DIR\")\n","df_aug_explode_my_dir = df_concat_pass_1[(df_concat_pass_1['code_snippet'].str.contains('MY_DIR'))].copy()\n","\n","fn_display_message(f' --> Display Rows with MY_DIR within code: {df_aug_explode_my_dir.shape[0]}')\n","\n","# Apply explode function\n","df_aug_explode_my_dir['result_array']  = df_aug_explode_my_dir.apply(lambda x: fn_explode_my_dir(x['code_description'], x['code_snippet']), axis=1)\n","\n","# Normalize the code on result_array\n","fn_display_message(f' --> Display Count Before Explode: {df_aug_explode_my_dir.shape[0]}')\n","df_aug_explode_my_dir = df_aug_explode_my_dir.explode('result_array').reset_index()\n","fn_display_message(f' --> Display Count After Explode: {df_aug_explode_my_dir.shape[0]}')\n","\n","# Split and assign result_array to code and comment\n","df_aug_explode_my_dir['code_description'] = df_aug_explode_my_dir['result_array'].apply(lambda x: x.split('--delim--')[0])\n","df_aug_explode_my_dir['code_snippet'] = df_aug_explode_my_dir['result_array'].apply(lambda x: x.split('--delim--')[1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":538},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1721678057994,"user":{"displayName":"SK G","userId":"17874234191554613076"},"user_tz":-330},"id":"X4AFxvUMepbb","outputId":"f042054f-0da5-4ff1-e49a-176bbdd7d4e3"},"outputs":[{"output_type":"stream","name":"stdout","text":[" --> Display Count Before dropping duplicates - 3578\n"," --> Display Count After dropping duplicates - 3492\n","--------------------------------------------------------------------------------\n","           Display Augmented Dataframe information: df_aug_explode_my_dir\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","Index: 3492 entries, 0 to 3577\n","Data columns (total 6 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   code_description  3492 non-null   object\n"," 1   code_snippet      3492 non-null   object\n"," 2   import_line       0 non-null      object\n"," 3   Category          3492 non-null   object\n"," 4   function          3492 non-null   object\n"," 5   origin_str        3492 non-null   object\n","dtypes: object(6)\n","memory usage: 191.0+ KB\n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7eb95af7f4c0>"],"text/html":["<style type=\"text/css\">\n","#T_5bd54_row0_col0, #T_5bd54_row0_col1, #T_5bd54_row0_col2, #T_5bd54_row0_col3, #T_5bd54_row0_col4, #T_5bd54_row0_col5, #T_5bd54_row1_col0, #T_5bd54_row1_col1, #T_5bd54_row1_col2, #T_5bd54_row1_col3, #T_5bd54_row1_col4, #T_5bd54_row1_col5, #T_5bd54_row2_col0, #T_5bd54_row2_col1, #T_5bd54_row2_col2, #T_5bd54_row2_col3, #T_5bd54_row2_col4, #T_5bd54_row2_col5, #T_5bd54_row3_col0, #T_5bd54_row3_col1, #T_5bd54_row3_col2, #T_5bd54_row3_col3, #T_5bd54_row3_col4, #T_5bd54_row3_col5, #T_5bd54_row4_col0, #T_5bd54_row4_col1, #T_5bd54_row4_col2, #T_5bd54_row4_col3, #T_5bd54_row4_col4, #T_5bd54_row4_col5 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_5bd54\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_5bd54_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_5bd54_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_5bd54_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_5bd54_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_5bd54_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_5bd54_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_5bd54_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_5bd54_row0_col0\" class=\"data row0 col0\" >Pyspark code to overwrite a DataFrame into a CSV file.The data is overwritten to file or directory RESULT_DIR .</td>\n","      <td id=\"T_5bd54_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(RESULT_DIR)</td>\n","      <td id=\"T_5bd54_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_5bd54_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_5bd54_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_5bd54_row0_col5\" class=\"data row0 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_5bd54_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_5bd54_row1_col0\" class=\"data row1 col0\" >Pyspark code to overwrite a DataFrame into a CSV file.The data is overwritten to file or directory result_dir .</td>\n","      <td id=\"T_5bd54_row1_col1\" class=\"data row1 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(result_dir)</td>\n","      <td id=\"T_5bd54_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_5bd54_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_5bd54_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_5bd54_row1_col5\" class=\"data row1 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_5bd54_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_5bd54_row2_col0\" class=\"data row2 col0\" >Pyspark code to overwrite a DataFrame into a CSV file.The data is overwritten to file or directory OUTPUT_FOLDER .</td>\n","      <td id=\"T_5bd54_row2_col1\" class=\"data row2 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(OUTPUT_FOLDER)</td>\n","      <td id=\"T_5bd54_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_5bd54_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_5bd54_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_5bd54_row2_col5\" class=\"data row2 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_5bd54_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_5bd54_row3_col0\" class=\"data row3 col0\" >Pyspark code to overwrite a DataFrame into a CSV file.The data is overwritten to file or directory output_folder .</td>\n","      <td id=\"T_5bd54_row3_col1\" class=\"data row3 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(output_folder)</td>\n","      <td id=\"T_5bd54_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_5bd54_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_5bd54_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_5bd54_row3_col5\" class=\"data row3 col5\" >original</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_5bd54_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_5bd54_row4_col0\" class=\"data row4 col0\" >Pyspark code to overwrite a DataFrame into a CSV file.The data is overwritten to file or directory OUTPUT_PATH .</td>\n","      <td id=\"T_5bd54_row4_col1\" class=\"data row4 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(OUTPUT_PATH)</td>\n","      <td id=\"T_5bd54_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_5bd54_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_5bd54_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_5bd54_row4_col5\" class=\"data row4 col5\" >original</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":27}],"source":["# DROP DUPLICATES from Explode dataset for MY_DIR\n","\n","df_aug_explode_my_dir = df_aug_explode_my_dir[['code_description', 'code_snippet', 'import_line', 'Category', 'function', 'origin_str']].copy()\n","\n","# Drop Duplicates\n","fn_display_message(f' --> Display Count Before dropping duplicates - {df_aug_explode_my_dir.shape[0]}')\n","df_aug_explode_my_dir = df_aug_explode_my_dir.drop_duplicates()\n","fn_display_message(f' --> Display Count After dropping duplicates - {df_aug_explode_my_dir.shape[0]}')\n","\n","fn_display_header('Display Augmented Dataframe information: df_aug_explode_my_dir')\n","df_aug_explode_my_dir.info()\n","\n","df_aug_explode_my_dir.head().style.set_properties(**{'text-align': 'left'})"]},{"cell_type":"markdown","metadata":{"id":"HBe0nS5ZoqhZ"},"source":["<br>\n","\n","# Augmentation: Explode on STANDARDIZED COLUMNS and LITERALS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5496,"status":"ok","timestamp":1721678609702,"user":{"displayName":"SK G","userId":"17874234191554613076"},"user_tz":-330},"id":"Z3bIaVW6YLpb","outputId":"2d575428-14b9-404d-f5d2-613122524f30"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           Explode on MY_DIR\n","--------------------------------------------------------------------------------\n"," --> Display Rows with COL_ within code: 533\n"," ----------- Process for COL_A -----------\n"," --> Display Count Before Explode for COL_A: 533\n"," --> Display Count After Explode  for COL_A: 2132\n"," --> Display Count Before dropping duplicates for COL_A: 2132\n"," --> Display Count After dropping duplicates  for COL_A: 1934\n"," ----------- Process for COL_B -----------\n"," --> Display Count Before Explode for COL_B: 1934\n"," --> Display Count After Explode  for COL_B: 7736\n"," --> Display Count Before dropping duplicates for COL_B: 7736\n"," --> Display Count After dropping duplicates  for COL_B: 3587\n"," ----------- Process for COL_C -----------\n"," --> Display Count Before Explode for COL_C: 3587\n"," --> Display Count After Explode  for COL_C: 14348\n"," --> Display Count Before dropping duplicates for COL_C: 14348\n"," --> Display Count After dropping duplicates  for COL_C: 4562\n"," ----------- Process for COL_D -----------\n"," --> Display Count Before Explode for COL_D: 4562\n"," --> Display Count After Explode  for COL_D: 27372\n"," --> Display Count Before dropping duplicates for COL_D: 27372\n"," --> Display Count After dropping duplicates  for COL_D: 4642\n"," ----------- Process for COL_E -----------\n"," --> Display Count Before Explode for COL_E: 4642\n"," --> Display Count After Explode  for COL_E: 27852\n"," --> Display Count Before dropping duplicates for COL_E: 27852\n"," --> Display Count After dropping duplicates  for COL_E: 5042\n"," ----------- Process for MY_ALIAS_A -----------\n"," --> Display Count Before Explode for MY_ALIAS_A: 5042\n"," --> Display Count After Explode  for MY_ALIAS_A: 15126\n"," --> Display Count Before dropping duplicates for MY_ALIAS_A: 15126\n"," --> Display Count After dropping duplicates  for MY_ALIAS_A: 11932\n"," ----------- Process for LIT_STR_0 -----------\n"," --> Display Count Before Explode for LIT_STR_0: 11932\n"," --> Display Count After Explode  for LIT_STR_0: 35796\n"," --> Display Count Before dropping duplicates for LIT_STR_0: 35796\n"," --> Display Count After dropping duplicates  for LIT_STR_0: 12168\n"," ----------- Process for LIT_STR_1 -----------\n"," --> Display Count Before Explode for LIT_STR_1: 12168\n"," --> Display Count After Explode  for LIT_STR_1: 48672\n"," --> Display Count Before dropping duplicates for LIT_STR_1: 48672\n"," --> Display Count After dropping duplicates  for LIT_STR_1: 12447\n"," ----------- Process for LIT_INT_0 -----------\n"," --> Display Count Before Explode for LIT_INT_0: 12447\n"," --> Display Count After Explode  for LIT_INT_0: 49788\n"," --> Display Count Before dropping duplicates for LIT_INT_0: 49788\n"," --> Display Count After dropping duplicates  for LIT_INT_0: 12873\n"," ----------- Process for LIT_INT_1 -----------\n"," --> Display Count Before Explode for LIT_INT_1: 12873\n"," --> Display Count After Explode  for LIT_INT_1: 51492\n"," --> Display Count Before dropping duplicates for LIT_INT_1: 51492\n"," --> Display Count After dropping duplicates  for LIT_INT_1: 14277\n"," ----------- Process for LIT_INT_2 -----------\n"," --> Display Count Before Explode for LIT_INT_2: 14277\n"," --> Display Count After Explode  for LIT_INT_2: 57108\n"," --> Display Count Before dropping duplicates for LIT_INT_2: 57108\n"," --> Display Count After dropping duplicates  for LIT_INT_2: 14853\n"," ----------- Process for LIT_DEC_0 -----------\n"," --> Display Count Before Explode for LIT_DEC_0: 14853\n"," --> Display Count After Explode  for LIT_DEC_0: 44559\n"," --> Display Count Before dropping duplicates for LIT_DEC_0: 44559\n"," --> Display Count After dropping duplicates  for LIT_DEC_0: 15017\n"," ----------- Process for LIT_DEC_1 -----------\n"," --> Display Count Before Explode for LIT_DEC_1: 15017\n"," --> Display Count After Explode  for LIT_DEC_1: 60068\n"," --> Display Count Before dropping duplicates for LIT_DEC_1: 60068\n"," --> Display Count After dropping duplicates  for LIT_DEC_1: 15062\n","--------------------------------------------------------------------------------\n","           Display Augmented Dataframe information: df_aug_explode_cols_lits\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 15062 entries, 0 to 15061\n","Data columns (total 8 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   index             15062 non-null  int64 \n"," 1   code_description  15062 non-null  object\n"," 2   code_snippet      15062 non-null  object\n"," 3   import_line       1191 non-null   object\n"," 4   Category          15062 non-null  object\n"," 5   function          15062 non-null  object\n"," 6   origin_str        15062 non-null  object\n"," 7   result_array      15062 non-null  object\n","dtypes: int64(1), object(7)\n","memory usage: 941.5+ KB\n"]}],"source":["# Augmentation: Explode on STANDARDIZED COLUMNS COL_A. COL_B etc\n","\n","# Function to replace columns with values\n","def fn_col_explode_replacement(code, comment, search_str, replace_str, delim, upcase):\n","  result = ''\n","  delim = '--delim--'\n","  if upcase == 'Y':\n","    # Pass 1 with upper case replacements\n","    result = comment.replace(search_str, replace_str.upper()) + delim + code.replace(search_str, replace_str.upper())\n","  else:\n","    # Pass 2 with lower case replacements\n","    result = comment.replace(search_str, replace_str) + delim + code.replace(search_str, replace_str)\n","\n","  return result\n","\n","# Function to replace literals with values\n","def fn_lit_explode_replacement(code, comment, search_str, replace_str, delim, upcase):\n","  result = ''\n","  delim = '--delim--'\n","  result = comment.replace(search_str, replace_str.upper()) + delim + code.replace(search_str, replace_str.upper())\n","\n","  return result\n","\n","# Function to explode the code and comments\n","def fn_explode_cols_lits(comment, code, std_name):\n","\n","  delim = '--delim--'\n","\n","  col_a_list = ['employee_id', 'customer_age', 'product_quantity', 'order_total', #'transaction_date',# 'product_category'\n","      #, 'sales_representative', 'customer_zipcode', 'inventory_stock', 'supplier_name', 'region_code', 'payment_method', 'customer_segment', 'product_rating', 'shipment_status', 'customer_feedback', 'product_quality', 'order_source', 'supplier_payment_terms', 'employee_certifications'\n","      ]\n","\n","  col_b_list = ['customer_gender', 'order_priority', 'product_brand', 'department_code', #'account_balance',# 'employee_role'\n","      #, 'customer_city', 'product_color', 'contract_duration', 'project_manager', 'store_location', 'customer_income', 'product_weight', 'campaign_channel', 'service_level', 'customer_purchase_history', 'product_sustainability', 'payment_security_code', 'sales_lead_time', 'customer_social_media'\n","      ]\n","\n","  col_c_list = ['product_type', 'order_date', 'supplier_country', 'employee_department', #'customer_lifetime_value',# 'product_discount'\n","      #, 'payment_due_date', 'sales_region', 'customer_email', 'product_material', 'order_ship_date', 'supplier_contact', 'employee_manager', 'customer_phone', 'product_size', 'product_compliance', 'order_return_reason', 'supplier_lead_time', 'employee_workload', 'customer_referral_source'\n","      ]\n","\n","  col_d_list = [\n","      'payment_status', 'sales_target', 'customer_address', 'product_expiry_date', 'order_quantity', 'supplier_rating'\n","      #, 'employee_tenure', 'customer_subscription', 'product_origin', 'payment_amount',\n","      #'sales_channel', 'customer_marital_status', 'product_warranty', 'order_status', 'supplier_location'\n","      ]\n","\n","  col_e_list = ['employee_performance', 'customer_membership', 'product_features', 'payment_reference', 'sales_growth_rate', 'customer_loyalty'\n","      #, 'product_style', 'order_discount', 'supplier_type', 'employee_shift', 'customer_interests', 'product_season', 'payment_frequency', 'sales_conversion_rate'\n","      ]\n","\n","  col_alias_list = ['target_column', 'output_column', 'result_col'] #, 'results'  ]\n","\n","  lit_str_0_list = [\"hello\", \"pyspark\", \"python\"]\n","  lit_str_1_list = [\"course\", \"sales\", \"order\", \"NA\"]\n","\n","  lit_int_0_list = ['1', '7', '-12', '33']\n","  lit_int_1_list = ['44', '-172', '165', '-775']\n","  lit_int_2_list = ['63', '-66', '1899', '1714']\n","\n","  lit_dec_0_list = ['0.723', '2.71', '-4.56']\n","  lit_dec_1_list = ['7.21', '8.23', '-3.5501', '-0.23']\n","\n","  result_array = []\n","\n","  final_column_list = col_a_list          if std_name == 'COL_A' else []\n","  final_column_list = col_b_list          if std_name == 'COL_B' else final_column_list\n","  final_column_list = col_c_list          if std_name == 'COL_C' else final_column_list\n","  final_column_list = col_d_list          if std_name == 'COL_D' else final_column_list\n","  final_column_list = col_e_list          if std_name == 'COL_E' else final_column_list\n","  final_column_list = col_alias_list      if std_name == 'MY_ALIAS_A' else final_column_list\n","\n","  final_column_list = lit_str_0_list      if std_name == 'LIT_STR_0' else final_column_list\n","  final_column_list = lit_str_1_list      if std_name == 'LIT_STR_1' else final_column_list\n","  final_column_list = lit_int_0_list      if std_name == 'LIT_INT_0' else final_column_list\n","  final_column_list = lit_int_1_list      if std_name == 'LIT_INT_1' else final_column_list\n","  final_column_list = lit_int_2_list      if std_name == 'LIT_INT_2' else final_column_list\n","  final_column_list = lit_dec_0_list      if std_name == 'LIT_DEC_0' else final_column_list\n","  final_column_list = lit_dec_1_list      if std_name == 'LIT_DEC_1' else final_column_list\n","\n","\n","  for idx, replace_std_name in enumerate(final_column_list):\n","    upcase = 'N'\n","    if idx % 2 == 0:\n","       upcase = 'Y'\n","    if std_name in code and std_name in comment:\n","      result_array.append(fn_col_explode_replacement(code, comment, std_name, replace_std_name, delim, upcase))\n","    else:\n","      result_array.append(comment + delim + code)\n","\n","  result = result_array\n","  return result\n","\n","fn_display_header(\"Explode on MY_DIR\")\n","df_aug_explode_cols_lits = df_concat_pass_1[\n","  (df_concat_pass_1['code_snippet'].str.contains('COL_')) | (df_concat_pass_1['code_snippet'].str.contains('MY_ALIAS_')) | (df_concat_pass_1['code_snippet'].str.contains('LIT_'))\n","  ].copy()\n","\n","fn_display_message(f' --> Display Rows with COL_ within code: {df_aug_explode_cols_lits.shape[0]}')\n","\n","# Apply explode function in a loop for all standardized columns\n","df_aug_explode_cols_lits['result_array']  = df_aug_explode_cols_lits.apply(lambda x: [], axis=1)\n","for std_name in ['COL_A', 'COL_B', 'COL_C', 'COL_D', 'COL_E', 'MY_ALIAS_A', 'LIT_STR_0', 'LIT_STR_1', 'LIT_INT_0', 'LIT_INT_1', 'LIT_INT_2', 'LIT_DEC_0', 'LIT_DEC_1']:\n","  # Apply explode function\n","  df_aug_explode_cols_lits['result_array']  = df_aug_explode_cols_lits.apply(lambda x: fn_explode_cols_lits(x['code_description'], x['code_snippet'], std_name), axis=1)\n","\n","  # Normalize the code on result_array\n","  fn_display_message(f' ----------- Process for {std_name} -----------')\n","  fn_display_message(f' --> Display Count Before Explode for {std_name}: {df_aug_explode_cols_lits.shape[0]}')\n","  df_aug_explode_cols_lits = df_aug_explode_cols_lits.explode('result_array')\n","  fn_display_message(f' --> Display Count After Explode  for {std_name}: {df_aug_explode_cols_lits.shape[0]}')\n","\n","  # Drop Duplicates\n","  fn_display_message(f' --> Display Count Before dropping duplicates for {std_name}: {df_aug_explode_cols_lits.shape[0]}')\n","  df_aug_explode_cols_lits = df_aug_explode_cols_lits.drop_duplicates()\n","  fn_display_message(f' --> Display Count After dropping duplicates  for {std_name}: {df_aug_explode_cols_lits.shape[0]}')\n","\n","  # Split and assign result_array to code and comment\n","  df_aug_explode_cols_lits['code_description'] = df_aug_explode_cols_lits['result_array'].apply(lambda x: x.split('--delim--')[0])\n","  df_aug_explode_cols_lits['code_snippet']     = df_aug_explode_cols_lits['result_array'].apply(lambda x: x.split('--delim--')[1])\n","\n","fn_display_header('Display Augmented Dataframe information: df_aug_explode_cols_lits')\n","df_aug_explode_cols_lits = df_aug_explode_cols_lits.reset_index()\n","df_aug_explode_cols_lits.info()"]},{"cell_type":"markdown","metadata":{"id":"FUKi4fyoyEQ1"},"source":["<br>\n","\n","# CONCATENATE EXPLODED DATASETS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":773},"executionInfo":{"elapsed":704,"status":"ok","timestamp":1721678610373,"user":{"displayName":"SK G","userId":"17874234191554613076"},"user_tz":-330},"id":"h_Ehp_dmdtbu","outputId":"b85bcb0d-c591-4ebd-e13b-a6ee19c30737"},"outputs":[{"output_type":"stream","name":"stdout","text":[" --> Count df_concat_pass_1 : 1395\n"," --> Count df_aug_explode_my_dir : 3492\n"," --> Count df_aug_explode_cols_lits : 15062\n"," --> Display Count Before dropping duplicates - 19949\n"," --> Display Count After dropping duplicates - 19949\n","--------------------------------------------------------------------------------\n","           Display Augmented Dataframe information: df_concat_pass_2\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 19949 entries, 0 to 19948\n","Data columns (total 8 columns):\n"," #   Column            Non-Null Count  Dtype  \n","---  ------            --------------  -----  \n"," 0   code_description  19949 non-null  object \n"," 1   code_snippet      19949 non-null  object \n"," 2   import_line       1344 non-null   object \n"," 3   Category          19949 non-null  object \n"," 4   function          19949 non-null  object \n"," 5   origin_str        19949 non-null  object \n"," 6   index             15062 non-null  float64\n"," 7   result_array      15062 non-null  object \n","dtypes: float64(1), object(7)\n","memory usage: 1.2+ MB\n","--------------------------------------------------------------------------------\n","           Display Augmented Dataframe rows: df_concat_pass_2\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7eb95af7e290>"],"text/html":["<style type=\"text/css\">\n","#T_f6ad2_row0_col0, #T_f6ad2_row0_col1, #T_f6ad2_row0_col2, #T_f6ad2_row0_col3, #T_f6ad2_row0_col4, #T_f6ad2_row0_col5, #T_f6ad2_row0_col6, #T_f6ad2_row0_col7, #T_f6ad2_row1_col0, #T_f6ad2_row1_col1, #T_f6ad2_row1_col2, #T_f6ad2_row1_col3, #T_f6ad2_row1_col4, #T_f6ad2_row1_col5, #T_f6ad2_row1_col6, #T_f6ad2_row1_col7, #T_f6ad2_row2_col0, #T_f6ad2_row2_col1, #T_f6ad2_row2_col2, #T_f6ad2_row2_col3, #T_f6ad2_row2_col4, #T_f6ad2_row2_col5, #T_f6ad2_row2_col6, #T_f6ad2_row2_col7, #T_f6ad2_row3_col0, #T_f6ad2_row3_col1, #T_f6ad2_row3_col2, #T_f6ad2_row3_col3, #T_f6ad2_row3_col4, #T_f6ad2_row3_col5, #T_f6ad2_row3_col6, #T_f6ad2_row3_col7, #T_f6ad2_row4_col0, #T_f6ad2_row4_col1, #T_f6ad2_row4_col2, #T_f6ad2_row4_col3, #T_f6ad2_row4_col4, #T_f6ad2_row4_col5, #T_f6ad2_row4_col6, #T_f6ad2_row4_col7 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_f6ad2\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_f6ad2_level0_col0\" class=\"col_heading level0 col0\" >code_description</th>\n","      <th id=\"T_f6ad2_level0_col1\" class=\"col_heading level0 col1\" >code_snippet</th>\n","      <th id=\"T_f6ad2_level0_col2\" class=\"col_heading level0 col2\" >import_line</th>\n","      <th id=\"T_f6ad2_level0_col3\" class=\"col_heading level0 col3\" >Category</th>\n","      <th id=\"T_f6ad2_level0_col4\" class=\"col_heading level0 col4\" >function</th>\n","      <th id=\"T_f6ad2_level0_col5\" class=\"col_heading level0 col5\" >origin_str</th>\n","      <th id=\"T_f6ad2_level0_col6\" class=\"col_heading level0 col6\" >index</th>\n","      <th id=\"T_f6ad2_level0_col7\" class=\"col_heading level0 col7\" >result_array</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_f6ad2_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_f6ad2_row0_col0\" class=\"data row0 col0\" >Pyspark code to overwrite a DataFrame into a CSV file.The data is overwritten to file or directory MY_DIR .</td>\n","      <td id=\"T_f6ad2_row0_col1\" class=\"data row0 col1\" >df.write.mode(\"overwrite\").format(\"csv\").save(MY_DIR)</td>\n","      <td id=\"T_f6ad2_row0_col2\" class=\"data row0 col2\" >nan</td>\n","      <td id=\"T_f6ad2_row0_col3\" class=\"data row0 col3\" >Input/Output</td>\n","      <td id=\"T_f6ad2_row0_col4\" class=\"data row0 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_f6ad2_row0_col5\" class=\"data row0 col5\" >original</td>\n","      <td id=\"T_f6ad2_row0_col6\" class=\"data row0 col6\" >nan</td>\n","      <td id=\"T_f6ad2_row0_col7\" class=\"data row0 col7\" >nan</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_f6ad2_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_f6ad2_row1_col0\" class=\"data row1 col0\" >Pyspark code to Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_f6ad2_row1_col1\" class=\"data row1 col1\" >df = spark.read.csv(MY_DIR, schema=df.schema, nullValue=\"Hyukjin Kwon\")</td>\n","      <td id=\"T_f6ad2_row1_col2\" class=\"data row1 col2\" >nan</td>\n","      <td id=\"T_f6ad2_row1_col3\" class=\"data row1 col3\" >Input/Output</td>\n","      <td id=\"T_f6ad2_row1_col4\" class=\"data row1 col4\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_f6ad2_row1_col5\" class=\"data row1 col5\" >original</td>\n","      <td id=\"T_f6ad2_row1_col6\" class=\"data row1 col6\" >nan</td>\n","      <td id=\"T_f6ad2_row1_col7\" class=\"data row1 col7\" >nan</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_f6ad2_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_f6ad2_row2_col0\" class=\"data row2 col0\" >Pyspark code to overwrite a DataFrame into a JSON file.The data is overwritten to file or directory MY_DIR .</td>\n","      <td id=\"T_f6ad2_row2_col1\" class=\"data row2 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_f6ad2_row2_col2\" class=\"data row2 col2\" >nan</td>\n","      <td id=\"T_f6ad2_row2_col3\" class=\"data row2 col3\" >Input/Output</td>\n","      <td id=\"T_f6ad2_row2_col4\" class=\"data row2 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_f6ad2_row2_col5\" class=\"data row2 col5\" >original</td>\n","      <td id=\"T_f6ad2_row2_col6\" class=\"data row2 col6\" >nan</td>\n","      <td id=\"T_f6ad2_row2_col7\" class=\"data row2 col7\" >nan</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_f6ad2_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_f6ad2_row3_col0\" class=\"data row3 col0\" >Pyspark code which Specifies the input data source format.The data is overwritten to file or directory MY_DIR .</td>\n","      <td id=\"T_f6ad2_row3_col1\" class=\"data row3 col1\" >df.write.mode(\"overwrite\").format(\"json\").save(MY_DIR)</td>\n","      <td id=\"T_f6ad2_row3_col2\" class=\"data row3 col2\" >nan</td>\n","      <td id=\"T_f6ad2_row3_col3\" class=\"data row3 col3\" >Input/Output</td>\n","      <td id=\"T_f6ad2_row3_col4\" class=\"data row3 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_f6ad2_row3_col5\" class=\"data row3 col5\" >original</td>\n","      <td id=\"T_f6ad2_row3_col6\" class=\"data row3 col6\" >nan</td>\n","      <td id=\"T_f6ad2_row3_col7\" class=\"data row3 col7\" >nan</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_f6ad2_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_f6ad2_row4_col0\" class=\"data row4 col0\" >Pyspark code to Read the JSON file as a DataFrame.The data is read from file or directory MY_DIR.</td>\n","      <td id=\"T_f6ad2_row4_col1\" class=\"data row4 col1\" >df = spark.read.format('json').load(MY_DIR)</td>\n","      <td id=\"T_f6ad2_row4_col2\" class=\"data row4 col2\" >nan</td>\n","      <td id=\"T_f6ad2_row4_col3\" class=\"data row4 col3\" >Input/Output</td>\n","      <td id=\"T_f6ad2_row4_col4\" class=\"data row4 col4\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_f6ad2_row4_col5\" class=\"data row4 col5\" >original</td>\n","      <td id=\"T_f6ad2_row4_col6\" class=\"data row4 col6\" >nan</td>\n","      <td id=\"T_f6ad2_row4_col7\" class=\"data row4 col7\" >nan</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":59}],"source":["# CONCATENATE AUGMENTED DATASETS - PASS 2\n","\n","df_concat_pass_2 = pd.concat([df_concat_pass_1, df_aug_explode_my_dir, df_aug_explode_cols_lits], ignore_index=True)\n","\n","# Drop Duplicates\n","fn_display_message(f' --> Count df_concat_pass_1 : {df_concat_pass_1.shape[0]}')\n","fn_display_message(f' --> Count df_aug_explode_my_dir : {df_aug_explode_my_dir.shape[0]}')\n","fn_display_message(f' --> Count df_aug_explode_cols_lits : {df_aug_explode_cols_lits.shape[0]}')\n","\n","fn_display_message(f' --> Display Count Before dropping duplicates - {df_concat_pass_2.shape[0]}')\n","df_concat_pass_2 = df_concat_pass_2.drop_duplicates()\n","fn_display_message(f' --> Display Count After dropping duplicates - {df_concat_pass_2.shape[0]}')\n","\n","# Fix some common syntax error\n","df_concat_pass_2['code_description']  = df_concat_pass_2['code_description'].apply(lambda x: x.replace('..', '.').replace('  ', ' ').replace('. .', '.'))\n","\n","fn_display_header('Display Augmented Dataframe information: df_concat_pass_2')\n","df_concat_pass_2.info()\n","\n","fn_display_header('Display Augmented Dataframe rows: df_concat_pass_2')\n","df_concat_pass_2.head().style.set_properties(**{'text-align': 'left'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XmWyVfChNVcd"},"outputs":[],"source":["# UNCOMMENT TO DEBUG\n","#df_concat_pass_1.head(1000).style.set_properties(**{'text-align': 'left'})\n","#df_aug_explode_my_dir.head(1000).style.set_properties(**{'text-align': 'left'})\n","#df_aug_explode_cols_lits.head(1000).style.set_properties(**{'text-align': 'left'})\n","#df_concat_pass_2.head(1000).style.set_properties(**{'text-align': 'left'})"]},{"cell_type":"markdown","metadata":{"id":"EFH_FKp20WVj"},"source":["<br>\n","\n","# CREATE FINAL DATASET AND DOWNLOAD"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1721678611334,"user":{"displayName":"SK G","userId":"17874234191554613076"},"user_tz":-330},"id":"PEJMIm99NVh3","outputId":"568b36ed-3a44-451b-cfd9-a886ad3afab9"},"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           Display df_final_augmented COLUMN/COUNT Details\n","--------------------------------------------------------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 19949 entries, 0 to 19948\n","Data columns (total 5 columns):\n"," #   Column            Non-Null Count  Dtype \n","---  ------            --------------  ----- \n"," 0   code_description  19949 non-null  object\n"," 1   code_snippet      19949 non-null  object\n"," 2   import_line       1344 non-null   object\n"," 3   Category          19949 non-null  object\n"," 4   function          19949 non-null  object\n","dtypes: object(5)\n","memory usage: 779.4+ KB\n"]}],"source":["# CREATE FINAL AUGMENTED DATASET\n","\n","df_final_augmented = df_concat_pass_2[['code_description', 'code_snippet', 'import_line', 'Category', 'function']].copy()\n","\n","fn_display_header('Display df_final_augmented COLUMN/COUNT Details')\n","df_final_augmented.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"elapsed":1063,"status":"ok","timestamp":1721678627333,"user":{"displayName":"SK G","userId":"17874234191554613076"},"user_tz":-330},"id":"w1Kn_xWkNVlH","outputId":"db45a6b4-106b-4c73-ab58-07468a131f2b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_7bef03ed-b0e0-4a53-8169-9e1cffe820dc\", \"ETL_P4_data_augmentation_v1_20K.csv\", 7854672)"]},"metadata":{}}],"source":["# WRITE FINAL DATASET TO CSV AND DOWNLOAD\n","\n","DOWNLOAD_FLAG = 'Y'\n","if DOWNLOAD_FLAG == 'Y':\n","  df_final_augmented.to_csv('ETL_P4_data_augmentation_v1_20K.csv')\n","\n","  from google.colab import files\n","  files.download('ETL_P4_data_augmentation_v1_20K.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_UDI_aGfhwh1"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1z1YHvZqvJTMYgk42sOzKpl1kKOC9eIao","timestamp":1709145326525},{"file_id":"1U5EAk7nA75pZnCHAu5Yx3w7Jm9M9b-_4","timestamp":1708867975574},{"file_id":"1b4kco862abMBn8j5PEdAWKMgLBydWEGF","timestamp":1704562577409},{"file_id":"1vqp38x5aWySx5FrEC_JbzHbA3kUK6-ZK","timestamp":1703961883733},{"file_id":"1DtDZ-FBxcNSbZAxbiAE3A3gfTx9We9ki","timestamp":1703872868379}],"authorship_tag":"ABX9TyPfypteieaRpgpb2gDq2WP+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"40828875e01c4671868188f6318510fd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7e3eb0f5a2b431d9e9751cee920500d","IPY_MODEL_309272fd4888469a82b93864ca4d3b3e","IPY_MODEL_9a024194a0444461a21cb7195bb2eb6d"],"layout":"IPY_MODEL_964e2de3e856430a8d824f1f3b8bce94"}},"f7e3eb0f5a2b431d9e9751cee920500d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b0410281c6a248f6b824bdb3db29d5ed","placeholder":"","style":"IPY_MODEL_22fef2e55d1b4f598fa378e05e9d64c9","value":"config.json:100%"}},"309272fd4888469a82b93864ca4d3b3e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5deba20d4494686b6dfcee43f8dfc0c","max":1206,"min":0,"orientation":"horizontal","style":"IPY_MODEL_864dba3355394cbfad5dfb67248974a8","value":1206}},"9a024194a0444461a21cb7195bb2eb6d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b5bfe6310364aa7aea95af57adcd5bc","placeholder":"","style":"IPY_MODEL_39039d0d971548a0943aeb93b043b380","value":"1.21k/1.21k[00:00&lt;00:00,12.8kB/s]"}},"964e2de3e856430a8d824f1f3b8bce94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0410281c6a248f6b824bdb3db29d5ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22fef2e55d1b4f598fa378e05e9d64c9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5deba20d4494686b6dfcee43f8dfc0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"864dba3355394cbfad5dfb67248974a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9b5bfe6310364aa7aea95af57adcd5bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39039d0d971548a0943aeb93b043b380":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"925d89769bf54542871565c74fd3a48e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4e9b9d3ebe4a4888b42c85cb72ba253d","IPY_MODEL_cefd9d9633894a59af0cf779f0c832bf","IPY_MODEL_c4e8aa05e79447d0bf45bfafcd058248"],"layout":"IPY_MODEL_97893a742dd348e29afb2fb4a0d40398"}},"4e9b9d3ebe4a4888b42c85cb72ba253d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7cbc3d659591443babb88f893d2eddb3","placeholder":"","style":"IPY_MODEL_5ad365d7e94d474dab96835a4f3a9fff","value":"model.safetensors:100%"}},"cefd9d9633894a59af0cf779f0c832bf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f7a16b0e58d45f39248c12a85ce87de","max":242043056,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a1d28ca097da47638f8b6886100174b7","value":242043056}},"c4e8aa05e79447d0bf45bfafcd058248":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5123260de25d4030a06f832d0c7c7d58","placeholder":"","style":"IPY_MODEL_b387312bdc2943c1a3be194f798be56b","value":"242M/242M[00:02&lt;00:00,87.9MB/s]"}},"97893a742dd348e29afb2fb4a0d40398":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7cbc3d659591443babb88f893d2eddb3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ad365d7e94d474dab96835a4f3a9fff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f7a16b0e58d45f39248c12a85ce87de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a1d28ca097da47638f8b6886100174b7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5123260de25d4030a06f832d0c7c7d58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b387312bdc2943c1a3be194f798be56b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eadc957be9e641c6b8120cab8ebd3d3d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9adc4d67bb95427c878fce963c8b8a6d","IPY_MODEL_a57cdca15ed14e6eb6364261314a76b2","IPY_MODEL_67ce391fe08547eeb89b6db141fc775c"],"layout":"IPY_MODEL_d1396a2b84aa41cdad589245e99fffef"}},"9adc4d67bb95427c878fce963c8b8a6d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_446f23c078be47128bd2dc14ba25d6ac","placeholder":"","style":"IPY_MODEL_b030d6373939432481b1d9967468dd3b","value":"generation_config.json:100%"}},"a57cdca15ed14e6eb6364261314a76b2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_718a1da8808e4bd6b761d5fcf6847926","max":147,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be999b9854c6442f99699ce0ac13ecf7","value":147}},"67ce391fe08547eeb89b6db141fc775c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0e31d7bd38d45f8811dae303ac707c6","placeholder":"","style":"IPY_MODEL_053144cd1f9c4d52830ae484787ae15b","value":"147/147[00:00&lt;00:00,3.23kB/s]"}},"d1396a2b84aa41cdad589245e99fffef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"446f23c078be47128bd2dc14ba25d6ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b030d6373939432481b1d9967468dd3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"718a1da8808e4bd6b761d5fcf6847926":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be999b9854c6442f99699ce0ac13ecf7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c0e31d7bd38d45f8811dae303ac707c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"053144cd1f9c4d52830ae484787ae15b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8da5a25691da48cb880c8dd92a792ede":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6b73ec913a04f358211bcfdc5839682","IPY_MODEL_bada275ba2b2420faf40c26bbad6e60c","IPY_MODEL_c7cf6a64876a4006b4291704357844d0"],"layout":"IPY_MODEL_fb240a121fb349ce8a3425513c987d21"}},"f6b73ec913a04f358211bcfdc5839682":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fe514d52379467587acf4cb880aaed3","placeholder":"","style":"IPY_MODEL_7b5cdd3b3e5b4bc797c6664396df9363","value":"tokenizer_config.json:100%"}},"bada275ba2b2420faf40c26bbad6e60c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7eb0b150e13e49e098cc5172e678535c","max":2324,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b7921536188343b7843ab233f0b16540","value":2324}},"c7cf6a64876a4006b4291704357844d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a00bf065fb14589a39a8c19df22143e","placeholder":"","style":"IPY_MODEL_9486dda6a37f49348964bf4abeaadfb7","value":"2.32k/2.32k[00:00&lt;00:00,37.2kB/s]"}},"fb240a121fb349ce8a3425513c987d21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fe514d52379467587acf4cb880aaed3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b5cdd3b3e5b4bc797c6664396df9363":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7eb0b150e13e49e098cc5172e678535c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7921536188343b7843ab233f0b16540":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a00bf065fb14589a39a8c19df22143e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9486dda6a37f49348964bf4abeaadfb7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0a618c38c6394c53b6f95cab276d2a2a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7555c11df1b048fbb351eda4564e6186","IPY_MODEL_cab4c4fa100442a6a3c7a48775c82e82","IPY_MODEL_bca5b7b988084e9d8f48acaf92cb7a4d"],"layout":"IPY_MODEL_37dde5b1795c465db8277e8bfbeef074"}},"7555c11df1b048fbb351eda4564e6186":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10c0c9531986464aa1cc966e930cd05a","placeholder":"","style":"IPY_MODEL_3f1e5d6329454f25bdf58aebb9686c8e","value":"spiece.model:100%"}},"cab4c4fa100442a6a3c7a48775c82e82":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb3693713eb7426dba1d3540ab0f7382","max":791656,"min":0,"orientation":"horizontal","style":"IPY_MODEL_410fd5b29dc1456b82c0ee9c0c9568e8","value":791656}},"bca5b7b988084e9d8f48acaf92cb7a4d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03c25f7ae75a4b7692aeb017fcd7b970","placeholder":"","style":"IPY_MODEL_891c82bb0e674ff9b05f5bec35a2b9b4","value":"792k/792k[00:00&lt;00:00,3.86MB/s]"}},"37dde5b1795c465db8277e8bfbeef074":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10c0c9531986464aa1cc966e930cd05a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f1e5d6329454f25bdf58aebb9686c8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb3693713eb7426dba1d3540ab0f7382":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"410fd5b29dc1456b82c0ee9c0c9568e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"03c25f7ae75a4b7692aeb017fcd7b970":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"891c82bb0e674ff9b05f5bec35a2b9b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ca1dbb4a207b4b8a8e19219db8023b1d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a1649040d6d1483eb2f3b639c91d54df","IPY_MODEL_2c4db98f4ef047e694ade3efb11c171c","IPY_MODEL_280df210ef5c4834997675f3fa1446f1"],"layout":"IPY_MODEL_c5b99384e2e04facb80b867d4ab40fbf"}},"a1649040d6d1483eb2f3b639c91d54df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e6d662c36dca467c8f36d7e460a4355f","placeholder":"","style":"IPY_MODEL_04d513b8dcf64c3c9eb313eb89c3b7ad","value":"tokenizer.json:100%"}},"2c4db98f4ef047e694ade3efb11c171c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0208c990b3444367ba4e5b7295166778","max":1389353,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5bdba1f73f0e468d80afa43a8c47e9d2","value":1389353}},"280df210ef5c4834997675f3fa1446f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0267f6b843b9477fb271392981689d23","placeholder":"","style":"IPY_MODEL_69579fade96846c5be6e89ae0bfd3c71","value":"1.39M/1.39M[00:00&lt;00:00,14.7MB/s]"}},"c5b99384e2e04facb80b867d4ab40fbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6d662c36dca467c8f36d7e460a4355f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04d513b8dcf64c3c9eb313eb89c3b7ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0208c990b3444367ba4e5b7295166778":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bdba1f73f0e468d80afa43a8c47e9d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0267f6b843b9477fb271392981689d23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69579fade96846c5be6e89ae0bfd3c71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}