{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1vqp38x5aWySx5FrEC_JbzHbA3kUK6-ZK","timestamp":1703961883733},{"file_id":"1DtDZ-FBxcNSbZAxbiAE3A3gfTx9We9ki","timestamp":1703872868379}],"authorship_tag":"ABX9TyPJ+BipJCzolhRbQJ/nuBSw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"aad136b8f67d4431833efca8719b1f42":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_78c7070fdc3c4784baf3b72e3fe59122","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"Processing... \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:11\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Processing... <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:00:11</span>\n</pre>\n"},"metadata":{}}]}},"78c7070fdc3c4784baf3b72e3fe59122":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9df24f2b63e24fa0a83340fbb9b1c694":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_c6047263e3d945259281c240bcc370b0","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"Processing... \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:04:53\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Processing... <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:04:53</span>\n</pre>\n"},"metadata":{}}]}},"c6047263e3d945259281c240bcc370b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a13830b1487d4ad49d90369ee874db6d":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_e4154cc21c454bb6806c692803128496","msg_id":"","outputs":[{"output_type":"display_data","data":{"text/plain":"Processing... \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:02:53\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Processing... <span style=\"color: #729c1f; text-decoration-color: #729c1f\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span> <span style=\"color: #800080; text-decoration-color: #800080\">100%</span> <span style=\"color: #808000; text-decoration-color: #808000\">0:02:53</span>\n</pre>\n"},"metadata":{}}]}},"e4154cc21c454bb6806c692803128496":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"cells":[{"cell_type":"markdown","source":["# WEB SCRAPE PYSPARK DOCUMENTATION\n","\n","----------------------------\n","#### GIST OF CHANGES DONE IN THIS NOTEBOOK\n","----------------------------\n","-   Pyspark documentation for Version 3.5.0 was used for this code\n","-   The code initially webscrapes the hyper links available in the landing page for pyspark.sql functions and collects the category and function names along with the hyper links\n","-   In the next step, the code scrapes the individual hyperlinks and using desired html tags, extracts the code snippets and the corresponding code descriptions\n","\n","NOTE: At the time of executing this code the latest URL was pointing to version 3.5.0\n","-   LATEST URL: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/\n","-   BASE URL  : https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/\n","\n","----------------------------\n","<br>\n","<br>"],"metadata":{"id":"jdYDazNza3x0"}},{"cell_type":"code","source":["# INSTALL NECESSARY PACKAGES\n","!pip install bs4 html5lib"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wlDwKcQLbagJ","executionInfo":{"status":"ok","timestamp":1710276797463,"user_tz":-330,"elapsed":9109,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"fd7b3d31-0e1d-4bda-b425-938c25a61495"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bs4\n","  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n","Requirement already satisfied: html5lib in /usr/local/lib/python3.10/dist-packages (1.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib) (1.16.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib) (0.5.1)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n","Installing collected packages: bs4\n","Successfully installed bs4-0.0.2\n"]}]},{"cell_type":"code","source":["# IMPORT NECESSARY PACKAGES\n","import re\n","import pandas as pd\n","import requests\n","import html5lib\n","from rich.progress import track\n","from multiprocessing import Pool\n","from bs4 import BeautifulSoup, SoupStrainer\n"],"metadata":{"id":"OfPB61Gz8VfY","executionInfo":{"status":"ok","timestamp":1710276799347,"user_tz":-330,"elapsed":1904,"user":{"displayName":"SK G","userId":"17874234191554613076"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# DISPLAY FUNCTIONS\n","def fn_display_header(msg):\n","  print('-' * 80)\n","  print(' ' * 10, msg)\n","  print('-' * 80)\n","\n","def fn_display_message(msg):\n","  print(msg)"],"metadata":{"id":"pEiTi6z_8rTF","executionInfo":{"status":"ok","timestamp":1710276799348,"user_tz":-330,"elapsed":16,"user":{"displayName":"SK G","userId":"17874234191554613076"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["aad136b8f67d4431833efca8719b1f42","78c7070fdc3c4784baf3b72e3fe59122"]},"id":"D7DzrtMqawGJ","executionInfo":{"status":"ok","timestamp":1710276908151,"user_tz":-330,"elapsed":12385,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"af0f5116-01eb-40e3-a5ad-c9a09ea5dc4e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aad136b8f67d4431833efca8719b1f42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           Display Initial Pyspark Links/Categories to Scrape\n","--------------------------------------------------------------------------------\n"," - Link Count: 712\n","--------------------------------------------------------------------------------\n","{'Category': 'Configuration', 'Function': 'pyspark.sql.conf.RuntimeConfig', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.conf.RuntimeConfig.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameReader.csv', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameReader.format', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.format.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameReader.jdbc', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.jdbc.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameReader.json', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameReader.load', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.load.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameReader.option', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.option.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameReader.options', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.options.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameReader.orc', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.orc.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameReader.parquet', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameReader.schema', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.schema.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameReader.table', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.table.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameReader.text', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.text.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.bucketBy', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.bucketBy.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.csv', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.csv.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.format', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.format.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.insertInto', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.insertInto.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.jdbc', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.jdbc.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.json', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.json.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.mode', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.mode.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.option', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.option.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.options', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.options.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.orc', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.orc.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.parquet', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.parquet.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.partitionBy', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.partitionBy.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.save', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.save.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.saveAsTable', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.saveAsTable.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.sortBy', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.sortBy.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriter.text', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriter.text.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriterV2.using', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriterV2.using.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriterV2.option', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriterV2.option.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriterV2.options', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriterV2.options.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriterV2.tableProperty', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriterV2.tableProperty.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriterV2.partitionedBy', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriterV2.partitionedBy.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriterV2.create', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriterV2.create.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriterV2.replace', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriterV2.replace.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriterV2.createOrReplace', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriterV2.createOrReplace.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriterV2.append', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriterV2.append.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriterV2.overwrite', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriterV2.overwrite.html'}\n","{'Category': 'Input/Output', 'Function': 'pyspark.sql.DataFrameWriterV2.overwritePartitions', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameWriterV2.overwritePartitions.html'}\n","{'Category': 'DataFrame', 'Function': 'pyspark.sql.DataFrame.__getattr__', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.__getattr__.html'}\n","{'Category': 'DataFrame', 'Function': 'pyspark.sql.DataFrame.__getitem__', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.__getitem__.html'}\n","{'Category': 'DataFrame', 'Function': 'pyspark.sql.DataFrame.agg', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.agg.html'}\n","{'Category': 'DataFrame', 'Function': 'pyspark.sql.DataFrame.alias', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.alias.html'}\n","{'Category': 'DataFrame', 'Function': 'pyspark.sql.DataFrame.approxQuantile', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.approxQuantile.html'}\n","{'Category': 'DataFrame', 'Function': 'pyspark.sql.DataFrame.cache', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.cache.html'}\n","{'Category': 'DataFrame', 'Function': 'pyspark.sql.DataFrame.checkpoint', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.checkpoint.html'}\n","{'Category': 'DataFrame', 'Function': 'pyspark.sql.DataFrame.coalesce', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.coalesce.html'}\n","{'Category': 'DataFrame', 'Function': 'pyspark.sql.DataFrame.colRegex', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.colRegex.html'}\n","{'Category': 'DataFrame', 'Function': 'pyspark.sql.DataFrame.collect', 'Link': 'https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.collect.html'}\n"]}],"source":["from scipy.special import tandg\n","# DEFINE BASE URL TO START THE WEB SCRAPING\n","# NOTE: At the time of executing this code the latest URL was pointing to version 3.5.0\n","#       LATEST URL: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/\n","#       BASE URL  : https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/\n","\n","\n","base_url = \"https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/\"\n","index_url = base_url + \"index.html\"\n","pd.set_option('display.max_colwidth', None)\n","\n","excluded_categories = ['Core Classes', 'Spark Session']\n","sublink_categories = ['Functions']\n","\n","response = requests.get(index_url)\n","soup = BeautifulSoup(response.text, 'html.parser', parse_only=SoupStrainer(['div', 'li', 'a']))  # Use parse_only to reduce the list of tags to be searched\n","\n","# -- Find Parent elements and sub-elements to obtain category and link\n","links= []\n","find_tags = soup.find('div', class_='section', id='spark-sql').find_all('li', class_='toctree-l1')\n","for tag_l1 in track(find_tags, description=\"Processing...\"):\n","  category = tag_l1.find('a').get_text()\n","  for tag_l2 in tag_l1.find_all('li'):\n","    function = tag_l2.get_text()\n","    link = base_url + tag_l2.find('a').get('href')\n","    if category in sublink_categories:                 # -- Find sublinks for relevant categories and update the parent link\n","      parent_link = link.split('#')[0]\n","      section_id = link.split('#')[1]\n","\n","      response_sublink = requests.get(parent_link).text\n","      soup_sublink = BeautifulSoup(response_sublink, 'html.parser', parse_only=SoupStrainer(['div', 'li', 'a']))\n","      find_sublink_tags = soup_sublink.find('div', class_='section', id=section_id).find_all('a', class_='reference internal')\n","\n","      new_category = function\n","      for sublink_tag in find_sublink_tags:\n","        link = base_url + sublink_tag.get('href')\n","        function = link.split('#')[1]\n","        if category not in excluded_categories:\n","          links.append({'Category': new_category, 'Function': function, 'Link': link})\n","    elif category not in excluded_categories:\n","      links.append({'Category': category, 'Function': function, 'Link': link})\n","\n","#print(f\" -- {link}\")\n","#print(f\" -- {find_sublink_tags}\")\n","\n","\n","fn_display_header(\"Display Initial Pyspark Links/Categories to Scrape\")\n","fn_display_message(f\" - Link Count: {len(links)}\")\n","fn_display_message(\"-\" * 80)\n","\n","rows_to_print = 50\n","\n","for item in links[:rows_to_print]:\n","  print(item)\n"]},{"cell_type":"code","source":["# PRINT DISTINCT CATEGORIES\n","fn_display_header(\"Display Initial Links Info\")\n","\n","relevant_category_list_1 = set([item['Category'] for item in links])\n","fn_display_message(f\" - Unique Categories: {relevant_category_list_1}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xmDHb0m2MEOH","executionInfo":{"status":"ok","timestamp":1710276914449,"user_tz":-330,"elapsed":654,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"b97ef27a-b74e-4e34-f892-879ef4d3f431"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           Display Initial Links Info\n","--------------------------------------------------------------------------------\n"," - Unique Categories: {'Aggregate Functions', 'Call Functions', 'Configuration', 'UDF', 'Window', 'Datetime Functions', 'Partition Transformation Functions', 'Input/Output', 'Math Functions', 'DataFrame', 'Misc Functions', 'Catalog', 'Protobuf', 'String Functions', 'UDTF', 'Row', 'Predicate Functions', 'Data Types', 'Grouping', 'Observation', 'Window Functions', 'Normal Functions', 'Xml Functions', 'Collection Functions', 'Column', 'Bitwise Functions', 'Avro', 'Sort Functions'}\n"]}]},{"cell_type":"code","source":["# SCRAPE INDIVIDUAL LINK CONTENT  - PASS 1\n","# Reminder to handle nulls - returntype\n","\n","function_dataset_list_1 = []\n","#relevant_category_list_1 = ['Functions']     # Override to test one specific category\n","\n","relevant_links1 = [item for item in links if item[\"Category\"] in relevant_category_list_1]\n","\n","trim_text = lambda text : text.replace('\\n', '').strip() if text else ''    # Remove newlines and trims\n","keep_only_alphanum = lambda text : re.sub('[^a-zA-Z0-9. ]', ' ', text) if text else '' # Remove special characters\n","remove_special_char = lambda text : \"\".join(char for char in text if ord(char)<128)   # Remove special characters\n","\n","with Pool(processes=2) as pool:\n","  for item in track(relevant_links1, description=\"Processing...\"):\n","    category = item[\"Category\"]\n","\n","    response = requests.get(item[\"Link\"])\n","    soup = BeautifulSoup(\n","        response.text,\n","        'lxml',                                         # lxml is faster than 'html.parser'\n","        parse_only=SoupStrainer(['h1', 'dd', 'p']))     # Use parse_only to reduce the list of tags to be searched\n","\n","    feature = item[\"Function\"]\n","\n","    feature_desc = ''\n","    find_desc_texts = soup.find('dd').find_all('p');\n","\n","    for text in find_desc_texts:\n","      if text.find('span', class_='versionmodified'):\n","        break\n","      else:\n","        feature_desc = feature_desc + ' ' + text.get_text()\n","\n","    feature_desc = trim_text(feature_desc)\n","    feature_desc = keep_only_alphanum(feature_desc)\n","    feature_desc = remove_special_char(feature_desc)\n","    function_dataset_list_1.append({'Category': category, 'function': feature, 'feature_desc': feature_desc})\n","\n","# -- Create a dataframe of the initial dataset\n","df_function_pass_1 = pd.DataFrame(function_dataset_list_1)\n","\n","fn_display_header(f\"PASS 1: Get and refine primary function description. \\nRelevant Category List: {relevant_category_list_1}\")\n","fn_display_message(f\" - function_dataset_list_1 Count: {len(function_dataset_list_1)}\")\n","fn_display_message(\"-\" * 80)\n","df_function_pass_1.head().style.set_properties(**{'text-align': 'left'})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406,"referenced_widgets":["9df24f2b63e24fa0a83340fbb9b1c694","c6047263e3d945259281c240bcc370b0"]},"id":"RgRPILnlqGBO","executionInfo":{"status":"ok","timestamp":1710277234831,"user_tz":-330,"elapsed":294414,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"outputId":"ded4c56f-266f-4509-9510-e44eb9a0480f"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9df24f2b63e24fa0a83340fbb9b1c694"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           PASS 1: Get and refine primary function description. \n","Relevant Category List: {'Aggregate Functions', 'Call Functions', 'Configuration', 'UDF', 'Window', 'Datetime Functions', 'Partition Transformation Functions', 'Input/Output', 'Math Functions', 'DataFrame', 'Misc Functions', 'Catalog', 'Protobuf', 'String Functions', 'UDTF', 'Row', 'Predicate Functions', 'Data Types', 'Grouping', 'Observation', 'Window Functions', 'Normal Functions', 'Xml Functions', 'Collection Functions', 'Column', 'Bitwise Functions', 'Avro', 'Sort Functions'}\n","--------------------------------------------------------------------------------\n"," - function_dataset_list_1 Count: 712\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7833ab1fca90>"],"text/html":["<style type=\"text/css\">\n","#T_c3322_row0_col0, #T_c3322_row0_col1, #T_c3322_row0_col2, #T_c3322_row1_col0, #T_c3322_row1_col1, #T_c3322_row1_col2, #T_c3322_row2_col0, #T_c3322_row2_col1, #T_c3322_row2_col2, #T_c3322_row3_col0, #T_c3322_row3_col1, #T_c3322_row3_col2, #T_c3322_row4_col0, #T_c3322_row4_col1, #T_c3322_row4_col2 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_c3322\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_c3322_level0_col0\" class=\"col_heading level0 col0\" >Category</th>\n","      <th id=\"T_c3322_level0_col1\" class=\"col_heading level0 col1\" >function</th>\n","      <th id=\"T_c3322_level0_col2\" class=\"col_heading level0 col2\" >feature_desc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_c3322_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_c3322_row0_col0\" class=\"data row0 col0\" >Configuration</td>\n","      <td id=\"T_c3322_row0_col1\" class=\"data row0 col1\" >pyspark.sql.conf.RuntimeConfig</td>\n","      <td id=\"T_c3322_row0_col2\" class=\"data row0 col2\" >User facing configuration API  accessible through SparkSession.conf. Options set here are automatically propagated to the Hadoop configuration during I O.</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_c3322_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_c3322_row1_col0\" class=\"data row1 col0\" >Input/Output</td>\n","      <td id=\"T_c3322_row1_col1\" class=\"data row1 col1\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_c3322_row1_col2\" class=\"data row1 col2\" >Loads a CSV file and returns the result as a  DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once  disableinferSchema option or specify the schema explicitly using schema.</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_c3322_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_c3322_row2_col0\" class=\"data row2 col0\" >Input/Output</td>\n","      <td id=\"T_c3322_row2_col1\" class=\"data row2 col1\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_c3322_row2_col2\" class=\"data row2 col2\" >Specifies the input data source format.</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_c3322_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_c3322_row3_col0\" class=\"data row3 col0\" >Input/Output</td>\n","      <td id=\"T_c3322_row3_col1\" class=\"data row3 col1\" >pyspark.sql.DataFrameReader.jdbc</td>\n","      <td id=\"T_c3322_row3_col2\" class=\"data row3 col2\" >Construct a DataFrame representing the database table named tableaccessible via JDBC URL url and connection properties. Partitions of the table will be retrieved in parallel if either column orpredicates is specified. lowerBound  upperBound and numPartitionsis needed when column is specified. If both column and predicates are specified  column will be used.</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_c3322_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_c3322_row4_col0\" class=\"data row4 col0\" >Input/Output</td>\n","      <td id=\"T_c3322_row4_col1\" class=\"data row4 col1\" >pyspark.sql.DataFrameReader.json</td>\n","      <td id=\"T_c3322_row4_col2\" class=\"data row4 col2\" >Loads JSON files and returns the results as a DataFrame. JSON Lines  newline delimited JSON  is supported by default.For JSON  one record per file   set the multiLine parameter to true. If the schema parameter is not specified  this function goesthrough the input once to determine the input schema.</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# FOR DEBUGGING\n","# for item in function_dataset_list_1:\n","#  print(item)\n"],"metadata":{"id":"desif0RpqGKK","executionInfo":{"status":"ok","timestamp":1710277234832,"user_tz":-330,"elapsed":62,"user":{"displayName":"SK G","userId":"17874234191554613076"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# SCRAPE INDIVIDUAL LINK CONTENT  - PASS 2\n","# Reminder to handle nulls - returntype\n","\n","function_dataset_list_2 = []\n","code_dataset_list_2 = []\n","#relevant_category_list_1 = ['DataFrame']     # Uncomment to test one specific category. Post testing Comment and rerun the notebook\n","\n","relevant_links1 = [item for item in links if item[\"Category\"] in relevant_category_list_1]\n","\n","with Pool(processes=2) as pool:\n","  for item in track(relevant_links1, description=\"Processing...\"):\n","    category = item[\"Category\"]\n","\n","    response = requests.get(item[\"Link\"])\n","    soup = BeautifulSoup(\n","        response.text,\n","        'lxml',                                         # lxml is faster than 'html.parser'\n","        parse_only=SoupStrainer(['pre', 'p']))     # Use parse_only to reduce the list of tags to be searched\n","\n","    feature = item[\"Function\"]\n","\n","    example_text = ''\n","    find_example_texts = soup.find_all(['pre', 'p']);\n","\n","    for text in find_example_texts:\n","      text_mod = text.get_text()\n","      if not text_mod.startswith(('>', '.', ' ', '+', '|')) and not text_mod.strip() == '':\n","        text_mod = '# ' + text_mod\n","      text_mod = text_mod.replace(' >>>', '<br>>>>')\n","      example_text = example_text + ' ' + text_mod.replace('\\n', '<br>')\n","      #print(f'---->{text_mod}')\n","\n","    example_text = re.sub(r'^.*?Examples', '', example_text)\n","    example_text = re.sub(r'# previous.*$', '', example_text)\n","    example_text = example_text.replace(' >>>', '<br>>>>')\n","    function_dataset_list_2.append({'Category': category, 'function': feature, 'example_text': example_text, 'link': item[\"Link\"]})\n","\n","\n","# -- Create a dataframe of the dataset with example text\n","df_function_pass_2 = pd.DataFrame(function_dataset_list_2)\n","\n","fn_display_header(f\"PASS 2: Get and refine primary function description. \\nRelevant Category List: {relevant_category_list_1}\")\n","fn_display_message(f\" - function_dataset_list_2 Count: {len(function_dataset_list_2)}\")\n","fn_display_message(\"-\" * 80)\n","df_function_pass_2.head().style.set_properties(**{'text-align': 'left'})"],"metadata":{"id":"vULd9DJVqGM2","executionInfo":{"status":"ok","timestamp":1710277408586,"user_tz":-330,"elapsed":173812,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a13830b1487d4ad49d90369ee874db6d","e4154cc21c454bb6806c692803128496"]},"outputId":"27044381-1823-4233-e3df-da3ebaad9d98"},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a13830b1487d4ad49d90369ee874db6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":[],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","           PASS 2: Get and refine primary function description. \n","Relevant Category List: {'Aggregate Functions', 'Call Functions', 'Configuration', 'UDF', 'Window', 'Datetime Functions', 'Partition Transformation Functions', 'Input/Output', 'Math Functions', 'DataFrame', 'Misc Functions', 'Catalog', 'Protobuf', 'String Functions', 'UDTF', 'Row', 'Predicate Functions', 'Data Types', 'Grouping', 'Observation', 'Window Functions', 'Normal Functions', 'Xml Functions', 'Collection Functions', 'Column', 'Bitwise Functions', 'Avro', 'Sort Functions'}\n","--------------------------------------------------------------------------------\n"," - function_dataset_list_2 Count: 712\n","--------------------------------------------------------------------------------\n"]},{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7833aaffc700>"],"text/html":["<style type=\"text/css\">\n","#T_27c8a_row0_col0, #T_27c8a_row0_col1, #T_27c8a_row0_col2, #T_27c8a_row0_col3, #T_27c8a_row1_col0, #T_27c8a_row1_col1, #T_27c8a_row1_col2, #T_27c8a_row1_col3, #T_27c8a_row2_col0, #T_27c8a_row2_col1, #T_27c8a_row2_col2, #T_27c8a_row2_col3, #T_27c8a_row3_col0, #T_27c8a_row3_col1, #T_27c8a_row3_col2, #T_27c8a_row3_col3, #T_27c8a_row4_col0, #T_27c8a_row4_col1, #T_27c8a_row4_col2, #T_27c8a_row4_col3 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_27c8a\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_27c8a_level0_col0\" class=\"col_heading level0 col0\" >Category</th>\n","      <th id=\"T_27c8a_level0_col1\" class=\"col_heading level0 col1\" >function</th>\n","      <th id=\"T_27c8a_level0_col2\" class=\"col_heading level0 col2\" >example_text</th>\n","      <th id=\"T_27c8a_level0_col3\" class=\"col_heading level0 col3\" >link</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_27c8a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_27c8a_row0_col0\" class=\"data row0 col0\" >Configuration</td>\n","      <td id=\"T_27c8a_row0_col1\" class=\"data row0 col1\" >pyspark.sql.conf.RuntimeConfig</td>\n","      <td id=\"T_27c8a_row0_col2\" class=\"data row0 col2\" > # User-facing configuration API, accessible through SparkSession.conf. # Options set here are automatically propagated to the Hadoop configuration during I/O. # Changed in version 3.4.0: Supports Spark Connect. # Methods # get(key[,Â default]) # Returns the value of Spark runtime configuration property for the given key, assuming it is set. # isModifiable(key) # Indicates whether the configuration property with the given key is modifiable in the current session. # set(key,Â value) # Sets the given Spark runtime configuration property. # unset(key) # Resets the configuration property for the given key. </td>\n","      <td id=\"T_27c8a_row0_col3\" class=\"data row0 col3\" >https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.conf.RuntimeConfig.html</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_27c8a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_27c8a_row1_col0\" class=\"data row1 col0\" >Input/Output</td>\n","      <td id=\"T_27c8a_row1_col1\" class=\"data row1 col1\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_27c8a_row1_col2\" class=\"data row1 col2\" > # Write a DataFrame into a CSV file and read it back.<br>>>> import tempfile<br>>>> with tempfile.TemporaryDirectory() as d:<br>...     # Write a DataFrame into a CSV file<br>...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])<br>...     df.write.mode(\"overwrite\").format(\"csv\").save(d)<br>...<br>...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.<br>...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()<br>+---+----+<br>|age|name|<br>+---+----+<br>|100|NULL|<br>+---+----+<br> </td>\n","      <td id=\"T_27c8a_row1_col3\" class=\"data row1 col3\" >https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_27c8a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_27c8a_row2_col0\" class=\"data row2 col0\" >Input/Output</td>\n","      <td id=\"T_27c8a_row2_col1\" class=\"data row2 col1\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_27c8a_row2_col2\" class=\"data row2 col2\" ><br>>>> spark.read.format('json')<br><...readwriter.DataFrameReader object ...><br> # Write a DataFrame into a JSON file and read it back.<br>>>> import tempfile<br>>>> with tempfile.TemporaryDirectory() as d:<br>...     # Write a DataFrame into a JSON file<br>...     spark.createDataFrame(<br>...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]<br>...     ).write.mode(\"overwrite\").format(\"json\").save(d)<br>...<br>...     # Read the JSON file as a DataFrame.<br>...     spark.read.format('json').load(d).show()<br>+---+------------+<br>|age|        name|<br>+---+------------+<br>|100|Hyukjin Kwon|<br>+---+------------+<br> </td>\n","      <td id=\"T_27c8a_row2_col3\" class=\"data row2 col3\" >https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.format.html</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_27c8a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_27c8a_row3_col0\" class=\"data row3 col0\" >Input/Output</td>\n","      <td id=\"T_27c8a_row3_col1\" class=\"data row3 col1\" >pyspark.sql.DataFrameReader.jdbc</td>\n","      <td id=\"T_27c8a_row3_col2\" class=\"data row3 col2\" > # Construct a DataFrame representing the database table named table<br>accessible via JDBC URL url and connection properties. # Partitions of the table will be retrieved in parallel if either column or<br>predicates is specified. lowerBound, upperBound and numPartitions<br>is needed when column is specified. # If both column and predicates are specified, column will be used. # New in version 1.4.0. # Changed in version 3.4.0: Supports Spark Connect. # the name of the table # alias of partitionColumn option. Refer to partitionColumn in<br>Data Source Option<br>for the version you use. # a list of expressions suitable for inclusion in WHERE clauses;<br>each one defines one partition of the DataFrame # a dictionary of JDBC database connection arguments. Normally at<br>least properties âuserâ and âpasswordâ with their corresponding values.<br>For example { âuserâ : âSYSTEMâ, âpasswordâ : âmypasswordâ } # For the extra options, refer to<br>Data Source Option<br>for the version you use. # Notes # Donât create too many partitions in parallel on a large cluster;<br>otherwise Spark might crash your external database systems. </td>\n","      <td id=\"T_27c8a_row3_col3\" class=\"data row3 col3\" >https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.jdbc.html</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_27c8a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_27c8a_row4_col0\" class=\"data row4 col0\" >Input/Output</td>\n","      <td id=\"T_27c8a_row4_col1\" class=\"data row4 col1\" >pyspark.sql.DataFrameReader.json</td>\n","      <td id=\"T_27c8a_row4_col2\" class=\"data row4 col2\" > # Write a DataFrame into a JSON file and read it back.<br>>>> import tempfile<br>>>> with tempfile.TemporaryDirectory() as d:<br>...     # Write a DataFrame into a JSON file<br>...     spark.createDataFrame(<br>...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]<br>...     ).write.mode(\"overwrite\").format(\"json\").save(d)<br>...<br>...     # Read the JSON file as a DataFrame.<br>...     spark.read.json(d).show()<br>+---+------------+<br>|age|        name|<br>+---+------------+<br>|100|Hyukjin Kwon|<br>+---+------------+<br> </td>\n","      <td id=\"T_27c8a_row4_col3\" class=\"data row4 col3\" >https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# MERGE SCRAPED DATA and SAVE TO CSV\n","\n","df_pyspark_raw = pd.merge(df_function_pass_1, df_function_pass_2, on=['Category', 'function'])\n","\n","df_pyspark_raw.head().style.set_properties(**{'text-align': 'left'})"],"metadata":{"id":"SMIqo-_VqGO6","executionInfo":{"status":"ok","timestamp":1710277412462,"user_tz":-330,"elapsed":1345,"user":{"displayName":"SK G","userId":"17874234191554613076"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"136d4e5b-6510-4c0a-844d-1572cc31cac9"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pandas.io.formats.style.Styler at 0x7833aaffc9d0>"],"text/html":["<style type=\"text/css\">\n","#T_cb50a_row0_col0, #T_cb50a_row0_col1, #T_cb50a_row0_col2, #T_cb50a_row0_col3, #T_cb50a_row0_col4, #T_cb50a_row1_col0, #T_cb50a_row1_col1, #T_cb50a_row1_col2, #T_cb50a_row1_col3, #T_cb50a_row1_col4, #T_cb50a_row2_col0, #T_cb50a_row2_col1, #T_cb50a_row2_col2, #T_cb50a_row2_col3, #T_cb50a_row2_col4, #T_cb50a_row3_col0, #T_cb50a_row3_col1, #T_cb50a_row3_col2, #T_cb50a_row3_col3, #T_cb50a_row3_col4, #T_cb50a_row4_col0, #T_cb50a_row4_col1, #T_cb50a_row4_col2, #T_cb50a_row4_col3, #T_cb50a_row4_col4 {\n","  text-align: left;\n","}\n","</style>\n","<table id=\"T_cb50a\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th class=\"blank level0\" >&nbsp;</th>\n","      <th id=\"T_cb50a_level0_col0\" class=\"col_heading level0 col0\" >Category</th>\n","      <th id=\"T_cb50a_level0_col1\" class=\"col_heading level0 col1\" >function</th>\n","      <th id=\"T_cb50a_level0_col2\" class=\"col_heading level0 col2\" >feature_desc</th>\n","      <th id=\"T_cb50a_level0_col3\" class=\"col_heading level0 col3\" >example_text</th>\n","      <th id=\"T_cb50a_level0_col4\" class=\"col_heading level0 col4\" >link</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th id=\"T_cb50a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n","      <td id=\"T_cb50a_row0_col0\" class=\"data row0 col0\" >Configuration</td>\n","      <td id=\"T_cb50a_row0_col1\" class=\"data row0 col1\" >pyspark.sql.conf.RuntimeConfig</td>\n","      <td id=\"T_cb50a_row0_col2\" class=\"data row0 col2\" >User facing configuration API  accessible through SparkSession.conf. Options set here are automatically propagated to the Hadoop configuration during I O.</td>\n","      <td id=\"T_cb50a_row0_col3\" class=\"data row0 col3\" > # User-facing configuration API, accessible through SparkSession.conf. # Options set here are automatically propagated to the Hadoop configuration during I/O. # Changed in version 3.4.0: Supports Spark Connect. # Methods # get(key[,Â default]) # Returns the value of Spark runtime configuration property for the given key, assuming it is set. # isModifiable(key) # Indicates whether the configuration property with the given key is modifiable in the current session. # set(key,Â value) # Sets the given Spark runtime configuration property. # unset(key) # Resets the configuration property for the given key. </td>\n","      <td id=\"T_cb50a_row0_col4\" class=\"data row0 col4\" >https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.conf.RuntimeConfig.html</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cb50a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n","      <td id=\"T_cb50a_row1_col0\" class=\"data row1 col0\" >Input/Output</td>\n","      <td id=\"T_cb50a_row1_col1\" class=\"data row1 col1\" >pyspark.sql.DataFrameReader.csv</td>\n","      <td id=\"T_cb50a_row1_col2\" class=\"data row1 col2\" >Loads a CSV file and returns the result as a  DataFrame. This function will go through the input once to determine the input schema ifinferSchema is enabled. To avoid going through the entire data once  disableinferSchema option or specify the schema explicitly using schema.</td>\n","      <td id=\"T_cb50a_row1_col3\" class=\"data row1 col3\" > # Write a DataFrame into a CSV file and read it back.<br>>>> import tempfile<br>>>> with tempfile.TemporaryDirectory() as d:<br>...     # Write a DataFrame into a CSV file<br>...     df = spark.createDataFrame([{\"age\": 100, \"name\": \"Hyukjin Kwon\"}])<br>...     df.write.mode(\"overwrite\").format(\"csv\").save(d)<br>...<br>...     # Read the CSV file as a DataFrame with 'nullValue' option set to 'Hyukjin Kwon'.<br>...     spark.read.csv(d, schema=df.schema, nullValue=\"Hyukjin Kwon\").show()<br>+---+----+<br>|age|name|<br>+---+----+<br>|100|NULL|<br>+---+----+<br> </td>\n","      <td id=\"T_cb50a_row1_col4\" class=\"data row1 col4\" >https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cb50a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n","      <td id=\"T_cb50a_row2_col0\" class=\"data row2 col0\" >Input/Output</td>\n","      <td id=\"T_cb50a_row2_col1\" class=\"data row2 col1\" >pyspark.sql.DataFrameReader.format</td>\n","      <td id=\"T_cb50a_row2_col2\" class=\"data row2 col2\" >Specifies the input data source format.</td>\n","      <td id=\"T_cb50a_row2_col3\" class=\"data row2 col3\" ><br>>>> spark.read.format('json')<br><...readwriter.DataFrameReader object ...><br> # Write a DataFrame into a JSON file and read it back.<br>>>> import tempfile<br>>>> with tempfile.TemporaryDirectory() as d:<br>...     # Write a DataFrame into a JSON file<br>...     spark.createDataFrame(<br>...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]<br>...     ).write.mode(\"overwrite\").format(\"json\").save(d)<br>...<br>...     # Read the JSON file as a DataFrame.<br>...     spark.read.format('json').load(d).show()<br>+---+------------+<br>|age|        name|<br>+---+------------+<br>|100|Hyukjin Kwon|<br>+---+------------+<br> </td>\n","      <td id=\"T_cb50a_row2_col4\" class=\"data row2 col4\" >https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.format.html</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cb50a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n","      <td id=\"T_cb50a_row3_col0\" class=\"data row3 col0\" >Input/Output</td>\n","      <td id=\"T_cb50a_row3_col1\" class=\"data row3 col1\" >pyspark.sql.DataFrameReader.jdbc</td>\n","      <td id=\"T_cb50a_row3_col2\" class=\"data row3 col2\" >Construct a DataFrame representing the database table named tableaccessible via JDBC URL url and connection properties. Partitions of the table will be retrieved in parallel if either column orpredicates is specified. lowerBound  upperBound and numPartitionsis needed when column is specified. If both column and predicates are specified  column will be used.</td>\n","      <td id=\"T_cb50a_row3_col3\" class=\"data row3 col3\" > # Construct a DataFrame representing the database table named table<br>accessible via JDBC URL url and connection properties. # Partitions of the table will be retrieved in parallel if either column or<br>predicates is specified. lowerBound, upperBound and numPartitions<br>is needed when column is specified. # If both column and predicates are specified, column will be used. # New in version 1.4.0. # Changed in version 3.4.0: Supports Spark Connect. # the name of the table # alias of partitionColumn option. Refer to partitionColumn in<br>Data Source Option<br>for the version you use. # a list of expressions suitable for inclusion in WHERE clauses;<br>each one defines one partition of the DataFrame # a dictionary of JDBC database connection arguments. Normally at<br>least properties âuserâ and âpasswordâ with their corresponding values.<br>For example { âuserâ : âSYSTEMâ, âpasswordâ : âmypasswordâ } # For the extra options, refer to<br>Data Source Option<br>for the version you use. # Notes # Donât create too many partitions in parallel on a large cluster;<br>otherwise Spark might crash your external database systems. </td>\n","      <td id=\"T_cb50a_row3_col4\" class=\"data row3 col4\" >https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.jdbc.html</td>\n","    </tr>\n","    <tr>\n","      <th id=\"T_cb50a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n","      <td id=\"T_cb50a_row4_col0\" class=\"data row4 col0\" >Input/Output</td>\n","      <td id=\"T_cb50a_row4_col1\" class=\"data row4 col1\" >pyspark.sql.DataFrameReader.json</td>\n","      <td id=\"T_cb50a_row4_col2\" class=\"data row4 col2\" >Loads JSON files and returns the results as a DataFrame. JSON Lines  newline delimited JSON  is supported by default.For JSON  one record per file   set the multiLine parameter to true. If the schema parameter is not specified  this function goesthrough the input once to determine the input schema.</td>\n","      <td id=\"T_cb50a_row4_col3\" class=\"data row4 col3\" > # Write a DataFrame into a JSON file and read it back.<br>>>> import tempfile<br>>>> with tempfile.TemporaryDirectory() as d:<br>...     # Write a DataFrame into a JSON file<br>...     spark.createDataFrame(<br>...         [{\"age\": 100, \"name\": \"Hyukjin Kwon\"}]<br>...     ).write.mode(\"overwrite\").format(\"json\").save(d)<br>...<br>...     # Read the JSON file as a DataFrame.<br>...     spark.read.json(d).show()<br>+---+------------+<br>|age|        name|<br>+---+------------+<br>|100|Hyukjin Kwon|<br>+---+------------+<br> </td>\n","      <td id=\"T_cb50a_row4_col4\" class=\"data row4 col4\" >https://spark.apache.org/docs/3.5.0/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.json.html</td>\n","    </tr>\n","  </tbody>\n","</table>\n"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# WRITE FINAL SCRAPED RAW DATASET TO CSV AND DOWNLOAD\n","\n","DOWNLOAD_FLAG = 'N'  # Set to Y to download\n","\n","if DOWNLOAD_FLAG == 'Y':\n","  df_pyspark_raw.to_csv('ETL_P1_get_raw_web_data_v1.csv')\n","\n","  from google.colab import files\n","  files.download('ETL_P1_get_raw_web_data_v1.csv')"],"metadata":{"id":"A5KhSc_3qGRY","executionInfo":{"status":"ok","timestamp":1710277412496,"user_tz":-330,"elapsed":1342,"user":{"displayName":"SK G","userId":"17874234191554613076"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LxwOLktIsMED"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"MT0lBBEAsMJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"jWYfCrGWsMwH"},"execution_count":null,"outputs":[]}]}